{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to mlxtend's documentation! Mlxtend (machine learning extensions) is a Python library of useful tools for the day-to-day data science tasks. Links Documentation: https://rasbt.github.io/mlxtend Source code repository: https://github.com/rasbt/mlxtend PyPI: https://pypi.python.org/pypi/mlxtend Questions? Check out the GitHub Discussions board Examples import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec import itertools from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from mlxtend.classifier import EnsembleVoteClassifier from mlxtend.data import iris_data from mlxtend.plotting import plot_decision_regions # Initializing Classifiers clf1 = LogisticRegression(random_state=0) clf2 = RandomForestClassifier(random_state=0) clf3 = SVC(random_state=0, probability=True) eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[2, 1, 1], voting='soft') # Loading some example data X, y = iris_data() X = X[:,[0, 2]] # Plotting Decision Regions gs = gridspec.GridSpec(2, 2) fig = plt.figure(figsize=(10, 8)) labels = ['Logistic Regression', 'Random Forest', 'RBF kernel SVM', 'Ensemble'] for clf, lab, grd in zip([clf1, clf2, clf3, eclf], labels, itertools.product([0, 1], repeat=2)): clf.fit(X, y) ax = plt.subplot(gs[grd[0], grd[1]]) fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2) plt.title(lab) plt.show() If you use mlxtend as part of your workflow in a scientific publication, please consider citing the mlxtend repository with the following DOI: @article{raschkas_2018_mlxtend, author = {Sebastian Raschka}, title = {MLxtend: Providing machine learning and data science utilities and extensions to Python\u2019s scientific computing stack}, journal = {The Journal of Open Source Software}, volume = {3}, number = {24}, month = apr, year = 2018, publisher = {The Open Journal}, doi = {10.21105/joss.00638}, url = {https://joss.theoj.org/papers/10.21105/joss.00638} } License This project is released under a permissive new BSD open source license ( LICENSE-BSD3.txt ) and commercially usable. There is no warranty; not even for merchantability or fitness for a particular purpose. In addition, you may use, copy, modify and redistribute all artistic creative works (figures and images) included in this distribution under the directory according to the terms and conditions of the Creative Commons Attribution 4.0 International License. See the file LICENSE-CC-BY.txt for details. (Computer-generated graphics such as the plots produced by matplotlib fall under the BSD license mentioned above). Contact I received a lot of feedback and questions about mlxtend recently, and I thought that it would be worthwhile to set up a public communication channel. Before you write an email with a question about mlxtend, please consider posting it here since it can also be useful to others! Please join the Google Groups Mailing List ! If Google Groups is not for you, please feel free to write me an email or consider filing an issue on GitHub's issue tracker for new feature requests or bug reports. In addition, I setup a Gitter channel for live discussions.","title":"Home"},{"location":"#welcome-to-mlxtends-documentation","text":"Mlxtend (machine learning extensions) is a Python library of useful tools for the day-to-day data science tasks.","title":"Welcome to mlxtend's documentation!"},{"location":"#links","text":"Documentation: https://rasbt.github.io/mlxtend Source code repository: https://github.com/rasbt/mlxtend PyPI: https://pypi.python.org/pypi/mlxtend Questions? Check out the GitHub Discussions board","title":"Links"},{"location":"#examples","text":"import numpy as np import matplotlib.pyplot as plt import matplotlib.gridspec as gridspec import itertools from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.ensemble import RandomForestClassifier from mlxtend.classifier import EnsembleVoteClassifier from mlxtend.data import iris_data from mlxtend.plotting import plot_decision_regions # Initializing Classifiers clf1 = LogisticRegression(random_state=0) clf2 = RandomForestClassifier(random_state=0) clf3 = SVC(random_state=0, probability=True) eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[2, 1, 1], voting='soft') # Loading some example data X, y = iris_data() X = X[:,[0, 2]] # Plotting Decision Regions gs = gridspec.GridSpec(2, 2) fig = plt.figure(figsize=(10, 8)) labels = ['Logistic Regression', 'Random Forest', 'RBF kernel SVM', 'Ensemble'] for clf, lab, grd in zip([clf1, clf2, clf3, eclf], labels, itertools.product([0, 1], repeat=2)): clf.fit(X, y) ax = plt.subplot(gs[grd[0], grd[1]]) fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2) plt.title(lab) plt.show() If you use mlxtend as part of your workflow in a scientific publication, please consider citing the mlxtend repository with the following DOI: @article{raschkas_2018_mlxtend, author = {Sebastian Raschka}, title = {MLxtend: Providing machine learning and data science utilities and extensions to Python\u2019s scientific computing stack}, journal = {The Journal of Open Source Software}, volume = {3}, number = {24}, month = apr, year = 2018, publisher = {The Open Journal}, doi = {10.21105/joss.00638}, url = {https://joss.theoj.org/papers/10.21105/joss.00638} }","title":"Examples"},{"location":"#license","text":"This project is released under a permissive new BSD open source license ( LICENSE-BSD3.txt ) and commercially usable. There is no warranty; not even for merchantability or fitness for a particular purpose. In addition, you may use, copy, modify and redistribute all artistic creative works (figures and images) included in this distribution under the directory according to the terms and conditions of the Creative Commons Attribution 4.0 International License. See the file LICENSE-CC-BY.txt for details. (Computer-generated graphics such as the plots produced by matplotlib fall under the BSD license mentioned above).","title":"License"},{"location":"#contact","text":"I received a lot of feedback and questions about mlxtend recently, and I thought that it would be worthwhile to set up a public communication channel. Before you write an email with a question about mlxtend, please consider posting it here since it can also be useful to others! Please join the Google Groups Mailing List ! If Google Groups is not for you, please feel free to write me an email or consider filing an issue on GitHub's issue tracker for new feature requests or bug reports. In addition, I setup a Gitter channel for live discussions.","title":"Contact"},{"location":"CHANGELOG/","text":"Release Notes The CHANGELOG for the current development version is available at https://github.com/rasbt/mlxtend/blob/master/docs/sources/CHANGELOG.md . Version 0.23.1 (5 Jan 2024) Downloads Source code (zip) Source code (tar.gz) Changes Updated dependency on distutils for python 3.12 and above ( #1072 via peanutsee ) Version 0.23.0 (22 Sep 2023) Downloads Source code (zip) Source code (tar.gz) Changes Address NumPy deprecations to make mlxtend compatible to NumPy 1.24 Changed the signature of the LinearRegression model of sklearn in the test removing the normalize parameter as it is deprecated. ( #1036 ) Add pyproject.toml to support PEP 518 builds ( #1065 via jmahlik ) Fixed installation from sdist failing ( #1065 via jmahlik ) Converted configuration to pyproject.toml ( #1065 via jmahlik ) Remove mlxtend.image submodule with face recognition functions due to poor dlib support in modern environments. New Features and Enhancements Document how to use SequentialFeatureSelector and multiclass ROC AUC. Version 0.22.0 (4 April 2023) Downloads Source code (zip) Source code (tar.gz) Changes When ExhaustiveFeatureSelector is run with n_jobs == 1 , joblib is now disabled, which enables more immediate (live) feedback when the verbose mode is enabled. ( #985 via Nima Sarajpoor ) Disabled unnecessary warning in EnsembleVoteClassifier ( #941 ) Fixed various documentation issues ( #849 and #951 via Lekshmanan Natarajan ) Fixed \"Edit on GitHub\" button ( #1024 ) New Features and Enhancements The mlxtend.frequent_patterns.association_rules function has a new metric - Zhang's Metric, which measures both association and dissociation. ( #980 ) Internal mlxtend.frequent_patterns.fpmax code improvement that avoids casting a sparse DataFrame into a dense NumPy array. ( #1000 via Tim Kellogg ) The plot_decision_regions function now has a n_jobs parameter to parallelize the computation. (In a particular use case, on a small dataset, there was a 21x speed-up (449 seconds vs 21 seconds on local HPC instance of 36 cores). ( #998 via Khalid ElHaj ) Added mlxtend.frequent_patterns.hmine algorithm and documentation for mining frequent itemsets using the H-Mine algorithm. ( #1020 via Fatih Sen ) Version 0.21.0 (09/17/2022) Downloads Source code (zip) Source code (tar.gz) New Features and Enhancements The mlxtend.evaluate.feature_importance_permutation function has a new feature_groups argument to treat user-specified feature groups as single features, which is useful for one-hot encoded features. ( #955 ) The mlxtend.feature_selection.ExhaustiveFeatureSelector and SequentialFeatureSelector also gained support for feature_groups with a behavior similar to the one described above. ( #957 and #965 via Nima Sarajpoor ) Changes The custom_feature_names parameter was removed from the ExhaustiveFeatureSelector due to redundancy and to simplify the code base. The ExhaustiveFeatureSelector documentation illustrates how the same behavior and outcome can be achieved using pandas DataFrames. ( #957 ) Bug Fixes None Version 0.20.0 (05/26/2022) Downloads Source code (zip) Source code (tar.gz) New Features and Enhancements The mlxtend.evaluate.bootstrap_point632_score now supports fit_params . ( #861 ) The mlxtend/plotting/decision_regions.py function now has a contourf_kwargs for matplotlib to change the look of the decision boundaries if desired. ( #881 via [ pbloem ]) Add a norm_colormap parameter to mlxtend.plotting.plot_confusion_matrix , to allow normalizing the colormap, e.g., using matplotlib.colors.LogNorm() ( #895 ) Add new GroupTimeSeriesSplit class for evaluation in time series tasks with support of custom groups and additional parameters in comparison with scikit-learn's TimeSeriesSplit . ( #915 via Dmitry Labazkin ) Changes Due to compatibility issues with newer package versions, certain functions from six.py have been removed so that mlxtend may not work anymore with Python 2.7. As an internal change to speed up unit testing, unit testing is now faciliated by GitHub workflows, and Travis CI and Appveyor hooks have been removed. Improved axis label rotation in mlxtend.plotting.heatmap and mlxtend.plotting.plot_confusion_matrix ( #872 ) Fix various typos in McNemar guides. Raises a warning if non-bool arrays are used in the frequent pattern functions apriori , fpmax , and fpgrowth . ( #934 via NimaSarajpoor ) Bug Fixes Fix unreadable labels in heatmap for certain colormaps. ( #852 ) Fix an issue in mlxtend.plotting.plot_confusion_matrix when string class names are passed ( #894 ) Version 0.19.0 (2021-09-02) Downloads Source code (zip) Source code (tar.gz) New Features Adds a second \"balanced accuracy\" interpretation (\"balanced\") to evaluate.accuracy_score in addition to the existing \"average\" option to compute the scikit-learn-style balanced accuracy. ( #764 ) Adds new scatter_hist function to mlxtend.plotting for generating a scattered histogram. ( #757 via Maitreyee Mhasaka ) The evaluate.permutation_test function now accepts a paired argument to specify to support paired permutation/randomization tests. ( #768 ) The StackingCVRegressor now also supports multi-dimensional targets similar to StackingRegressor via StackingCVRegressor(..., multi_output=True) . ( #802 via Marco Tiraboschi ) Changes Updates unit tests for scikit-learn 0.24.1 compatibility. ( #774 ) StackingRegressor now requires setting StackingRegressor(..., multi_output=True) if the target is multi-dimensional; this allows for better input validation. ( #802 ) Removes deprecated res argument from plot_decision_regions . ( #803 ) Adds a title_fontsize parameter to plot_learning_curves for controlling the title font size; also the plot style is now the matplotlib default. ( #818 ) Internal change using 'c': 'none' instead of 'c': '' in mlxtend.plotting.plot_decision_regions 's scatterplot highlights to stay compatible with Matplotlib 3.4 and newer. ( #822 ) Adds a fontcolor_threshold parameter to the mlxtend.plotting.plot_confusion_matrix function as an additional option for determining the font color cut-off manually. ( #827 ) The frequent_patterns.association_rules now raises a ValueError if an empty frequent itemset DataFrame is passed. ( #843 ) The .632 and .632+ bootstrap method implemented in the mlxtend.evaluate.bootstrap_point632_score function now use the whole training set for the resubstitution weighting term instead of the internal training set that is a new bootstrap sample in each round. ( #844 ) Bug Fixes Fixes a typo in the SequentialFeatureSelector documentation ( #835 via Jo\u00e3o Pedro Zanlorensi Cardoso ) Version 0.18.0 (2020-11-25) Downloads Source code (zip) Source code (tar.gz) New Features The bias_variance_decomp function now supports optional fit_params for the estimators that are fit on bootstrap samples. ( #748 ) The bias_variance_decomp function now supports Keras estimators. ( #725 via @hanzigs ) Adds new mlxtend.classifier.OneRClassifier (One Rule Classfier) class, a simple rule-based classifier that is often used as a performance baseline or simple interpretable model. ( #726 Adds new create_counterfactual method for creating counterfactuals to explain model predictions. ( #740 ) Changes permutation_test ( mlxtend.evaluate.permutation ) \u00ecs corrected to give the proportion of permutations whose statistic is at least as extreme as the one observed. ( #721 via Florian Charlier ) Fixes the McNemar confusion matrix layout to match the convention (and documentation), swapping the upper left and lower right cells. ( #744 via mmarius ) Bug Fixes The loss in LogisticRegression for logging purposes didn't include the L2 penalty for the first weight in the weight vector (this is not the bias unit). However, since this loss function was only used for logging purposes, and the gradient remains correct, this does not have an effect on the main code. ( #741 ) Fixes a bug in bias_variance_decomp where when the mse loss was used, downcasting to integers caused imprecise results for small numbers. ( #749 ) Version 0.17.3 (2020-07-27) Downloads Source code (zip) Source code (tar.gz) New Features Add predict_proba kwarg to bootstrap methods, to allow bootstrapping of scoring functions that take in probability values. ( #700 via Adam Li ) Add a cell_values parameter to mlxtend.plotting.heatmap() to optionally suppress cell annotations by setting cell_values=False . ( #703 Changes Implemented both use_clones and fit_base_estimators (previously refit in EnsembleVoteClassifier ) for EnsembleVoteClassifier and StackingClassifier . ( #670 via Katrina Ni ) Switched to using raw strings for regex in mlxtend.text to prevent deprecation warning in Python 3.8 ( #688 ) Slice data in sequential forward selection before sending to parallel backend, reducing memory consumption. Bug Fixes Fixes axis DeprecationWarning in matplotlib v3.1.0 and newer. ( #673 ) Fixes an issue with using meshgrid in no_information_rate function used by the bootstrap_point632_score function for the .632+ estimate. ( #688 ) Fixes an issue in fpmax that could lead to incorrect support values. ( #692 via Steve Harenberg ) Version 0.17.2 (2020-02-24) Downloads Source code (zip) Source code (tar.gz) New Features - Changes The previously deprecated OnehotTransactions has been removed in favor of the TransactionEncoder. Removed SparseDataFrame support in frequent pattern mining functions in favor of pandas >=1.0's new way for working sparse data. If you used SparseDataFrame formats, please see pandas' migration guide at https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#migrating. ( #667 ) The plot_confusion_matrix.py now also accepts a matplotlib figure and axis as input to which the confusion matrix plot can be added. ( #671 via Vahid Mirjalili ) Bug Fixes - Version 0.17.1 (2020-01-28) Downloads Source code (zip) Source code (tar.gz) New Features The SequentialFeatureSelector now supports using pre-specified feature sets via the fixed_features parameter. ( #578 ) Adds a new accuracy_score function to mlxtend.evaluate for computing basic classifcation accuracy, per-class accuracy, and average per-class accuracy. ( #624 via Deepan Das ) StackingClassifier and StackingCVClassifier now have a decision_function method, which serves as a preferred choice over predict_proba in calculating roc_auc and average_precision scores when the meta estimator is a linear model or support vector classifier. ( #634 via Qiang Gu ) Changes Improve the runtime performance for the apriori frequent itemset generating function when low_memory=True . Setting low_memory=False (default) is still faster for small itemsets, but low_memory=True can be much faster for large itemsets and requires less memory. Also, input validation for apriori , \u0300 fpgrowth and fpmax takes a significant amount of time when input pandas DataFrame is large; this is now dramatically reduced when input contains boolean values (and not zeros/ones), which is the case when using TransactionEncoder`. ( #619 via Denis Barbier ) Add support for newer sparse pandas DataFrame for frequent itemset algorithms. Also, input validation for apriori , \u0300 fpgrowth and fpmax` runs much faster on sparse DataFrame when input pandas DataFrame contains integer values. ( #621 via Denis Barbier ) Let fpgrowth and fpmax directly work on sparse DataFrame, they were previously converted into dense Numpy arrays. ( #622 via Denis Barbier ) Bug Fixes Fixes a bug in mlxtend.plotting.plot_pca_correlation_graph that caused the explaind variances not summing up to 1. Also, improves the runtime performance of the correlation computation and adds a missing function argument for the explained variances (eigenvalues) if users provide their own principal components. ( #593 via Gabriel Azevedo Ferreira ) Behavior of fpgrowth and apriori consistent for edgecases such as min_support=0 . ( #573 via Steve Harenberg ) fpmax returns an empty data frame now instead of raising an error if the frequent itemset set is empty. ( #573 via Steve Harenberg ) Fixes and issue in mlxtend.plotting.plot_confusion_matrix , where the font-color choice for medium-dark cells was not ideal and hard to read. #588 via sohrabtowfighi ) The svd mode of mlxtend.feature_extraction.PrincipalComponentAnalysis now also n-1 degrees of freedom instead of n d.o.f. when computing the eigenvalues to match the behavior of eigen . #595 Disable input validation for StackingCVClassifier because it causes issues if pipelines are used as input. #606 Version 0.17.0 (2019-07-19) Downloads Source code (zip) Source code (tar.gz) New Features Added an enhancement to the existing iris_data() such that both the UCI Repository version of the Iris dataset as well as the corrected, original version of the dataset can be loaded, which has a slight difference in two data points (consistent with Fisher's paper; this is also the same as in R). (via #539 via janismdhanbad ) Added optional groups parameter to SequentialFeatureSelector and ExhaustiveFeatureSelector fit() methods for forwarding to sklearn CV ( #537 via arc12 ) Added a new plot_pca_correlation_graph function to the mlxtend.plotting submodule for plotting a PCA correlation graph. ( #544 via Gabriel-Azevedo-Ferreira ) Added a zoom_factor parameter to the mlxten.plotting.plot_decision_region function that allows users to zoom in and out of the decision region plots. ( #545 ) Added a function fpgrowth that implements the FP-Growth algorithm for mining frequent itemsets as a drop-in replacement for the existing apriori algorithm. ( #550 via Steve Harenberg ) New heatmap function in mlxtend.plotting . ( #552 ) Added a function fpmax that implements the FP-Max algorithm for mining maximal itemsets as a drop-in replacement for the fpgrowth algorithm. ( #553 via Steve Harenberg ) New figsize parameter for the plot_decision_regions function in mlxtend.plotting . ( #555 via Mirza Hasanbasic ) New low_memory option for the apriori frequent itemset generating function. Setting low_memory=False (default) uses a substantially optimized version of the algorithm that is 3-6x faster than the original implementation ( low_memory=True ). ( #567 via jmayse ) Added numerically stable OLS methods which uses QR decomposition and Singular Value Decomposition (SVD) methods to LinearRegression in mlxtend.regressor.linear_regression . ( #575 via PuneetGrov3r ) Changes Now uses the latest joblib library under the hood for multiprocessing instead of sklearn.externals.joblib . ( #547 ) Changes to StackingCVClassifier and StackingCVRegressor such that first-level models are allowed to generate output of non-numeric type. ( #562 ) Bug Fixes Fixed documentation of iris_data() under iris.py by adding a note about differences in the iris data in R and UCI machine learning repo. Make sure that if the 'svd' mode is used in PCA, the number of eigenvalues is the same as when using 'eigen' (append 0's zeros in that case) ( #565 ) Version 0.16.0 (2019-05-12) Downloads Source code (zip) Source code (tar.gz) New Features StackingCVClassifier and StackingCVRegressor now support random_state parameter, which, together with shuffle , controls the randomness in the cv splitting. ( #523 via Qiang Gu ) StackingCVClassifier and StackingCVRegressor now have a new drop_last_proba parameter. It drops the last \"probability\" column in the feature set since if True , because it is redundant: p(y_c) = 1 - p(y_1) + p(y_2) + ... + p(y_{c-1}). This can be useful for meta-classifiers that are sensitive to perfectly collinear features. ( #532 ) Other stacking estimators, including StackingClassifier , StackingCVClassifier and StackingRegressor , support grid search over the regressors and even a single base regressor. ( #522 via Qiang Gu ) Adds multiprocessing support to StackingCVClassifier . ( #522 via Qiang Gu ) Adds multiprocessing support to StackingCVRegressor . ( #512 via Qiang Gu ) Now, the StackingCVRegressor also enables grid search over the regressors and even a single base regressor. When there are level-mixed parameters, GridSearchCV will try to replace hyperparameters in a top-down order (see the documentation for examples details). ( #515 via Qiang Gu ) Adds a verbose parameter to apriori to show the current iteration number as well as the itemset size currently being sampled. ( #519 Adds an optional class_name parameter to the confusion matrix function to display class names on the axis as tick marks. ( #487 via sandpiturtle ) Adds a pca.e_vals_normalized_ attribute to PCA for storing the eigenvalues also in normalized form; this is commonly referred to as variance explained ratios. #545 Changes Due to new features, restructuring, and better scikit-learn support (for GridSearchCV , etc.) the StackingCVRegressor 's meta regressor is now being accessed via 'meta_regressor__* in the parameter grid. E.g., if a RandomForestRegressor as meta- egressor was previously tuned via 'randomforestregressor__n_estimators' , this has now changed to 'meta_regressor__n_estimators' . ( #515 via Qiang Gu ) The same change mentioned above is now applied to other stacking estimators, including StackingClassifier , StackingCVClassifier and StackingRegressor . ( #522 via Qiang Gu ) Automatically performs mean centering for PCA solver 'SVD' such that using SVD is always equal to using the covariance matrix approach #545 Bug Fixes The feature_selection.ColumnSelector now also supports column names of type int (in addition to str names) if the input is a pandas DataFrame. ( #500 via tetrar124 Fix unreadable labels in plot_confusion_matrix for imbalanced datasets if show_absolute=True and show_normed=True . ( #504 ) Raises a more informative error if a SparseDataFrame is passed to apriori and the dataframe has integer column names that don't start with 0 due to current limitations of the SparseDataFrame implementation in pandas. ( #503 ) SequentialFeatureSelector now supports DataFrame as input for all operating modes (forward/backward/floating). #506 mlxtend.evaluate.feature_importance_permutation now correctly accepts scoring functions with proper function signature as metric argument. #528 Version 0.15.0 (2019-01-19) Downloads Source code (zip) Source code (tar.gz) New Features Adds a new transformer class to mlxtend.image , EyepadAlign , that aligns face images based on the location of the eyes. ( #466 by Vahid Mirjalili ) Adds a new function, mlxtend.evaluate.bias_variance_decomp that decomposes the loss of a regressor or classifier into bias and variance terms. ( #470 ) Adds a whitening parameter to PrincipalComponentAnalysis , to optionally whiten the transformed data such that the features have unit variance. ( #475 ) Changes Changed the default solver in PrincipalComponentAnalysis to 'svd' instead of 'eigen' to improve numerical stability. ( #474 ) The mlxtend.image.extract_face_landmarks now returns None if no facial landmarks were detected instead of an array of all zeros. ( #466 ) Bug Fixes The eigenvectors maybe have not been sorted in certain edge cases if solver was 'eigen' in PrincipalComponentAnalysis and LinearDiscriminantAnalysis . ( #477 , #478 ) Version 0.14.0 (2018-11-09) Downloads Source code (zip) Source code (tar.gz) New Features Added a scatterplotmatrix function to the plotting module. ( #437 ) Added sample_weight option to StackingRegressor , StackingClassifier , StackingCVRegressor , StackingCVClassifier , EnsembleVoteClassifier . ( #438 ) Added a RandomHoldoutSplit class to perform a random train/valid split without rotation in SequentialFeatureSelector , scikit-learn GridSearchCV etc. ( #442 ) Added a PredefinedHoldoutSplit class to perform a train/valid split, based on user-specified indices, without rotation in SequentialFeatureSelector , scikit-learn GridSearchCV etc. ( #443 ) Created a new mlxtend.image submodule for working on image processing-related tasks. ( #457 ) Added a new convenience function extract_face_landmarks based on dlib to mlxtend.image . ( #458 ) Added a method='oob' option to the mlxtend.evaluate.bootstrap_point632_score method to compute the classic out-of-bag bootstrap estimate ( #459 ) Added a method='.632+' option to the mlxtend.evaluate.bootstrap_point632_score method to compute the .632+ bootstrap estimate that addresses the optimism bias of the .632 bootstrap ( #459 ) Added a new mlxtend.evaluate.ftest function to perform an F-test for comparing the accuracies of two or more classification models. ( #460 ) Added a new mlxtend.evaluate.combined_ftest_5x2cv function to perform an combined 5x2cv F-Test for comparing the performance of two models. ( #461 ) Added a new mlxtend.evaluate.difference_proportions test for comparing two proportions (e.g., classifier accuracies) ( #462 ) Changes Addressed deprecations warnings in NumPy 0.15. ( #425 ) Because of complications in PR ( #459 ), Python 2.7 was now dropped; since official support for Python 2.7 by the Python Software Foundation is ending in approx. 12 months anyways, this re-focussing will hopefully free up some developer time with regard to not having to worry about backward compatibility Bug Fixes Fixed an issue with a missing import in mlxtend.plotting.plot_confusion_matrix . ( #428 ) Version 0.13.0 (2018-07-20) Downloads Source code (zip) Source code (tar.gz) New Features A meaningful error message is now raised when a cross-validation generator is used with SequentialFeatureSelector . ( #377 ) The SequentialFeatureSelector now accepts custom feature names via the fit method for more interpretable feature subset reports. ( #379 ) The SequentialFeatureSelector is now also compatible with Pandas DataFrames and uses DataFrame column-names for more interpretable feature subset reports. ( #379 ) ColumnSelector now works with Pandas DataFrames columns. ( #378 by Manuel Garrido ) The ExhaustiveFeatureSelector estimator in mlxtend.feature_selection now is safely stoppable mid-process by control+c. ( #380 ) Two new functions, vectorspace_orthonormalization and vectorspace_dimensionality were added to mlxtend.math to use the Gram-Schmidt process to convert a set of linearly independent vectors into a set of orthonormal basis vectors, and to compute the dimensionality of a vectorspace, respectively. ( #382 ) mlxtend.frequent_patterns.apriori now supports pandas SparseDataFrame s to generate frequent itemsets. ( #404 via Daniel Morales ) The plot_confusion_matrix function now has the ability to show normalized confusion matrix coefficients in addition to or instead of absolute confusion matrix coefficients with or without a colorbar. The text display method has been changed so that the full range of the colormap is used. The default size is also now set based on the number of classes. Added support for merging the meta features with the original input features in StackingRegressor (via use_features_in_secondary ) like it is already supported in the other Stacking classes. ( #418 ) Added a support_only to the association_rules function, which allow constructing association rules (based on the support metric only) for cropped input DataFrames that don't contain a complete set of antecedent and consequent support values. ( #421 ) Changes Itemsets generated with apriori are now frozenset s ( #393 by William Laney and #394 ) Now raises an error if a input DataFrame to apriori contains non 0, 1, True, False values. #419 ) Bug Fixes Allow mlxtend estimators to be cloned via scikit-learn's clone function. ( #374 ) Fixes bug to allow the correct use of refit=False in StackingRegressor and StackingCVRegressor ( #384 and ( #385 ) by selay01 ) Allow StackingClassifier to work with sparse matrices when use_features_in_secondary=True ( #408 by Floris Hoogenbook ) Allow StackingCVRegressor to work with sparse matrices when use_features_in_secondary=True ( #416 ) Allow StackingCVClassifier to work with sparse matrices when use_features_in_secondary=True ( #417 ) Version 0.12.0 (2018-21-04) Downloads Source code (zip) Source code (tar.gz) New Features A new feature_importance_permuation function to compute the feature importance in classifiers and regressors via the permutation importance method ( #358 ) The fit method of the ExhaustiveFeatureSelector now optionally accepts **fit_params for the estimator that is used for the feature selection. ( #354 by Zach Griffith) The fit method of the SequentialFeatureSelector now optionally accepts **fit_params for the estimator that is used for the feature selection. ( #350 by Zach Griffith) Changes Replaced plot_decision_regions colors by a colorblind-friendly palette and adds contour lines for decision regions. ( #348 ) All stacking estimators now raise NonFittedErrors if any method for inference is called prior to fitting the estimator. ( #353 ) Renamed the refit parameter of both the StackingClassifier and StackingCVClassifier to use_clones to be more explicit and less misleading. ( #368 ) Bug Fixes Various changes in the documentation and documentation tools to fix formatting issues ( #363 ) Fixes a bug where the StackingCVClassifier 's meta features were not stored in the original order when shuffle=True ( #370 ) Many documentation improvements, including links to the User Guides in the API docs ( #371 ) Version 0.11.0 (2018-03-14) Downloads Source code (zip) Source code (tar.gz) New Features New function implementing the resampled paired t-test procedure ( paired_ttest_resampled ) to compare the performance of two models. ( #323 ) New function implementing the k-fold paired t-test procedure ( paired_ttest_kfold_cv ) to compare the performance of two models (also called k-hold-out paired t-test). ( #324 ) New function implementing the 5x2cv paired t-test procedure ( paired_ttest_5x2cv ) proposed by Dieterrich (1998) to compare the performance of two models. ( #325 ) A refit parameter was added to stacking classes (similar to the refit parameter in the EnsembleVoteClassifier ), to support classifiers and regressors that follow the scikit-learn API but are not compatible with scikit-learn's clone function. ( #322 ) The ColumnSelector now has a drop_axis argument to use it in pipelines with CountVectorizers . ( #333 ) Changes Raises an informative error message if predict or predict_meta_features is called prior to calling the fit method in StackingRegressor and StackingCVRegressor . ( #315 ) The plot_decision_regions function now automatically determines the optimal setting based on the feature dimensions and supports anti-aliasing. The old res parameter has been deprecated. ( #309 by Guillaume Poirier-Morency ) Apriori code is faster due to optimization in onehot transformation and the amount of candidates generated by the apriori algorithm. ( #327 by Jakub Smid ) The OnehotTransactions class (which is typically often used in combination with the apriori function for association rule mining) is now more memory efficient as it uses boolean arrays instead of integer arrays. In addition, the OnehotTransactions class can be now be provided with sparse argument to generate sparse representations of the onehot matrix to further improve memory efficiency. ( #328 by Jakub Smid ) The OneHotTransactions has been deprecated and replaced by the TransactionEncoder . ( #332 The plot_decision_regions function now has three new parameters, scatter_kwargs , contourf_kwargs , and scatter_highlight_kwargs , that can be used to modify the plotting style. ( #342 by James Bourbeau ) Bug Fixes Fixed issue when class labels were provided to the EnsembleVoteClassifier when refit was set to false . ( #322 ) Allow arrays with 16-bit and 32-bit precision in plot_decision_regions function. ( #337 ) Fixed bug that raised an indexing error if the number of items was <= 1 when computing association rules using the conviction metric. ( #340 ) Version 0.10.0 (2017-12-22) Downloads Source code (zip) Source code (tar.gz) New Features New store_train_meta_features parameter for fit in StackingCVRegressor. if True, train meta-features are stored in self.train_meta_features_ . New pred_meta_features method for StackingCVRegressor . People can get test meta-features using this method. ( #294 via takashioya ) The new store_train_meta_features attribute and pred_meta_features method for the StackingCVRegressor were also added to the StackingRegressor , StackingClassifier , and StackingCVClassifier ( #299 & #300 ) New function ( evaluate.mcnemar_tables ) for creating multiple 2x2 contigency from model predictions arrays that can be used in multiple McNemar (post-hoc) tests or Cochran's Q or F tests, etc. ( #307 ) New function ( evaluate.cochrans_q ) for performing Cochran's Q test to compare the accuracy of multiple classifiers. ( #310 ) Changes Added requirements.txt to setup.py . ( #304 via Colin Carrol ) Bug Fixes Improved numerical stability for p-values computed via the the exact McNemar test ( #306 ) nose is not required to use the library ( #302 ) Version 0.9.1 (2017-11-19) Downloads Source code (zip) Source code (tar.gz) New Features Added mlxtend.evaluate.bootstrap_point632_score to evaluate the performance of estimators using the .632 bootstrap. ( #283 ) New max_len parameter for the frequent itemset generation via the apriori function to allow for early stopping. ( #270 ) Changes All feature index tuples in SequentialFeatureSelector or now in sorted order. ( #262 ) The SequentialFeatureSelector now runs the continuation of the floating inclusion/exclusion as described in Novovicova & Kittler (1994). Note that this didn't cause any difference in performance on any of the test scenarios but could lead to better performance in certain edge cases. ( #262 ) utils.Counter now accepts a name variable to help distinguish between multiple counters, time precision can be set with the 'precision' kwarg and the new attribute end_time holds the time the last iteration completed. ( #278 via Mathew Savage ) Bug Fixes Fixed an deprecation error that occured with McNemar test when using SciPy 1.0. ( #283 ) Version 0.9.0 (2017-10-21) Downloads Source code (zip) Source code (tar.gz) New Features Added evaluate.permutation_test , a permutation test for hypothesis testing (or A/B testing) to test if two samples come from the same distribution. Or in other words, a procedure to test the null hypothesis that that two groups are not significantly different (e.g., a treatment and a control group). ( #250 ) Added 'leverage' and 'conviction as evaluation metrics to the frequent_patterns.association_rules function. ( #246 & #247 ) Added a loadings_ attribute to PrincipalComponentAnalysis to compute the factor loadings of the features on the principal components. ( #251 ) Allow grid search over classifiers/regressors in ensemble and stacking estimators. ( #259 ) New make_multiplexer_dataset function that creates a dataset generated by a n-bit Boolean multiplexer for evaluating supervised learning algorithms. ( #263 ) Added a new BootstrapOutOfBag class, an implementation of the out-of-bag bootstrap to evaluate supervised learning algorithms. ( #265 ) The parameters for StackingClassifier , StackingCVClassifier , StackingRegressor , StackingCVRegressor , and EnsembleVoteClassifier can now be tuned using scikit-learn's GridSearchCV ( #254 via James Bourbeau ) Changes The 'support' column returned by frequent_patterns.association_rules was changed to compute the support of \"antecedant union consequent\", and new antecedant support' and 'consequent support' column were added to avoid ambiguity. ( #245 ) Allow the OnehotTransactions to be cloned via scikit-learn's clone function, which is required by e.g., scikit-learn's FeatureUnion or GridSearchCV (via Iaroslav Shcherbatyi ). ( #249 ) Bug Fixes Fix issues with self._init_time parameter in _IterativeModel subclasses. ( #256 ) Fix imprecision bug that occurred in plot_ecdf when run on Python 2.7. ( 264 ) The vectors from SVD in PrincipalComponentAnalysis are now being scaled so that the eigenvalues via solver='eigen' and solver='svd' now store eigenvalues that have the same magnitudes. ( #251 ) Version 0.8.0 (2017-09-09) Downloads Source code (zip) Source code (tar.gz) New Features Added a mlxtend.evaluate.bootstrap that implements the ordinary nonparametric bootstrap to bootstrap a single statistic (for example, the mean. median, R^2 of a regression fit, and so forth) #232 SequentialFeatureSelecor 's k_features now accepts a string argument \"best\" or \"parsimonious\" for more \"automated\" feature selection. For instance, if \"best\" is provided, the feature selector will return the feature subset with the best cross-validation performance. If \"parsimonious\" is provided as an argument, the smallest feature subset that is within one standard error of the cross-validation performance will be selected. #238 Changes SequentialFeatureSelector now uses np.nanmean over normal mean to support scorers that may return np.nan #211 (via mrkaiser ) The skip_if_stuck parameter was removed from SequentialFeatureSelector in favor of a more efficient implementation comparing the conditional inclusion/exclusion results (in the floating versions) to the performances of previously sampled feature sets that were cached #237 ExhaustiveFeatureSelector was modified to consume substantially less memory #195 (via Adam Erickson ) Bug Fixes Fixed a bug where the SequentialFeatureSelector selected a feature subset larger than then specified via the k_features tuple max-value #213 Version 0.7.0 (2017-06-22) Downloads Source code (zip) Source code (tar.gz) New Features New mlxtend.plotting.ecdf function for plotting empirical cumulative distribution functions ( #196 ). New StackingCVRegressor for stacking regressors with out-of-fold predictions to prevent overfitting ( #201 via Eike Dehling ). Changes The TensorFlow estimator have been removed from mlxtend, since TensorFlow has now very convenient ways to build on estimators, which render those implementations obsolete. plot_decision_regions now supports plotting decision regions for more than 2 training features #189 , via James Bourbeau ). Parallel execution in mlxtend.feature_selection.SequentialFeatureSelector and mlxtend.feature_selection.ExhaustiveFeatureSelector is now performed over different feature subsets instead of the different cross-validation folds to better utilize machines with multiple processors if the number of features is large ( #193 , via @whalebot-helmsman ). Raise meaningful error messages if pandas DataFrame s or Python lists of lists are fed into the StackingCVClassifer as a fit arguments ( 198 ). The n_folds parameter of the StackingCVClassifier was changed to cv and can now accept any kind of cross validation technique that is available from scikit-learn. For example, StackingCVClassifier(..., cv=StratifiedKFold(n_splits=3)) or StackingCVClassifier(..., cv=GroupKFold(n_splits=3)) ( #203 , via Konstantinos Paliouras ). Bug Fixes SequentialFeatureSelector now correctly accepts a None argument for the scoring parameter to infer the default scoring metric from scikit-learn classifiers and regressors ( #171 ). The plot_decision_regions function now supports pre-existing axes objects generated via matplotlib's plt.subplots . ( #184 , see example ) Made math.num_combinations and math.num_permutations numerically stable for large numbers of combinations and permutations ( #200 ). Version 0.6.0 (2017-03-18) Downloads Source code (zip) Source code (tar.gz) New Features An association_rules function is implemented that allows to generate rules based on a list of frequent itemsets (via Joshua Goerner ). Changes Adds a black edgecolor to plots via plotting.plot_decision_regions to make markers more distinguishable from the background in matplotlib>=2.0 . The association submodule was renamed to frequent_patterns . Bug Fixes The DataFrame index of apriori results are now unique and ordered. Fixed typos in autompg and wine datasets (via James Bourbeau ). Version 0.5.1 (2017-02-14) Downloads Source code (zip) Source code (tar.gz) New Features The EnsembleVoteClassifier has a new refit attribute that prevents refitting classifiers if refit=False to save computational time. Added a new lift_score function in evaluate to compute lift score (via Batuhan Bardak ). StackingClassifier and StackingRegressor support multivariate targets if the underlying models do (via kernc ). StackingClassifier has a new use_features_in_secondary attribute like StackingCVClassifier . Changes Changed default verbosity level in SequentialFeatureSelector to 0 The EnsembleVoteClassifier now raises a NotFittedError if the estimator wasn't fit before calling predict . (via Anton Loss ) Added new TensorFlow variable initialization syntax to guarantee compatibility with TensorFlow 1.0 Bug Fixes Fixed wrong default value for k_features in SequentialFeatureSelector Cast selected feature subsets in the SequentialFeautureSelector as sets to prevent the iterator from getting stuck if the k_idx are different permutations of the same combination (via Zac Wellmer ). Fixed an issue with learning curves that caused the performance metrics to be reversed (via ipashchenko ) Fixed a bug that could occur in the SequentialFeatureSelector if there are similarly-well performing subsets in the floating variants (via Zac Wellmer ). Version 0.5.0 (2016-11-09) Downloads Source code (zip) Source code (tar.gz) New Features New ExhaustiveFeatureSelector estimator in mlxtend.feature_selection for evaluating all feature combinations in a specified range The StackingClassifier has a new parameter average_probas that is set to True by default to maintain the current behavior. A deprecation warning was added though, and it will default to False in future releases (0.6.0); average_probas=False will result in stacking of the level-1 predicted probabilities rather than averaging these. New StackingCVClassifier estimator in 'mlxtend.classifier' for implementing a stacking ensemble that uses cross-validation techniques for training the meta-estimator to avoid overfitting ( Reiichiro Nakano ) New OnehotTransactions encoder class added to the preprocessing submodule for transforming transaction data into a one-hot encoded array The SequentialFeatureSelector estimator in mlxtend.feature_selection now is safely stoppable mid-process by control+c, and deprecated print_progress in favor of a more tunable verbose parameter ( Will McGinnis ) New apriori function in association to extract frequent itemsets from transaction data for association rule mining New checkerboard_plot function in plotting to plot checkerboard tables / heat maps New mcnemar_table and mcnemar functions in evaluate to compute 2x2 contingency tables and McNemar's test Changes All plotting functions have been moved to mlxtend.plotting for compatibility reasons with continuous integration services and to make the installation of matplotlib optional for users of mlxtend 's core functionality Added a compatibility layer for scikit-learn 0.18 using the new model_selection module while maintaining backwards compatibility to scikit-learn 0.17. Bug Fixes mlxtend.plotting.plot_decision_regions now draws decision regions correctly if more than 4 class labels are present Raise AttributeError in plot_decision_regions when the X_higlight argument is a 1D array ( chkoar ) Version 0.4.2 (2016-08-24) Downloads Source code (zip) Source code (tar.gz) PDF documentation New Features Added preprocessing.CopyTransformer , a mock class that returns copies of imput arrays via transform and fit_transform Changes Added AppVeyor to CI to ensure MS Windows compatibility Dataset are now saved as compressed .txt or .csv files rather than being imported as Python objects feature_selection.SequentialFeatureSelector now supports the selection of k_features using a tuple to specify a \"min-max\" k_features range Added \"SVD solver\" option to the PrincipalComponentAnalysis Raise a AttributeError with \"not fitted\" message in SequentialFeatureSelector if transform or get_metric_dict are called prior to fit Use small, positive bias units in TfMultiLayerPerceptron 's hidden layer(s) if the activations are ReLUs in order to avoid dead neurons Added an optional clone_estimator parameter to the SequentialFeatureSelector that defaults to True , avoiding the modification of the original estimator objects More rigorous type and shape checks in the evaluate.plot_decision_regions function DenseTransformer now doesn't raise and error if the input array is not sparse API clean-up using scikit-learn's BaseEstimator as parent class for feature_selection.ColumnSelector Bug Fixes Fixed a problem when a tuple-range was provided as argument to the SequentialFeatureSelector 's k_features parameter and the scoring metric was more negative than -1 (e.g., as in scikit-learn's MSE scoring function) (wahutch](https://github.com/wahutch)) Fixed an AttributeError issue when verbose > 1 in StackingClassifier Fixed a bug in classifier.SoftmaxRegression where the mean values of the offsets were used to update the bias units rather than their sum Fixed rare bug in MLP _layer_mapping functions that caused a swap between the random number generation seed when initializing weights and biases Version 0.4.1 (2016-05-01) Downloads Source code (zip) Source code (tar.gz) PDF documentation New Features New TensorFlow estimator for Linear Regression ( tf_regressor.TfLinearRegression ) New k-means clustering estimator ( cluster.Kmeans ) New TensorFlow k-means clustering estimator ( tf_cluster.Kmeans ) Changes Due to refactoring of the estimator classes, the init_weights parameter of the fit methods was globally renamed to init_params Overall performance improvements of estimators due to code clean-up and refactoring Added several additional checks for correct array types and more meaningful exception messages Added optional dropout to the tf_classifier.TfMultiLayerPerceptron classifier for regularization Added an optional decay parameter to the tf_classifier.TfMultiLayerPerceptron classifier for adaptive learning via an exponential decay of the learning rate eta Replaced old NeuralNetMLP by more streamlined MultiLayerPerceptron ( classifier.MultiLayerPerceptron ); now also with softmax in the output layer and categorical cross-entropy loss. Unified init_params parameter for fit functions to continue training where the algorithm left off (if supported) Version 0.4.0 (2016-04-09) New Features New TfSoftmaxRegression classifier using Tensorflow ( tf_classifier.TfSoftmaxRegression ) New SoftmaxRegression classifier ( classifier.SoftmaxRegression ) New TfMultiLayerPerceptron classifier using Tensorflow ( tf_classifier.TfMultiLayerPerceptron ) New StackingRegressor ( regressor.StackingRegressor ) New StackingClassifier ( classifier.StackingClassifier ) New function for one-hot encoding of class labels ( preprocessing.one_hot ) Added GridSearch support to the SequentialFeatureSelector ( feature_selection/.SequentialFeatureSelector ) evaluate.plot_decision_regions improvements: Function now handles class y-class labels correctly if array is of type float Correct handling of input arguments markers and colors Accept an existing Axes via the ax argument New print_progress parameter for all generalized models and multi-layer neural networks for printing time elapsed, ETA, and the current cost of the current epoch Minibatch learning for classifier.LogisticRegression , classifier.Adaline , and regressor.LinearRegression plus streamlined API New Principal Component Analysis class via mlxtend.feature_extraction.PrincipalComponentAnalysis New RBF Kernel Principal Component Analysis class via mlxtend.feature_extraction.RBFKernelPCA New Linear Discriminant Analysis class via mlxtend.feature_extraction.LinearDiscriminantAnalysis Changes The column parameter in mlxtend.preprocessing.standardize now defaults to None to standardize all columns more conveniently Version 0.3.0 (2016-01-31) Downloads Source code (zip) Source code (tar.gz) New Features Added a progress bar tracker to classifier.NeuralNetMLP Added a function to score predicted vs. target class labels evaluate.scoring Added confusion matrix functions to create ( evaluate.confusion_matrix ) and plot ( evaluate.plot_confusion_matrix ) confusion matrices New style parameter and improved axis scaling in mlxtend.evaluate.plot_learning_curves Added loadlocal_mnist to mlxtend.data for streaming MNIST from a local byte files into numpy arrays New NeuralNetMLP parameters: random_weights , shuffle_init , shuffle_epoch New SFS features such as the generation of pandas DataFrame results tables and plotting functions (with confidence intervals, standard deviation, and standard error bars) Added support for regression estimators in SFS Added Boston housing dataset New shuffle parameter for classifier.NeuralNetMLP Changes The mlxtend.preprocessing.standardize function now optionally returns the parameters, which are estimated from the array, for re-use. A further improvement makes the standardize function smarter in order to avoid zero-division errors Cosmetic improvements to the evaluate.plot_decision_regions function such as hiding plot axes Renaming of classifier.EnsembleClassfier to classifier.EnsembleVoteClassifier Improved random weight initialization in Perceptron , Adaline , LinearRegression , and LogisticRegression Changed learning parameter of mlxtend.classifier.Adaline to solver and added \"normal equation\" as closed-form solution solver Hide y-axis labels in mlxtend.evaluate.plot_decision_regions in 1 dimensional evaluations Sequential Feature Selection algorithms were unified into a single SequentialFeatureSelector class with parameters to enable floating selection and toggle between forward and backward selection. Stratified sampling of MNIST (now 500x random samples from each of the 10 digit categories) Renaming mlxtend.plotting to mlxtend.general_plotting in order to distinguish general plotting function from specialized utility function such as evaluate.plot_decision_regions Version 0.2.9 (2015-07-14) Downloads Source code (zip) Source code (tar.gz) New Features Sequential Feature Selection algorithms: SFS, SFFS, SBS, and SFBS Changes Changed regularization & lambda parameters in LogisticRegression to single parameter l2_lambda Version 0.2.8 (2015-06-27) API changes: mlxtend.sklearn.EnsembleClassifier -> mlxtend.classifier.EnsembleClassifier mlxtend.sklearn.ColumnSelector -> mlxtend.feature_selection.ColumnSelector mlxtend.sklearn.DenseTransformer -> mlxtend.preprocessing.DenseTransformer mlxtend.pandas.standardizing -> mlxtend.preprocessing.standardizing mlxtend.pandas.minmax_scaling -> mlxtend.preprocessing.minmax_scaling mlxtend.matplotlib -> mlxtend.plotting Added momentum learning parameter (alpha coefficient) to mlxtend.classifier.NeuralNetMLP . Added adaptive learning rate (decrease constant) to mlxtend.classifier.NeuralNetMLP . mlxtend.pandas.minmax_scaling became mlxtend.preprocessing.minmax_scaling and also supports NumPy arrays now mlxtend.pandas.standardizing became mlxtend.preprocessing.standardizing and now supports both NumPy arrays and pandas DataFrames; also, now ddof parameters to set the degrees of freedom when calculating the standard deviation Version 0.2.7 (2015-06-20) Added multilayer perceptron (feedforward artificial neural network) classifier as mlxtend.classifier.NeuralNetMLP . Added 5000 labeled trainingsamples from the MNIST handwritten digits dataset to mlxtend.data Version 0.2.6 (2015-05-08) Added ordinary least square regression using different solvers (gradient and stochastic gradient descent, and the closed form solution (normal equation) Added option for random weight initialization to logistic regression classifier and updated l2 regularization Added wine dataset to mlxtend.data Added invert_axes parameter mlxtend.matplotlib.enrichtment_plot to optionally plot the \"Count\" on the x-axis New verbose parameter for mlxtend.sklearn.EnsembleClassifier by Alejandro C. Bahnsen Added mlxtend.pandas.standardizing to standardize columns in a Pandas DataFrame Added parameters linestyles and markers to mlxtend.matplotlib.enrichment_plot mlxtend.regression.lin_regplot automatically adds np.newaxis and works w. python lists Added tokenizers: mlxtend.text.extract_emoticons and mlxtend.text.extract_words_and_emoticons Version 0.2.5 (2015-04-17) Added Sequential Backward Selection (mlxtend.sklearn.SBS) Added X_highlight parameter to mlxtend.evaluate.plot_decision_regions for highlighting test data points. Added mlxtend.regression.lin_regplot to plot the fitted line from linear regression. Added mlxtend.matplotlib.stacked_barplot to conveniently produce stacked barplots using pandas DataFrame s. Added mlxtend.matplotlib.enrichment_plot Version 0.2.4 (2015-03-15) Added scoring to mlxtend.evaluate.learning_curves (by user pfsq) Fixed setup.py bug caused by the missing README.html file matplotlib.category_scatter for pandas DataFrames and Numpy arrays Version 0.2.3 (2015-03-11) Added Logistic regression Gradient descent and stochastic gradient descent perceptron was changed to Adaline (Adaptive Linear Neuron) Perceptron and Adaline for {0, 1} classes Added mlxtend.preprocessing.shuffle_arrays_unison function to shuffle one or more NumPy arrays. Added shuffle and random seed parameter to stochastic gradient descent classifier. Added rstrip parameter to mlxtend.file_io.find_filegroups to allow trimming of base names. Added ignore_substring parameter to mlxtend.file_io.find_filegroups and find_files . Replaced .rstrip in mlxtend.file_io.find_filegroups with more robust regex. Gridsearch support for mlxtend.sklearn.EnsembleClassifier Version 0.2.2 (2015-03-01) Improved robustness of EnsembleClassifier. Extended plot_decision_regions() functionality for plotting 1D decision boundaries. Function matplotlib.plot_decision_regions was reorganized to evaluate.plot_decision_regions . evaluate.plot_learning_curves() function added. Added Rosenblatt, gradient descent, and stochastic gradient descent perceptrons. Version 0.2.1 (2015-01-20) Added mlxtend.pandas.minmax_scaling - a function to rescale pandas DataFrame columns. Slight update to the EnsembleClassifier interface (additional voting parameter) Fixed EnsembleClassifier to return correct class labels if class labels are not integers from 0 to n. Added new matplotlib function to plot decision regions of classifiers. Version 0.2.0 (2015-01-13) Improved mlxtend.text.generalize_duplcheck to remove duplicates and prevent endless looping issue. Added recursive search parameter to mlxtend.file_io.find_files. Added check_ext parameter mlxtend.file_io.find_files to search based on file extensions. Default parameter to ignore invisible files for mlxtend.file_io.find. Added transform and fit_transform to the EnsembleClassifier . Added mlxtend.file_io.find_filegroups function. Version 0.1.9 (2015-01-10) Implemented scikit-learn EnsembleClassifier (majority voting rule) class. Version 0.1.8 (2015-01-07) Improvements to mlxtend.text.generalize_names to handle certain Dutch last name prefixes (van, van der, de, etc.). Added mlxtend.text.generalize_name_duplcheck function to apply mlxtend.text.generalize_names function to a pandas DataFrame without creating duplicates. Version 0.1.7 (2015-01-07) Added text utilities with name generalization function. Added and file_io utilities. Version 0.1.6 (2015-01-04) Added combinations and permutations estimators. Version 0.1.5 (2014-12-11) Added DenseTransformer for pipelines and grid search. Version 0.1.4 (2014-08-20) mean_centering function is now a Class that creates MeanCenterer objects that can be used to fit data via the fit method, and center data at the column means via the transform and fit_transform method. Version 0.1.3 (2014-08-19) Added preprocessing module and mean_centering function. Version 0.1.2 (2014-08-19) Added matplotlib utilities and remove_borders function. Version 0.1.1 (2014-08-13) Simplified code for ColumnSelector.","title":"Release Notes"},{"location":"CHANGELOG/#release-notes","text":"The CHANGELOG for the current development version is available at https://github.com/rasbt/mlxtend/blob/master/docs/sources/CHANGELOG.md .","title":"Release Notes"},{"location":"CHANGELOG/#version-0231-5-jan-2024","text":"","title":"Version 0.23.1 (5 Jan 2024)"},{"location":"CHANGELOG/#downloads","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#changes","text":"Updated dependency on distutils for python 3.12 and above ( #1072 via peanutsee )","title":"Changes"},{"location":"CHANGELOG/#version-0230-22-sep-2023","text":"","title":"Version 0.23.0 (22 Sep 2023)"},{"location":"CHANGELOG/#downloads_1","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#changes_1","text":"Address NumPy deprecations to make mlxtend compatible to NumPy 1.24 Changed the signature of the LinearRegression model of sklearn in the test removing the normalize parameter as it is deprecated. ( #1036 ) Add pyproject.toml to support PEP 518 builds ( #1065 via jmahlik ) Fixed installation from sdist failing ( #1065 via jmahlik ) Converted configuration to pyproject.toml ( #1065 via jmahlik ) Remove mlxtend.image submodule with face recognition functions due to poor dlib support in modern environments.","title":"Changes"},{"location":"CHANGELOG/#new-features-and-enhancements","text":"Document how to use SequentialFeatureSelector and multiclass ROC AUC.","title":"New Features and Enhancements"},{"location":"CHANGELOG/#version-0220-4-april-2023","text":"","title":"Version 0.22.0 (4 April 2023)"},{"location":"CHANGELOG/#downloads_2","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#changes_2","text":"When ExhaustiveFeatureSelector is run with n_jobs == 1 , joblib is now disabled, which enables more immediate (live) feedback when the verbose mode is enabled. ( #985 via Nima Sarajpoor ) Disabled unnecessary warning in EnsembleVoteClassifier ( #941 ) Fixed various documentation issues ( #849 and #951 via Lekshmanan Natarajan ) Fixed \"Edit on GitHub\" button ( #1024 )","title":"Changes"},{"location":"CHANGELOG/#new-features-and-enhancements_1","text":"The mlxtend.frequent_patterns.association_rules function has a new metric - Zhang's Metric, which measures both association and dissociation. ( #980 ) Internal mlxtend.frequent_patterns.fpmax code improvement that avoids casting a sparse DataFrame into a dense NumPy array. ( #1000 via Tim Kellogg ) The plot_decision_regions function now has a n_jobs parameter to parallelize the computation. (In a particular use case, on a small dataset, there was a 21x speed-up (449 seconds vs 21 seconds on local HPC instance of 36 cores). ( #998 via Khalid ElHaj ) Added mlxtend.frequent_patterns.hmine algorithm and documentation for mining frequent itemsets using the H-Mine algorithm. ( #1020 via Fatih Sen )","title":"New Features and Enhancements"},{"location":"CHANGELOG/#version-0210-09172022","text":"","title":"Version 0.21.0 (09/17/2022)"},{"location":"CHANGELOG/#downloads_3","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features-and-enhancements_2","text":"The mlxtend.evaluate.feature_importance_permutation function has a new feature_groups argument to treat user-specified feature groups as single features, which is useful for one-hot encoded features. ( #955 ) The mlxtend.feature_selection.ExhaustiveFeatureSelector and SequentialFeatureSelector also gained support for feature_groups with a behavior similar to the one described above. ( #957 and #965 via Nima Sarajpoor )","title":"New Features and Enhancements"},{"location":"CHANGELOG/#changes_3","text":"The custom_feature_names parameter was removed from the ExhaustiveFeatureSelector due to redundancy and to simplify the code base. The ExhaustiveFeatureSelector documentation illustrates how the same behavior and outcome can be achieved using pandas DataFrames. ( #957 )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes","text":"None","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0200-05262022","text":"","title":"Version 0.20.0 (05/26/2022)"},{"location":"CHANGELOG/#downloads_4","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features-and-enhancements_3","text":"The mlxtend.evaluate.bootstrap_point632_score now supports fit_params . ( #861 ) The mlxtend/plotting/decision_regions.py function now has a contourf_kwargs for matplotlib to change the look of the decision boundaries if desired. ( #881 via [ pbloem ]) Add a norm_colormap parameter to mlxtend.plotting.plot_confusion_matrix , to allow normalizing the colormap, e.g., using matplotlib.colors.LogNorm() ( #895 ) Add new GroupTimeSeriesSplit class for evaluation in time series tasks with support of custom groups and additional parameters in comparison with scikit-learn's TimeSeriesSplit . ( #915 via Dmitry Labazkin )","title":"New Features and Enhancements"},{"location":"CHANGELOG/#changes_4","text":"Due to compatibility issues with newer package versions, certain functions from six.py have been removed so that mlxtend may not work anymore with Python 2.7. As an internal change to speed up unit testing, unit testing is now faciliated by GitHub workflows, and Travis CI and Appveyor hooks have been removed. Improved axis label rotation in mlxtend.plotting.heatmap and mlxtend.plotting.plot_confusion_matrix ( #872 ) Fix various typos in McNemar guides. Raises a warning if non-bool arrays are used in the frequent pattern functions apriori , fpmax , and fpgrowth . ( #934 via NimaSarajpoor )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_1","text":"Fix unreadable labels in heatmap for certain colormaps. ( #852 ) Fix an issue in mlxtend.plotting.plot_confusion_matrix when string class names are passed ( #894 )","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0190-2021-09-02","text":"","title":"Version 0.19.0 (2021-09-02)"},{"location":"CHANGELOG/#downloads_5","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features","text":"Adds a second \"balanced accuracy\" interpretation (\"balanced\") to evaluate.accuracy_score in addition to the existing \"average\" option to compute the scikit-learn-style balanced accuracy. ( #764 ) Adds new scatter_hist function to mlxtend.plotting for generating a scattered histogram. ( #757 via Maitreyee Mhasaka ) The evaluate.permutation_test function now accepts a paired argument to specify to support paired permutation/randomization tests. ( #768 ) The StackingCVRegressor now also supports multi-dimensional targets similar to StackingRegressor via StackingCVRegressor(..., multi_output=True) . ( #802 via Marco Tiraboschi )","title":"New Features"},{"location":"CHANGELOG/#changes_5","text":"Updates unit tests for scikit-learn 0.24.1 compatibility. ( #774 ) StackingRegressor now requires setting StackingRegressor(..., multi_output=True) if the target is multi-dimensional; this allows for better input validation. ( #802 ) Removes deprecated res argument from plot_decision_regions . ( #803 ) Adds a title_fontsize parameter to plot_learning_curves for controlling the title font size; also the plot style is now the matplotlib default. ( #818 ) Internal change using 'c': 'none' instead of 'c': '' in mlxtend.plotting.plot_decision_regions 's scatterplot highlights to stay compatible with Matplotlib 3.4 and newer. ( #822 ) Adds a fontcolor_threshold parameter to the mlxtend.plotting.plot_confusion_matrix function as an additional option for determining the font color cut-off manually. ( #827 ) The frequent_patterns.association_rules now raises a ValueError if an empty frequent itemset DataFrame is passed. ( #843 ) The .632 and .632+ bootstrap method implemented in the mlxtend.evaluate.bootstrap_point632_score function now use the whole training set for the resubstitution weighting term instead of the internal training set that is a new bootstrap sample in each round. ( #844 )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_2","text":"Fixes a typo in the SequentialFeatureSelector documentation ( #835 via Jo\u00e3o Pedro Zanlorensi Cardoso )","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0180-2020-11-25","text":"","title":"Version 0.18.0 (2020-11-25)"},{"location":"CHANGELOG/#downloads_6","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_1","text":"The bias_variance_decomp function now supports optional fit_params for the estimators that are fit on bootstrap samples. ( #748 ) The bias_variance_decomp function now supports Keras estimators. ( #725 via @hanzigs ) Adds new mlxtend.classifier.OneRClassifier (One Rule Classfier) class, a simple rule-based classifier that is often used as a performance baseline or simple interpretable model. ( #726 Adds new create_counterfactual method for creating counterfactuals to explain model predictions. ( #740 )","title":"New Features"},{"location":"CHANGELOG/#changes_6","text":"permutation_test ( mlxtend.evaluate.permutation ) \u00ecs corrected to give the proportion of permutations whose statistic is at least as extreme as the one observed. ( #721 via Florian Charlier ) Fixes the McNemar confusion matrix layout to match the convention (and documentation), swapping the upper left and lower right cells. ( #744 via mmarius )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_3","text":"The loss in LogisticRegression for logging purposes didn't include the L2 penalty for the first weight in the weight vector (this is not the bias unit). However, since this loss function was only used for logging purposes, and the gradient remains correct, this does not have an effect on the main code. ( #741 ) Fixes a bug in bias_variance_decomp where when the mse loss was used, downcasting to integers caused imprecise results for small numbers. ( #749 )","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0173-2020-07-27","text":"","title":"Version 0.17.3 (2020-07-27)"},{"location":"CHANGELOG/#downloads_7","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_2","text":"Add predict_proba kwarg to bootstrap methods, to allow bootstrapping of scoring functions that take in probability values. ( #700 via Adam Li ) Add a cell_values parameter to mlxtend.plotting.heatmap() to optionally suppress cell annotations by setting cell_values=False . ( #703","title":"New Features"},{"location":"CHANGELOG/#changes_7","text":"Implemented both use_clones and fit_base_estimators (previously refit in EnsembleVoteClassifier ) for EnsembleVoteClassifier and StackingClassifier . ( #670 via Katrina Ni ) Switched to using raw strings for regex in mlxtend.text to prevent deprecation warning in Python 3.8 ( #688 ) Slice data in sequential forward selection before sending to parallel backend, reducing memory consumption.","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_4","text":"Fixes axis DeprecationWarning in matplotlib v3.1.0 and newer. ( #673 ) Fixes an issue with using meshgrid in no_information_rate function used by the bootstrap_point632_score function for the .632+ estimate. ( #688 ) Fixes an issue in fpmax that could lead to incorrect support values. ( #692 via Steve Harenberg )","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0172-2020-02-24","text":"","title":"Version 0.17.2 (2020-02-24)"},{"location":"CHANGELOG/#downloads_8","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_3","text":"-","title":"New Features"},{"location":"CHANGELOG/#changes_8","text":"The previously deprecated OnehotTransactions has been removed in favor of the TransactionEncoder. Removed SparseDataFrame support in frequent pattern mining functions in favor of pandas >=1.0's new way for working sparse data. If you used SparseDataFrame formats, please see pandas' migration guide at https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#migrating. ( #667 ) The plot_confusion_matrix.py now also accepts a matplotlib figure and axis as input to which the confusion matrix plot can be added. ( #671 via Vahid Mirjalili )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_5","text":"-","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0171-2020-01-28","text":"","title":"Version 0.17.1 (2020-01-28)"},{"location":"CHANGELOG/#downloads_9","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_4","text":"The SequentialFeatureSelector now supports using pre-specified feature sets via the fixed_features parameter. ( #578 ) Adds a new accuracy_score function to mlxtend.evaluate for computing basic classifcation accuracy, per-class accuracy, and average per-class accuracy. ( #624 via Deepan Das ) StackingClassifier and StackingCVClassifier now have a decision_function method, which serves as a preferred choice over predict_proba in calculating roc_auc and average_precision scores when the meta estimator is a linear model or support vector classifier. ( #634 via Qiang Gu )","title":"New Features"},{"location":"CHANGELOG/#changes_9","text":"Improve the runtime performance for the apriori frequent itemset generating function when low_memory=True . Setting low_memory=False (default) is still faster for small itemsets, but low_memory=True can be much faster for large itemsets and requires less memory. Also, input validation for apriori , \u0300 fpgrowth and fpmax takes a significant amount of time when input pandas DataFrame is large; this is now dramatically reduced when input contains boolean values (and not zeros/ones), which is the case when using TransactionEncoder`. ( #619 via Denis Barbier ) Add support for newer sparse pandas DataFrame for frequent itemset algorithms. Also, input validation for apriori , \u0300 fpgrowth and fpmax` runs much faster on sparse DataFrame when input pandas DataFrame contains integer values. ( #621 via Denis Barbier ) Let fpgrowth and fpmax directly work on sparse DataFrame, they were previously converted into dense Numpy arrays. ( #622 via Denis Barbier )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_6","text":"Fixes a bug in mlxtend.plotting.plot_pca_correlation_graph that caused the explaind variances not summing up to 1. Also, improves the runtime performance of the correlation computation and adds a missing function argument for the explained variances (eigenvalues) if users provide their own principal components. ( #593 via Gabriel Azevedo Ferreira ) Behavior of fpgrowth and apriori consistent for edgecases such as min_support=0 . ( #573 via Steve Harenberg ) fpmax returns an empty data frame now instead of raising an error if the frequent itemset set is empty. ( #573 via Steve Harenberg ) Fixes and issue in mlxtend.plotting.plot_confusion_matrix , where the font-color choice for medium-dark cells was not ideal and hard to read. #588 via sohrabtowfighi ) The svd mode of mlxtend.feature_extraction.PrincipalComponentAnalysis now also n-1 degrees of freedom instead of n d.o.f. when computing the eigenvalues to match the behavior of eigen . #595 Disable input validation for StackingCVClassifier because it causes issues if pipelines are used as input. #606","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0170-2019-07-19","text":"","title":"Version 0.17.0 (2019-07-19)"},{"location":"CHANGELOG/#downloads_10","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_5","text":"Added an enhancement to the existing iris_data() such that both the UCI Repository version of the Iris dataset as well as the corrected, original version of the dataset can be loaded, which has a slight difference in two data points (consistent with Fisher's paper; this is also the same as in R). (via #539 via janismdhanbad ) Added optional groups parameter to SequentialFeatureSelector and ExhaustiveFeatureSelector fit() methods for forwarding to sklearn CV ( #537 via arc12 ) Added a new plot_pca_correlation_graph function to the mlxtend.plotting submodule for plotting a PCA correlation graph. ( #544 via Gabriel-Azevedo-Ferreira ) Added a zoom_factor parameter to the mlxten.plotting.plot_decision_region function that allows users to zoom in and out of the decision region plots. ( #545 ) Added a function fpgrowth that implements the FP-Growth algorithm for mining frequent itemsets as a drop-in replacement for the existing apriori algorithm. ( #550 via Steve Harenberg ) New heatmap function in mlxtend.plotting . ( #552 ) Added a function fpmax that implements the FP-Max algorithm for mining maximal itemsets as a drop-in replacement for the fpgrowth algorithm. ( #553 via Steve Harenberg ) New figsize parameter for the plot_decision_regions function in mlxtend.plotting . ( #555 via Mirza Hasanbasic ) New low_memory option for the apriori frequent itemset generating function. Setting low_memory=False (default) uses a substantially optimized version of the algorithm that is 3-6x faster than the original implementation ( low_memory=True ). ( #567 via jmayse ) Added numerically stable OLS methods which uses QR decomposition and Singular Value Decomposition (SVD) methods to LinearRegression in mlxtend.regressor.linear_regression . ( #575 via PuneetGrov3r )","title":"New Features"},{"location":"CHANGELOG/#changes_10","text":"Now uses the latest joblib library under the hood for multiprocessing instead of sklearn.externals.joblib . ( #547 ) Changes to StackingCVClassifier and StackingCVRegressor such that first-level models are allowed to generate output of non-numeric type. ( #562 )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_7","text":"Fixed documentation of iris_data() under iris.py by adding a note about differences in the iris data in R and UCI machine learning repo. Make sure that if the 'svd' mode is used in PCA, the number of eigenvalues is the same as when using 'eigen' (append 0's zeros in that case) ( #565 )","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0160-2019-05-12","text":"","title":"Version 0.16.0 (2019-05-12)"},{"location":"CHANGELOG/#downloads_11","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_6","text":"StackingCVClassifier and StackingCVRegressor now support random_state parameter, which, together with shuffle , controls the randomness in the cv splitting. ( #523 via Qiang Gu ) StackingCVClassifier and StackingCVRegressor now have a new drop_last_proba parameter. It drops the last \"probability\" column in the feature set since if True , because it is redundant: p(y_c) = 1 - p(y_1) + p(y_2) + ... + p(y_{c-1}). This can be useful for meta-classifiers that are sensitive to perfectly collinear features. ( #532 ) Other stacking estimators, including StackingClassifier , StackingCVClassifier and StackingRegressor , support grid search over the regressors and even a single base regressor. ( #522 via Qiang Gu ) Adds multiprocessing support to StackingCVClassifier . ( #522 via Qiang Gu ) Adds multiprocessing support to StackingCVRegressor . ( #512 via Qiang Gu ) Now, the StackingCVRegressor also enables grid search over the regressors and even a single base regressor. When there are level-mixed parameters, GridSearchCV will try to replace hyperparameters in a top-down order (see the documentation for examples details). ( #515 via Qiang Gu ) Adds a verbose parameter to apriori to show the current iteration number as well as the itemset size currently being sampled. ( #519 Adds an optional class_name parameter to the confusion matrix function to display class names on the axis as tick marks. ( #487 via sandpiturtle ) Adds a pca.e_vals_normalized_ attribute to PCA for storing the eigenvalues also in normalized form; this is commonly referred to as variance explained ratios. #545","title":"New Features"},{"location":"CHANGELOG/#changes_11","text":"Due to new features, restructuring, and better scikit-learn support (for GridSearchCV , etc.) the StackingCVRegressor 's meta regressor is now being accessed via 'meta_regressor__* in the parameter grid. E.g., if a RandomForestRegressor as meta- egressor was previously tuned via 'randomforestregressor__n_estimators' , this has now changed to 'meta_regressor__n_estimators' . ( #515 via Qiang Gu ) The same change mentioned above is now applied to other stacking estimators, including StackingClassifier , StackingCVClassifier and StackingRegressor . ( #522 via Qiang Gu ) Automatically performs mean centering for PCA solver 'SVD' such that using SVD is always equal to using the covariance matrix approach #545","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_8","text":"The feature_selection.ColumnSelector now also supports column names of type int (in addition to str names) if the input is a pandas DataFrame. ( #500 via tetrar124 Fix unreadable labels in plot_confusion_matrix for imbalanced datasets if show_absolute=True and show_normed=True . ( #504 ) Raises a more informative error if a SparseDataFrame is passed to apriori and the dataframe has integer column names that don't start with 0 due to current limitations of the SparseDataFrame implementation in pandas. ( #503 ) SequentialFeatureSelector now supports DataFrame as input for all operating modes (forward/backward/floating). #506 mlxtend.evaluate.feature_importance_permutation now correctly accepts scoring functions with proper function signature as metric argument. #528","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0150-2019-01-19","text":"","title":"Version 0.15.0 (2019-01-19)"},{"location":"CHANGELOG/#downloads_12","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_7","text":"Adds a new transformer class to mlxtend.image , EyepadAlign , that aligns face images based on the location of the eyes. ( #466 by Vahid Mirjalili ) Adds a new function, mlxtend.evaluate.bias_variance_decomp that decomposes the loss of a regressor or classifier into bias and variance terms. ( #470 ) Adds a whitening parameter to PrincipalComponentAnalysis , to optionally whiten the transformed data such that the features have unit variance. ( #475 )","title":"New Features"},{"location":"CHANGELOG/#changes_12","text":"Changed the default solver in PrincipalComponentAnalysis to 'svd' instead of 'eigen' to improve numerical stability. ( #474 ) The mlxtend.image.extract_face_landmarks now returns None if no facial landmarks were detected instead of an array of all zeros. ( #466 )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_9","text":"The eigenvectors maybe have not been sorted in certain edge cases if solver was 'eigen' in PrincipalComponentAnalysis and LinearDiscriminantAnalysis . ( #477 , #478 )","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0140-2018-11-09","text":"","title":"Version 0.14.0 (2018-11-09)"},{"location":"CHANGELOG/#downloads_13","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_8","text":"Added a scatterplotmatrix function to the plotting module. ( #437 ) Added sample_weight option to StackingRegressor , StackingClassifier , StackingCVRegressor , StackingCVClassifier , EnsembleVoteClassifier . ( #438 ) Added a RandomHoldoutSplit class to perform a random train/valid split without rotation in SequentialFeatureSelector , scikit-learn GridSearchCV etc. ( #442 ) Added a PredefinedHoldoutSplit class to perform a train/valid split, based on user-specified indices, without rotation in SequentialFeatureSelector , scikit-learn GridSearchCV etc. ( #443 ) Created a new mlxtend.image submodule for working on image processing-related tasks. ( #457 ) Added a new convenience function extract_face_landmarks based on dlib to mlxtend.image . ( #458 ) Added a method='oob' option to the mlxtend.evaluate.bootstrap_point632_score method to compute the classic out-of-bag bootstrap estimate ( #459 ) Added a method='.632+' option to the mlxtend.evaluate.bootstrap_point632_score method to compute the .632+ bootstrap estimate that addresses the optimism bias of the .632 bootstrap ( #459 ) Added a new mlxtend.evaluate.ftest function to perform an F-test for comparing the accuracies of two or more classification models. ( #460 ) Added a new mlxtend.evaluate.combined_ftest_5x2cv function to perform an combined 5x2cv F-Test for comparing the performance of two models. ( #461 ) Added a new mlxtend.evaluate.difference_proportions test for comparing two proportions (e.g., classifier accuracies) ( #462 )","title":"New Features"},{"location":"CHANGELOG/#changes_13","text":"Addressed deprecations warnings in NumPy 0.15. ( #425 ) Because of complications in PR ( #459 ), Python 2.7 was now dropped; since official support for Python 2.7 by the Python Software Foundation is ending in approx. 12 months anyways, this re-focussing will hopefully free up some developer time with regard to not having to worry about backward compatibility","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_10","text":"Fixed an issue with a missing import in mlxtend.plotting.plot_confusion_matrix . ( #428 )","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0130-2018-07-20","text":"","title":"Version 0.13.0 (2018-07-20)"},{"location":"CHANGELOG/#downloads_14","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_9","text":"A meaningful error message is now raised when a cross-validation generator is used with SequentialFeatureSelector . ( #377 ) The SequentialFeatureSelector now accepts custom feature names via the fit method for more interpretable feature subset reports. ( #379 ) The SequentialFeatureSelector is now also compatible with Pandas DataFrames and uses DataFrame column-names for more interpretable feature subset reports. ( #379 ) ColumnSelector now works with Pandas DataFrames columns. ( #378 by Manuel Garrido ) The ExhaustiveFeatureSelector estimator in mlxtend.feature_selection now is safely stoppable mid-process by control+c. ( #380 ) Two new functions, vectorspace_orthonormalization and vectorspace_dimensionality were added to mlxtend.math to use the Gram-Schmidt process to convert a set of linearly independent vectors into a set of orthonormal basis vectors, and to compute the dimensionality of a vectorspace, respectively. ( #382 ) mlxtend.frequent_patterns.apriori now supports pandas SparseDataFrame s to generate frequent itemsets. ( #404 via Daniel Morales ) The plot_confusion_matrix function now has the ability to show normalized confusion matrix coefficients in addition to or instead of absolute confusion matrix coefficients with or without a colorbar. The text display method has been changed so that the full range of the colormap is used. The default size is also now set based on the number of classes. Added support for merging the meta features with the original input features in StackingRegressor (via use_features_in_secondary ) like it is already supported in the other Stacking classes. ( #418 ) Added a support_only to the association_rules function, which allow constructing association rules (based on the support metric only) for cropped input DataFrames that don't contain a complete set of antecedent and consequent support values. ( #421 )","title":"New Features"},{"location":"CHANGELOG/#changes_14","text":"Itemsets generated with apriori are now frozenset s ( #393 by William Laney and #394 ) Now raises an error if a input DataFrame to apriori contains non 0, 1, True, False values. #419 )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_11","text":"Allow mlxtend estimators to be cloned via scikit-learn's clone function. ( #374 ) Fixes bug to allow the correct use of refit=False in StackingRegressor and StackingCVRegressor ( #384 and ( #385 ) by selay01 ) Allow StackingClassifier to work with sparse matrices when use_features_in_secondary=True ( #408 by Floris Hoogenbook ) Allow StackingCVRegressor to work with sparse matrices when use_features_in_secondary=True ( #416 ) Allow StackingCVClassifier to work with sparse matrices when use_features_in_secondary=True ( #417 )","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0120-2018-21-04","text":"","title":"Version 0.12.0 (2018-21-04)"},{"location":"CHANGELOG/#downloads_15","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_10","text":"A new feature_importance_permuation function to compute the feature importance in classifiers and regressors via the permutation importance method ( #358 ) The fit method of the ExhaustiveFeatureSelector now optionally accepts **fit_params for the estimator that is used for the feature selection. ( #354 by Zach Griffith) The fit method of the SequentialFeatureSelector now optionally accepts **fit_params for the estimator that is used for the feature selection. ( #350 by Zach Griffith)","title":"New Features"},{"location":"CHANGELOG/#changes_15","text":"Replaced plot_decision_regions colors by a colorblind-friendly palette and adds contour lines for decision regions. ( #348 ) All stacking estimators now raise NonFittedErrors if any method for inference is called prior to fitting the estimator. ( #353 ) Renamed the refit parameter of both the StackingClassifier and StackingCVClassifier to use_clones to be more explicit and less misleading. ( #368 )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_12","text":"Various changes in the documentation and documentation tools to fix formatting issues ( #363 ) Fixes a bug where the StackingCVClassifier 's meta features were not stored in the original order when shuffle=True ( #370 ) Many documentation improvements, including links to the User Guides in the API docs ( #371 )","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0110-2018-03-14","text":"","title":"Version 0.11.0 (2018-03-14)"},{"location":"CHANGELOG/#downloads_16","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_11","text":"New function implementing the resampled paired t-test procedure ( paired_ttest_resampled ) to compare the performance of two models. ( #323 ) New function implementing the k-fold paired t-test procedure ( paired_ttest_kfold_cv ) to compare the performance of two models (also called k-hold-out paired t-test). ( #324 ) New function implementing the 5x2cv paired t-test procedure ( paired_ttest_5x2cv ) proposed by Dieterrich (1998) to compare the performance of two models. ( #325 ) A refit parameter was added to stacking classes (similar to the refit parameter in the EnsembleVoteClassifier ), to support classifiers and regressors that follow the scikit-learn API but are not compatible with scikit-learn's clone function. ( #322 ) The ColumnSelector now has a drop_axis argument to use it in pipelines with CountVectorizers . ( #333 )","title":"New Features"},{"location":"CHANGELOG/#changes_16","text":"Raises an informative error message if predict or predict_meta_features is called prior to calling the fit method in StackingRegressor and StackingCVRegressor . ( #315 ) The plot_decision_regions function now automatically determines the optimal setting based on the feature dimensions and supports anti-aliasing. The old res parameter has been deprecated. ( #309 by Guillaume Poirier-Morency ) Apriori code is faster due to optimization in onehot transformation and the amount of candidates generated by the apriori algorithm. ( #327 by Jakub Smid ) The OnehotTransactions class (which is typically often used in combination with the apriori function for association rule mining) is now more memory efficient as it uses boolean arrays instead of integer arrays. In addition, the OnehotTransactions class can be now be provided with sparse argument to generate sparse representations of the onehot matrix to further improve memory efficiency. ( #328 by Jakub Smid ) The OneHotTransactions has been deprecated and replaced by the TransactionEncoder . ( #332 The plot_decision_regions function now has three new parameters, scatter_kwargs , contourf_kwargs , and scatter_highlight_kwargs , that can be used to modify the plotting style. ( #342 by James Bourbeau )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_13","text":"Fixed issue when class labels were provided to the EnsembleVoteClassifier when refit was set to false . ( #322 ) Allow arrays with 16-bit and 32-bit precision in plot_decision_regions function. ( #337 ) Fixed bug that raised an indexing error if the number of items was <= 1 when computing association rules using the conviction metric. ( #340 )","title":"Bug Fixes"},{"location":"CHANGELOG/#version-0100-2017-12-22","text":"","title":"Version 0.10.0 (2017-12-22)"},{"location":"CHANGELOG/#downloads_17","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_12","text":"New store_train_meta_features parameter for fit in StackingCVRegressor. if True, train meta-features are stored in self.train_meta_features_ . New pred_meta_features method for StackingCVRegressor . People can get test meta-features using this method. ( #294 via takashioya ) The new store_train_meta_features attribute and pred_meta_features method for the StackingCVRegressor were also added to the StackingRegressor , StackingClassifier , and StackingCVClassifier ( #299 & #300 ) New function ( evaluate.mcnemar_tables ) for creating multiple 2x2 contigency from model predictions arrays that can be used in multiple McNemar (post-hoc) tests or Cochran's Q or F tests, etc. ( #307 ) New function ( evaluate.cochrans_q ) for performing Cochran's Q test to compare the accuracy of multiple classifiers. ( #310 )","title":"New Features"},{"location":"CHANGELOG/#changes_17","text":"Added requirements.txt to setup.py . ( #304 via Colin Carrol )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_14","text":"Improved numerical stability for p-values computed via the the exact McNemar test ( #306 ) nose is not required to use the library ( #302 )","title":"Bug Fixes"},{"location":"CHANGELOG/#version-091-2017-11-19","text":"","title":"Version 0.9.1 (2017-11-19)"},{"location":"CHANGELOG/#downloads_18","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_13","text":"Added mlxtend.evaluate.bootstrap_point632_score to evaluate the performance of estimators using the .632 bootstrap. ( #283 ) New max_len parameter for the frequent itemset generation via the apriori function to allow for early stopping. ( #270 )","title":"New Features"},{"location":"CHANGELOG/#changes_18","text":"All feature index tuples in SequentialFeatureSelector or now in sorted order. ( #262 ) The SequentialFeatureSelector now runs the continuation of the floating inclusion/exclusion as described in Novovicova & Kittler (1994). Note that this didn't cause any difference in performance on any of the test scenarios but could lead to better performance in certain edge cases. ( #262 ) utils.Counter now accepts a name variable to help distinguish between multiple counters, time precision can be set with the 'precision' kwarg and the new attribute end_time holds the time the last iteration completed. ( #278 via Mathew Savage )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_15","text":"Fixed an deprecation error that occured with McNemar test when using SciPy 1.0. ( #283 )","title":"Bug Fixes"},{"location":"CHANGELOG/#version-090-2017-10-21","text":"","title":"Version 0.9.0 (2017-10-21)"},{"location":"CHANGELOG/#downloads_19","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_14","text":"Added evaluate.permutation_test , a permutation test for hypothesis testing (or A/B testing) to test if two samples come from the same distribution. Or in other words, a procedure to test the null hypothesis that that two groups are not significantly different (e.g., a treatment and a control group). ( #250 ) Added 'leverage' and 'conviction as evaluation metrics to the frequent_patterns.association_rules function. ( #246 & #247 ) Added a loadings_ attribute to PrincipalComponentAnalysis to compute the factor loadings of the features on the principal components. ( #251 ) Allow grid search over classifiers/regressors in ensemble and stacking estimators. ( #259 ) New make_multiplexer_dataset function that creates a dataset generated by a n-bit Boolean multiplexer for evaluating supervised learning algorithms. ( #263 ) Added a new BootstrapOutOfBag class, an implementation of the out-of-bag bootstrap to evaluate supervised learning algorithms. ( #265 ) The parameters for StackingClassifier , StackingCVClassifier , StackingRegressor , StackingCVRegressor , and EnsembleVoteClassifier can now be tuned using scikit-learn's GridSearchCV ( #254 via James Bourbeau )","title":"New Features"},{"location":"CHANGELOG/#changes_19","text":"The 'support' column returned by frequent_patterns.association_rules was changed to compute the support of \"antecedant union consequent\", and new antecedant support' and 'consequent support' column were added to avoid ambiguity. ( #245 ) Allow the OnehotTransactions to be cloned via scikit-learn's clone function, which is required by e.g., scikit-learn's FeatureUnion or GridSearchCV (via Iaroslav Shcherbatyi ). ( #249 )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_16","text":"Fix issues with self._init_time parameter in _IterativeModel subclasses. ( #256 ) Fix imprecision bug that occurred in plot_ecdf when run on Python 2.7. ( 264 ) The vectors from SVD in PrincipalComponentAnalysis are now being scaled so that the eigenvalues via solver='eigen' and solver='svd' now store eigenvalues that have the same magnitudes. ( #251 )","title":"Bug Fixes"},{"location":"CHANGELOG/#version-080-2017-09-09","text":"","title":"Version 0.8.0 (2017-09-09)"},{"location":"CHANGELOG/#downloads_20","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_15","text":"Added a mlxtend.evaluate.bootstrap that implements the ordinary nonparametric bootstrap to bootstrap a single statistic (for example, the mean. median, R^2 of a regression fit, and so forth) #232 SequentialFeatureSelecor 's k_features now accepts a string argument \"best\" or \"parsimonious\" for more \"automated\" feature selection. For instance, if \"best\" is provided, the feature selector will return the feature subset with the best cross-validation performance. If \"parsimonious\" is provided as an argument, the smallest feature subset that is within one standard error of the cross-validation performance will be selected. #238","title":"New Features"},{"location":"CHANGELOG/#changes_20","text":"SequentialFeatureSelector now uses np.nanmean over normal mean to support scorers that may return np.nan #211 (via mrkaiser ) The skip_if_stuck parameter was removed from SequentialFeatureSelector in favor of a more efficient implementation comparing the conditional inclusion/exclusion results (in the floating versions) to the performances of previously sampled feature sets that were cached #237 ExhaustiveFeatureSelector was modified to consume substantially less memory #195 (via Adam Erickson )","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_17","text":"Fixed a bug where the SequentialFeatureSelector selected a feature subset larger than then specified via the k_features tuple max-value #213","title":"Bug Fixes"},{"location":"CHANGELOG/#version-070-2017-06-22","text":"","title":"Version 0.7.0 (2017-06-22)"},{"location":"CHANGELOG/#downloads_21","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_16","text":"New mlxtend.plotting.ecdf function for plotting empirical cumulative distribution functions ( #196 ). New StackingCVRegressor for stacking regressors with out-of-fold predictions to prevent overfitting ( #201 via Eike Dehling ).","title":"New Features"},{"location":"CHANGELOG/#changes_21","text":"The TensorFlow estimator have been removed from mlxtend, since TensorFlow has now very convenient ways to build on estimators, which render those implementations obsolete. plot_decision_regions now supports plotting decision regions for more than 2 training features #189 , via James Bourbeau ). Parallel execution in mlxtend.feature_selection.SequentialFeatureSelector and mlxtend.feature_selection.ExhaustiveFeatureSelector is now performed over different feature subsets instead of the different cross-validation folds to better utilize machines with multiple processors if the number of features is large ( #193 , via @whalebot-helmsman ). Raise meaningful error messages if pandas DataFrame s or Python lists of lists are fed into the StackingCVClassifer as a fit arguments ( 198 ). The n_folds parameter of the StackingCVClassifier was changed to cv and can now accept any kind of cross validation technique that is available from scikit-learn. For example, StackingCVClassifier(..., cv=StratifiedKFold(n_splits=3)) or StackingCVClassifier(..., cv=GroupKFold(n_splits=3)) ( #203 , via Konstantinos Paliouras ).","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_18","text":"SequentialFeatureSelector now correctly accepts a None argument for the scoring parameter to infer the default scoring metric from scikit-learn classifiers and regressors ( #171 ). The plot_decision_regions function now supports pre-existing axes objects generated via matplotlib's plt.subplots . ( #184 , see example ) Made math.num_combinations and math.num_permutations numerically stable for large numbers of combinations and permutations ( #200 ).","title":"Bug Fixes"},{"location":"CHANGELOG/#version-060-2017-03-18","text":"","title":"Version 0.6.0 (2017-03-18)"},{"location":"CHANGELOG/#downloads_22","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_17","text":"An association_rules function is implemented that allows to generate rules based on a list of frequent itemsets (via Joshua Goerner ).","title":"New Features"},{"location":"CHANGELOG/#changes_22","text":"Adds a black edgecolor to plots via plotting.plot_decision_regions to make markers more distinguishable from the background in matplotlib>=2.0 . The association submodule was renamed to frequent_patterns .","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_19","text":"The DataFrame index of apriori results are now unique and ordered. Fixed typos in autompg and wine datasets (via James Bourbeau ).","title":"Bug Fixes"},{"location":"CHANGELOG/#version-051-2017-02-14","text":"","title":"Version 0.5.1 (2017-02-14)"},{"location":"CHANGELOG/#downloads_23","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_18","text":"The EnsembleVoteClassifier has a new refit attribute that prevents refitting classifiers if refit=False to save computational time. Added a new lift_score function in evaluate to compute lift score (via Batuhan Bardak ). StackingClassifier and StackingRegressor support multivariate targets if the underlying models do (via kernc ). StackingClassifier has a new use_features_in_secondary attribute like StackingCVClassifier .","title":"New Features"},{"location":"CHANGELOG/#changes_23","text":"Changed default verbosity level in SequentialFeatureSelector to 0 The EnsembleVoteClassifier now raises a NotFittedError if the estimator wasn't fit before calling predict . (via Anton Loss ) Added new TensorFlow variable initialization syntax to guarantee compatibility with TensorFlow 1.0","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_20","text":"Fixed wrong default value for k_features in SequentialFeatureSelector Cast selected feature subsets in the SequentialFeautureSelector as sets to prevent the iterator from getting stuck if the k_idx are different permutations of the same combination (via Zac Wellmer ). Fixed an issue with learning curves that caused the performance metrics to be reversed (via ipashchenko ) Fixed a bug that could occur in the SequentialFeatureSelector if there are similarly-well performing subsets in the floating variants (via Zac Wellmer ).","title":"Bug Fixes"},{"location":"CHANGELOG/#version-050-2016-11-09","text":"","title":"Version 0.5.0 (2016-11-09)"},{"location":"CHANGELOG/#downloads_24","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_19","text":"New ExhaustiveFeatureSelector estimator in mlxtend.feature_selection for evaluating all feature combinations in a specified range The StackingClassifier has a new parameter average_probas that is set to True by default to maintain the current behavior. A deprecation warning was added though, and it will default to False in future releases (0.6.0); average_probas=False will result in stacking of the level-1 predicted probabilities rather than averaging these. New StackingCVClassifier estimator in 'mlxtend.classifier' for implementing a stacking ensemble that uses cross-validation techniques for training the meta-estimator to avoid overfitting ( Reiichiro Nakano ) New OnehotTransactions encoder class added to the preprocessing submodule for transforming transaction data into a one-hot encoded array The SequentialFeatureSelector estimator in mlxtend.feature_selection now is safely stoppable mid-process by control+c, and deprecated print_progress in favor of a more tunable verbose parameter ( Will McGinnis ) New apriori function in association to extract frequent itemsets from transaction data for association rule mining New checkerboard_plot function in plotting to plot checkerboard tables / heat maps New mcnemar_table and mcnemar functions in evaluate to compute 2x2 contingency tables and McNemar's test","title":"New Features"},{"location":"CHANGELOG/#changes_24","text":"All plotting functions have been moved to mlxtend.plotting for compatibility reasons with continuous integration services and to make the installation of matplotlib optional for users of mlxtend 's core functionality Added a compatibility layer for scikit-learn 0.18 using the new model_selection module while maintaining backwards compatibility to scikit-learn 0.17.","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_21","text":"mlxtend.plotting.plot_decision_regions now draws decision regions correctly if more than 4 class labels are present Raise AttributeError in plot_decision_regions when the X_higlight argument is a 1D array ( chkoar )","title":"Bug Fixes"},{"location":"CHANGELOG/#version-042-2016-08-24","text":"","title":"Version 0.4.2 (2016-08-24)"},{"location":"CHANGELOG/#downloads_25","text":"Source code (zip) Source code (tar.gz) PDF documentation","title":"Downloads"},{"location":"CHANGELOG/#new-features_20","text":"Added preprocessing.CopyTransformer , a mock class that returns copies of imput arrays via transform and fit_transform","title":"New Features"},{"location":"CHANGELOG/#changes_25","text":"Added AppVeyor to CI to ensure MS Windows compatibility Dataset are now saved as compressed .txt or .csv files rather than being imported as Python objects feature_selection.SequentialFeatureSelector now supports the selection of k_features using a tuple to specify a \"min-max\" k_features range Added \"SVD solver\" option to the PrincipalComponentAnalysis Raise a AttributeError with \"not fitted\" message in SequentialFeatureSelector if transform or get_metric_dict are called prior to fit Use small, positive bias units in TfMultiLayerPerceptron 's hidden layer(s) if the activations are ReLUs in order to avoid dead neurons Added an optional clone_estimator parameter to the SequentialFeatureSelector that defaults to True , avoiding the modification of the original estimator objects More rigorous type and shape checks in the evaluate.plot_decision_regions function DenseTransformer now doesn't raise and error if the input array is not sparse API clean-up using scikit-learn's BaseEstimator as parent class for feature_selection.ColumnSelector","title":"Changes"},{"location":"CHANGELOG/#bug-fixes_22","text":"Fixed a problem when a tuple-range was provided as argument to the SequentialFeatureSelector 's k_features parameter and the scoring metric was more negative than -1 (e.g., as in scikit-learn's MSE scoring function) (wahutch](https://github.com/wahutch)) Fixed an AttributeError issue when verbose > 1 in StackingClassifier Fixed a bug in classifier.SoftmaxRegression where the mean values of the offsets were used to update the bias units rather than their sum Fixed rare bug in MLP _layer_mapping functions that caused a swap between the random number generation seed when initializing weights and biases","title":"Bug Fixes"},{"location":"CHANGELOG/#version-041-2016-05-01","text":"","title":"Version 0.4.1 (2016-05-01)"},{"location":"CHANGELOG/#downloads_26","text":"Source code (zip) Source code (tar.gz) PDF documentation","title":"Downloads"},{"location":"CHANGELOG/#new-features_21","text":"New TensorFlow estimator for Linear Regression ( tf_regressor.TfLinearRegression ) New k-means clustering estimator ( cluster.Kmeans ) New TensorFlow k-means clustering estimator ( tf_cluster.Kmeans )","title":"New Features"},{"location":"CHANGELOG/#changes_26","text":"Due to refactoring of the estimator classes, the init_weights parameter of the fit methods was globally renamed to init_params Overall performance improvements of estimators due to code clean-up and refactoring Added several additional checks for correct array types and more meaningful exception messages Added optional dropout to the tf_classifier.TfMultiLayerPerceptron classifier for regularization Added an optional decay parameter to the tf_classifier.TfMultiLayerPerceptron classifier for adaptive learning via an exponential decay of the learning rate eta Replaced old NeuralNetMLP by more streamlined MultiLayerPerceptron ( classifier.MultiLayerPerceptron ); now also with softmax in the output layer and categorical cross-entropy loss. Unified init_params parameter for fit functions to continue training where the algorithm left off (if supported)","title":"Changes"},{"location":"CHANGELOG/#version-040-2016-04-09","text":"","title":"Version 0.4.0 (2016-04-09)"},{"location":"CHANGELOG/#new-features_22","text":"New TfSoftmaxRegression classifier using Tensorflow ( tf_classifier.TfSoftmaxRegression ) New SoftmaxRegression classifier ( classifier.SoftmaxRegression ) New TfMultiLayerPerceptron classifier using Tensorflow ( tf_classifier.TfMultiLayerPerceptron ) New StackingRegressor ( regressor.StackingRegressor ) New StackingClassifier ( classifier.StackingClassifier ) New function for one-hot encoding of class labels ( preprocessing.one_hot ) Added GridSearch support to the SequentialFeatureSelector ( feature_selection/.SequentialFeatureSelector ) evaluate.plot_decision_regions improvements: Function now handles class y-class labels correctly if array is of type float Correct handling of input arguments markers and colors Accept an existing Axes via the ax argument New print_progress parameter for all generalized models and multi-layer neural networks for printing time elapsed, ETA, and the current cost of the current epoch Minibatch learning for classifier.LogisticRegression , classifier.Adaline , and regressor.LinearRegression plus streamlined API New Principal Component Analysis class via mlxtend.feature_extraction.PrincipalComponentAnalysis New RBF Kernel Principal Component Analysis class via mlxtend.feature_extraction.RBFKernelPCA New Linear Discriminant Analysis class via mlxtend.feature_extraction.LinearDiscriminantAnalysis","title":"New Features"},{"location":"CHANGELOG/#changes_27","text":"The column parameter in mlxtend.preprocessing.standardize now defaults to None to standardize all columns more conveniently","title":"Changes"},{"location":"CHANGELOG/#version-030-2016-01-31","text":"","title":"Version 0.3.0 (2016-01-31)"},{"location":"CHANGELOG/#downloads_27","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_23","text":"Added a progress bar tracker to classifier.NeuralNetMLP Added a function to score predicted vs. target class labels evaluate.scoring Added confusion matrix functions to create ( evaluate.confusion_matrix ) and plot ( evaluate.plot_confusion_matrix ) confusion matrices New style parameter and improved axis scaling in mlxtend.evaluate.plot_learning_curves Added loadlocal_mnist to mlxtend.data for streaming MNIST from a local byte files into numpy arrays New NeuralNetMLP parameters: random_weights , shuffle_init , shuffle_epoch New SFS features such as the generation of pandas DataFrame results tables and plotting functions (with confidence intervals, standard deviation, and standard error bars) Added support for regression estimators in SFS Added Boston housing dataset New shuffle parameter for classifier.NeuralNetMLP","title":"New Features"},{"location":"CHANGELOG/#changes_28","text":"The mlxtend.preprocessing.standardize function now optionally returns the parameters, which are estimated from the array, for re-use. A further improvement makes the standardize function smarter in order to avoid zero-division errors Cosmetic improvements to the evaluate.plot_decision_regions function such as hiding plot axes Renaming of classifier.EnsembleClassfier to classifier.EnsembleVoteClassifier Improved random weight initialization in Perceptron , Adaline , LinearRegression , and LogisticRegression Changed learning parameter of mlxtend.classifier.Adaline to solver and added \"normal equation\" as closed-form solution solver Hide y-axis labels in mlxtend.evaluate.plot_decision_regions in 1 dimensional evaluations Sequential Feature Selection algorithms were unified into a single SequentialFeatureSelector class with parameters to enable floating selection and toggle between forward and backward selection. Stratified sampling of MNIST (now 500x random samples from each of the 10 digit categories) Renaming mlxtend.plotting to mlxtend.general_plotting in order to distinguish general plotting function from specialized utility function such as evaluate.plot_decision_regions","title":"Changes"},{"location":"CHANGELOG/#version-029-2015-07-14","text":"","title":"Version 0.2.9 (2015-07-14)"},{"location":"CHANGELOG/#downloads_28","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features_24","text":"Sequential Feature Selection algorithms: SFS, SFFS, SBS, and SFBS","title":"New Features"},{"location":"CHANGELOG/#changes_29","text":"Changed regularization & lambda parameters in LogisticRegression to single parameter l2_lambda","title":"Changes"},{"location":"CHANGELOG/#version-028-2015-06-27","text":"API changes: mlxtend.sklearn.EnsembleClassifier -> mlxtend.classifier.EnsembleClassifier mlxtend.sklearn.ColumnSelector -> mlxtend.feature_selection.ColumnSelector mlxtend.sklearn.DenseTransformer -> mlxtend.preprocessing.DenseTransformer mlxtend.pandas.standardizing -> mlxtend.preprocessing.standardizing mlxtend.pandas.minmax_scaling -> mlxtend.preprocessing.minmax_scaling mlxtend.matplotlib -> mlxtend.plotting Added momentum learning parameter (alpha coefficient) to mlxtend.classifier.NeuralNetMLP . Added adaptive learning rate (decrease constant) to mlxtend.classifier.NeuralNetMLP . mlxtend.pandas.minmax_scaling became mlxtend.preprocessing.minmax_scaling and also supports NumPy arrays now mlxtend.pandas.standardizing became mlxtend.preprocessing.standardizing and now supports both NumPy arrays and pandas DataFrames; also, now ddof parameters to set the degrees of freedom when calculating the standard deviation","title":"Version 0.2.8 (2015-06-27)"},{"location":"CHANGELOG/#version-027-2015-06-20","text":"Added multilayer perceptron (feedforward artificial neural network) classifier as mlxtend.classifier.NeuralNetMLP . Added 5000 labeled trainingsamples from the MNIST handwritten digits dataset to mlxtend.data","title":"Version 0.2.7 (2015-06-20)"},{"location":"CHANGELOG/#version-026-2015-05-08","text":"Added ordinary least square regression using different solvers (gradient and stochastic gradient descent, and the closed form solution (normal equation) Added option for random weight initialization to logistic regression classifier and updated l2 regularization Added wine dataset to mlxtend.data Added invert_axes parameter mlxtend.matplotlib.enrichtment_plot to optionally plot the \"Count\" on the x-axis New verbose parameter for mlxtend.sklearn.EnsembleClassifier by Alejandro C. Bahnsen Added mlxtend.pandas.standardizing to standardize columns in a Pandas DataFrame Added parameters linestyles and markers to mlxtend.matplotlib.enrichment_plot mlxtend.regression.lin_regplot automatically adds np.newaxis and works w. python lists Added tokenizers: mlxtend.text.extract_emoticons and mlxtend.text.extract_words_and_emoticons","title":"Version 0.2.6 (2015-05-08)"},{"location":"CHANGELOG/#version-025-2015-04-17","text":"Added Sequential Backward Selection (mlxtend.sklearn.SBS) Added X_highlight parameter to mlxtend.evaluate.plot_decision_regions for highlighting test data points. Added mlxtend.regression.lin_regplot to plot the fitted line from linear regression. Added mlxtend.matplotlib.stacked_barplot to conveniently produce stacked barplots using pandas DataFrame s. Added mlxtend.matplotlib.enrichment_plot","title":"Version 0.2.5 (2015-04-17)"},{"location":"CHANGELOG/#version-024-2015-03-15","text":"Added scoring to mlxtend.evaluate.learning_curves (by user pfsq) Fixed setup.py bug caused by the missing README.html file matplotlib.category_scatter for pandas DataFrames and Numpy arrays","title":"Version 0.2.4 (2015-03-15)"},{"location":"CHANGELOG/#version-023-2015-03-11","text":"Added Logistic regression Gradient descent and stochastic gradient descent perceptron was changed to Adaline (Adaptive Linear Neuron) Perceptron and Adaline for {0, 1} classes Added mlxtend.preprocessing.shuffle_arrays_unison function to shuffle one or more NumPy arrays. Added shuffle and random seed parameter to stochastic gradient descent classifier. Added rstrip parameter to mlxtend.file_io.find_filegroups to allow trimming of base names. Added ignore_substring parameter to mlxtend.file_io.find_filegroups and find_files . Replaced .rstrip in mlxtend.file_io.find_filegroups with more robust regex. Gridsearch support for mlxtend.sklearn.EnsembleClassifier","title":"Version 0.2.3 (2015-03-11)"},{"location":"CHANGELOG/#version-022-2015-03-01","text":"Improved robustness of EnsembleClassifier. Extended plot_decision_regions() functionality for plotting 1D decision boundaries. Function matplotlib.plot_decision_regions was reorganized to evaluate.plot_decision_regions . evaluate.plot_learning_curves() function added. Added Rosenblatt, gradient descent, and stochastic gradient descent perceptrons.","title":"Version 0.2.2 (2015-03-01)"},{"location":"CHANGELOG/#version-021-2015-01-20","text":"Added mlxtend.pandas.minmax_scaling - a function to rescale pandas DataFrame columns. Slight update to the EnsembleClassifier interface (additional voting parameter) Fixed EnsembleClassifier to return correct class labels if class labels are not integers from 0 to n. Added new matplotlib function to plot decision regions of classifiers.","title":"Version 0.2.1 (2015-01-20)"},{"location":"CHANGELOG/#version-020-2015-01-13","text":"Improved mlxtend.text.generalize_duplcheck to remove duplicates and prevent endless looping issue. Added recursive search parameter to mlxtend.file_io.find_files. Added check_ext parameter mlxtend.file_io.find_files to search based on file extensions. Default parameter to ignore invisible files for mlxtend.file_io.find. Added transform and fit_transform to the EnsembleClassifier . Added mlxtend.file_io.find_filegroups function.","title":"Version 0.2.0 (2015-01-13)"},{"location":"CHANGELOG/#version-019-2015-01-10","text":"Implemented scikit-learn EnsembleClassifier (majority voting rule) class.","title":"Version 0.1.9 (2015-01-10)"},{"location":"CHANGELOG/#version-018-2015-01-07","text":"Improvements to mlxtend.text.generalize_names to handle certain Dutch last name prefixes (van, van der, de, etc.). Added mlxtend.text.generalize_name_duplcheck function to apply mlxtend.text.generalize_names function to a pandas DataFrame without creating duplicates.","title":"Version 0.1.8 (2015-01-07)"},{"location":"CHANGELOG/#version-017-2015-01-07","text":"Added text utilities with name generalization function. Added and file_io utilities.","title":"Version 0.1.7 (2015-01-07)"},{"location":"CHANGELOG/#version-016-2015-01-04","text":"Added combinations and permutations estimators.","title":"Version 0.1.6 (2015-01-04)"},{"location":"CHANGELOG/#version-015-2014-12-11","text":"Added DenseTransformer for pipelines and grid search.","title":"Version 0.1.5 (2014-12-11)"},{"location":"CHANGELOG/#version-014-2014-08-20","text":"mean_centering function is now a Class that creates MeanCenterer objects that can be used to fit data via the fit method, and center data at the column means via the transform and fit_transform method.","title":"Version 0.1.4 (2014-08-20)"},{"location":"CHANGELOG/#version-013-2014-08-19","text":"Added preprocessing module and mean_centering function.","title":"Version 0.1.3 (2014-08-19)"},{"location":"CHANGELOG/#version-012-2014-08-19","text":"Added matplotlib utilities and remove_borders function.","title":"Version 0.1.2 (2014-08-19)"},{"location":"CHANGELOG/#version-011-2014-08-13","text":"Simplified code for ColumnSelector.","title":"Version 0.1.1 (2014-08-13)"},{"location":"CONTRIBUTING/","text":"How to Contribute I would be very happy about any kind of contributions that help to improve and extend the functionality of mlxtend. Quick Contributor Checklist This is a quick checklist about the different steps of a typical contribution to mlxtend (and other open source projects). Consider copying this list to a local text file (or the issue tracker) and checking off items as you go. 1) Making and testing code changes: [ ] Open a new \"issue\" on GitHub to discuss the new feature / bug fix [ ] Fork the mlxtend repository from GitHub (if not already done earlier) [ ] Create and check out a new topic branch (please don't make modifications in the master branch) [ ] Implement the new feature or apply the bug-fix [ ] Add appropriate unit test functions in mlxtend/*/tests [ ] Run PYTHONPATH='.' pytest ./mlxtend -sv and make sure that all unit tests pass [ ] Make sure the newly implemented feature has good test coverage: python -m pip install coverage # test all: # coverage run --source=mlxtend --branch -m pytest . coverage run --source=mlxtend --branch -m pytest mlxtend/<insert_path> coverage html [ ] Modify documentation in the appropriate location under mlxtend/docs/sources/ [ ] Add a note about the modification/contribution to the ./docs/sources/changelog.md file 2) Checking code style: When you check in a PR, mlxtend will run code style checks via flak8 and black. To make the contributor experience easier, we recommend you check the code style locally before pushing it to the repository. This way it is less likely that the automated checkers will complain and prompt you to make fixes. There are two ways you can do this: Option A : Running the tools manually [ ] Check for style issues by running flake8 ./mlxtend (you may want to run pytest again after you made modifications to the code) [ ] We recommend using black to format the code automatically according to recommended style changes. After installing black , you can do this via black [source_file_or_directory] [ ] Run isort which will sort the imports alphabetically. We recommend the following command: isort -p mlxtend --line-length 88 --multi-line 3 --profile black mypythonfile.py Option B : Using pre-commit hooks (recommended) The pre-commit hooks for mlxtend will check your code via flake8 , black , and isort automatically before you make a git commit . You can read more about pre-commit hooks here . [ ] Install the pre-commit package via pip install pre-commit . [ ] In the mlxtend folder, run pre-commit install (you only have to do it once). 3) Submitting your code [ ] Push the topic branch to the server and create a pull request. [ ] Check the automated tests passed. [ ] The automatic PEP8 / black integrations may prompt you to modify the code stylistically. It would be nice if you could apply the suggested changes. Tips for Contributors Getting Started - Creating a New Issue and Forking the Repository If you don't have a GitHub account, yet, please create one to contribute to this project. Please submit a ticket for your issue to discuss the fix or new feature before too much time and effort is spent for the implementation. Fork the mlxtend repository from the GitHub web interface. Clone the mlxtend repository to your local machine by executing git clone https://github.com/<your_username>/mlxtend.git Syncing an Existing Fork If you already forked mlxtend earlier, you can bring you \"Fork\" up to date with the master branch as follows: 1. Configuring a remote that points to the upstream repository on GitHub List the current configured remote repository of your fork by executing $ git remote -v If you see something like origin https://github.com/<your username>/mlxtend.git (fetch) origin https://github.com/<your username>/mlxtend.git (push) you need to specify a new remote upstream repository via $ git remote add upstream https://github.com/rasbt/mlxtend.git Now, verify the new upstream repository you've specified for your fork by executing $ git remote -v You should see following output if everything is configured correctly: origin https://github.com/<your username>/mlxtend.git (fetch) origin https://github.com/<your username>/mlxtend.git (push) upstream https://github.com/rasbt/mlxtend.git (fetch) upstream https://github.com/rasbt/mlxtend.git (push) 2. Syncing your Fork First, fetch the updates of the original project's master branch by executing: $ git fetch upstream You should see the following output remote: Counting objects: xx, done. remote: Compressing objects: 100% (xx/xx), done. remote: Total xx (delta xx), reused xx (delta x) Unpacking objects: 100% (xx/xx), done. From https://github.com/rasbt/mlxtend * [new branch] master -> upstream/master This means that the commits to the rasbt/mlxtend master branch are now stored in the local branch upstream/master . If you are not already on your local project's master branch, execute $ git checkout master Finally, merge the changes in upstream/master to your local master branch by executing $ git merge upstream/master which will give you an output that looks similar to Updating xxx...xxx Fast-forward SOME FILE1 | 12 +++++++ SOME FILE2 | 10 +++++++ 2 files changed, 22 insertions(+), *The Main Workflow - Making Changes in a New Topic Branch Listed below are the 9 typical steps of a contribution. 1. Discussing the Feature or Modification Before you start coding, please discuss the new feature, bugfix, or other modification to the project on the project's issue tracker . Before you open a \"new issue,\" please do a quick search to see if a similar issue has been submitted already. 2. Creating a new feature branch Please avoid working directly on the master branch but create a new feature branch: $ git branch <new_feature> Switch to the new feature branch by executing $ git checkout <new_feature> 3. Developing the new feature / bug fix Now it's time to modify existing code or to contribute new code to the project. 4. Testing your code Add the respective unit tests and check if they pass: $ PYTHONPATH='.' pytest ./mlxtend ---with-coverage 5. Documenting changes Please add an entry to the mlxtend/docs/sources/changelog.md file. If it is a new feature, it would also be nice if you could update the documentation in appropriate location in mlxtend/sources . 6. Committing changes When you are ready to commit the changes, please provide a meaningful commit message: $ git add <modifies_files> # or `git add .` $ git commit -m '<meaningful commit message>' 7. Optional: squashing commits If you made multiple smaller commits, it would be nice if you could group them into a larger, summarizing commit. First, list your recent commit via Note Due to the improved GitHub UI, this is no longer necessary/encouraged. $ git log which will list the commits from newest to oldest in the following format by default: commit 046e3af8a9127df8eac879454f029937c8a31c41 Author: rasbt <mail@sebastianraschka.com> Date: Tue Nov 24 03:46:37 2015 -0500 fixed setup.py commit c3c00f6ba0e8f48bbe1c9081b8ae3817e57ecc5c Author: rasbt <mail@sebastianraschka.com> Date: Tue Nov 24 03:04:39 2015 -0500 documented feature x commit d87934fe8726c46f0b166d6290a3bf38915d6e75 Author: rasbt <mail@sebastianraschka.com> Date: Tue Nov 24 02:44:45 2015 -0500 added support for feature x Assuming that it would make sense to group these 3 commits into one, we can execute $ git rebase -i HEAD~3 which will bring our default git editor with the following contents: pick d87934f added support for feature x pick c3c00f6 documented feature x pick 046e3af fixed setup.py Since c3c00f6 and 046e3af are related to the original commit of feature x , let's keep the d87934f and squash the 2 following commits into this initial one by changes the lines to pick d87934f added support for feature x squash c3c00f6 documented feature x squash 046e3af fixed setup.py Now, save the changes in your editor. Now, quitting the editor will apply the rebase changes, and the editor will open a second time, prompting you to enter a new commit message. In this case, we could enter support for feature x to summarize the contributions. 8. Uploading changes Push your changes to a topic branch to the git server by executing: $ git push origin <feature_branch> 9. Submitting a pull request Go to your GitHub repository online, select the new feature branch, and submit a new pull request: Notes for Developers Building the documentation The documentation is built via MkDocs ; to ensure that the documentation is rendered correctly, you can view the documentation locally by executing mkdocs serve from the mlxtend/docs directory. For example, ~/github/mlxtend/docs$ mkdocs serve 1. Building the API documentation To build the API documentation, navigate to mlxtend/docs and execute the make_api.py file from this directory via ~/github/mlxtend/docs$ python make_api.py This should place the API documentation into the correct directories into the two directories: mlxtend/docs/sources/api_modules mlxtend/docs/sources/api_subpackes 2. Editing the User Guide The documents containing code examples for the \"User Guide\" are generated from IPython Notebook files. In order to convert a IPython notebook file to markdown after editing, please follow the following steps: Modify or edit the existing notebook. Execute all cells in the current notebook and make sure that no errors occur. Convert the notebook to markdown using the ipynb2markdown.py converter ~/github/mlxtend/docs$ python ipynb2markdown.py --ipynb ./sources/user_guide/subpackage/notebookname.ipynb Note If you are adding a new document, please also include it in the pages section in the mlxtend/docs/mkdocs.yml file. 3. Building static HTML files of the documentation First, please check the documenation via localhost (https://127.0.0.1:8000/): ~/github/mlxtend/docs$ mkdocs serve Next, build the static HTML files of the mlxtend documentation via ~/github/mlxtend/docs$ mkdocs build --clean To deploy the documentation, execute ~/github/mlxtend/docs$ mkdocs gh-deploy --clean 4. Generate a PDF of the documentation To generate a PDF version of the documentation, simply cd into the mlxtend/docs directory and execute: python md2pdf.py Uploading a new version to PyPI 1. Creating a new testing environment Assuming we are using conda , create a new python environment via $ conda create -n 'mlxtend-testing' python=3 numpy scipy pandas Next, activate the environment by executing $ source activate mlxtend-testing 2. Installing the package from local files Test the installation by executing the following from within the mlxtend main directory: $ pip install . -e the --record files.txt flag will create a files.txt file listing the locations where these files will be installed. Try to import the package to see if it works, for example, by executing $ python -c 'import mlxtend; print(mlxtend.__file__)' If everything seems to be fine, remove the installation via $ cat files.txt | xargs rm -rf ; rm files.txt Next, test if pip is able to install the packages. First, navigate to a different directory, and from there, install the package: $ pip uninstall $ pip install mlxtend and uninstall it again $ pip uninstall mlxtend 3. Deploying the package Consider deploying the package to the PyPI test server first. The setup instructions can be found here . First , install Twine if you don't have it already installed. E.g., use the following to install all recommended packages: $ python -m pip install twine build Second , create the distribution. This by default creates an sdist and wheel in the ./dist directory. $ python -m build Install the wheel and sdist to make sure they work. The distributions file names will change with each version. python -m pip install ./dist/mlxtend-0.23.0.dev0.tar.gz --force-reinstall python -m pip install ./dist/mlxtend-0.23.0.dev0-py3-none-any.whl --force-reinstall python -m pip uninstall mlxtend Third , upload the packages to the test server: $ twine upload --repository-url https://upload.pypi.org/legacy dist/ Then, install it and see if it works: $ pip install -i https://testpypi.python.org/pypi mlxtend Next, uninstall it again as follows: $ pip uninstall mlxtend Fourth , after this dry-run succeeded, repeat this process using the \"real\" PyPI: $ python -m twine upload dist/* 4. Removing the virtual environment Finally, to cleanup our local drive, remove the virtual testing environment via $ conda remove --name 'mlxtend-testing' --all Note : if you get an error like HTTPError: 403 Forbidden from https://upload.pypi.org/legacy/ make sure you have an up to date version of twine installed (helped me to uninstall in conda and install via pip.) 5. Updating the conda-forge recipe Once a new version of mlxtend has been uploaded to PyPI, update the conda-forge build recipe at https://github.com/conda-forge/mlxtend-feedstock by changing the version number in the recipe/meta.yaml file appropriately.","title":"How To Contribute"},{"location":"CONTRIBUTING/#how-to-contribute","text":"I would be very happy about any kind of contributions that help to improve and extend the functionality of mlxtend.","title":"How to Contribute"},{"location":"CONTRIBUTING/#quick-contributor-checklist","text":"This is a quick checklist about the different steps of a typical contribution to mlxtend (and other open source projects). Consider copying this list to a local text file (or the issue tracker) and checking off items as you go.","title":"Quick Contributor Checklist"},{"location":"CONTRIBUTING/#1-making-and-testing-code-changes","text":"[ ] Open a new \"issue\" on GitHub to discuss the new feature / bug fix [ ] Fork the mlxtend repository from GitHub (if not already done earlier) [ ] Create and check out a new topic branch (please don't make modifications in the master branch) [ ] Implement the new feature or apply the bug-fix [ ] Add appropriate unit test functions in mlxtend/*/tests [ ] Run PYTHONPATH='.' pytest ./mlxtend -sv and make sure that all unit tests pass [ ] Make sure the newly implemented feature has good test coverage: python -m pip install coverage # test all: # coverage run --source=mlxtend --branch -m pytest . coverage run --source=mlxtend --branch -m pytest mlxtend/<insert_path> coverage html [ ] Modify documentation in the appropriate location under mlxtend/docs/sources/ [ ] Add a note about the modification/contribution to the ./docs/sources/changelog.md file","title":"1) Making and testing code changes:"},{"location":"CONTRIBUTING/#2-checking-code-style","text":"When you check in a PR, mlxtend will run code style checks via flak8 and black. To make the contributor experience easier, we recommend you check the code style locally before pushing it to the repository. This way it is less likely that the automated checkers will complain and prompt you to make fixes. There are two ways you can do this: Option A : Running the tools manually [ ] Check for style issues by running flake8 ./mlxtend (you may want to run pytest again after you made modifications to the code) [ ] We recommend using black to format the code automatically according to recommended style changes. After installing black , you can do this via black [source_file_or_directory] [ ] Run isort which will sort the imports alphabetically. We recommend the following command: isort -p mlxtend --line-length 88 --multi-line 3 --profile black mypythonfile.py Option B : Using pre-commit hooks (recommended) The pre-commit hooks for mlxtend will check your code via flake8 , black , and isort automatically before you make a git commit . You can read more about pre-commit hooks here . [ ] Install the pre-commit package via pip install pre-commit . [ ] In the mlxtend folder, run pre-commit install (you only have to do it once).","title":"2) Checking code style:"},{"location":"CONTRIBUTING/#3-submitting-your-code","text":"[ ] Push the topic branch to the server and create a pull request. [ ] Check the automated tests passed. [ ] The automatic PEP8 / black integrations may prompt you to modify the code stylistically. It would be nice if you could apply the suggested changes.","title":"3) Submitting your code"},{"location":"CONTRIBUTING/#tips-for-contributors","text":"","title":"Tips for Contributors"},{"location":"CONTRIBUTING/#getting-started-creating-a-new-issue-and-forking-the-repository","text":"If you don't have a GitHub account, yet, please create one to contribute to this project. Please submit a ticket for your issue to discuss the fix or new feature before too much time and effort is spent for the implementation. Fork the mlxtend repository from the GitHub web interface. Clone the mlxtend repository to your local machine by executing git clone https://github.com/<your_username>/mlxtend.git","title":"Getting Started - Creating a New Issue and Forking the Repository"},{"location":"CONTRIBUTING/#syncing-an-existing-fork","text":"If you already forked mlxtend earlier, you can bring you \"Fork\" up to date with the master branch as follows:","title":"Syncing an Existing Fork"},{"location":"CONTRIBUTING/#1-configuring-a-remote-that-points-to-the-upstream-repository-on-github","text":"List the current configured remote repository of your fork by executing $ git remote -v If you see something like origin https://github.com/<your username>/mlxtend.git (fetch) origin https://github.com/<your username>/mlxtend.git (push) you need to specify a new remote upstream repository via $ git remote add upstream https://github.com/rasbt/mlxtend.git Now, verify the new upstream repository you've specified for your fork by executing $ git remote -v You should see following output if everything is configured correctly: origin https://github.com/<your username>/mlxtend.git (fetch) origin https://github.com/<your username>/mlxtend.git (push) upstream https://github.com/rasbt/mlxtend.git (fetch) upstream https://github.com/rasbt/mlxtend.git (push)","title":"1. Configuring a remote that points to the upstream repository on GitHub"},{"location":"CONTRIBUTING/#2-syncing-your-fork","text":"First, fetch the updates of the original project's master branch by executing: $ git fetch upstream You should see the following output remote: Counting objects: xx, done. remote: Compressing objects: 100% (xx/xx), done. remote: Total xx (delta xx), reused xx (delta x) Unpacking objects: 100% (xx/xx), done. From https://github.com/rasbt/mlxtend * [new branch] master -> upstream/master This means that the commits to the rasbt/mlxtend master branch are now stored in the local branch upstream/master . If you are not already on your local project's master branch, execute $ git checkout master Finally, merge the changes in upstream/master to your local master branch by executing $ git merge upstream/master which will give you an output that looks similar to Updating xxx...xxx Fast-forward SOME FILE1 | 12 +++++++ SOME FILE2 | 10 +++++++ 2 files changed, 22 insertions(+),","title":"2. Syncing your Fork"},{"location":"CONTRIBUTING/#the-main-workflow-making-changes-in-a-new-topic-branch","text":"Listed below are the 9 typical steps of a contribution.","title":"*The Main Workflow - Making Changes in a New Topic Branch"},{"location":"CONTRIBUTING/#1-discussing-the-feature-or-modification","text":"Before you start coding, please discuss the new feature, bugfix, or other modification to the project on the project's issue tracker . Before you open a \"new issue,\" please do a quick search to see if a similar issue has been submitted already.","title":"1. Discussing the Feature or Modification"},{"location":"CONTRIBUTING/#2-creating-a-new-feature-branch","text":"Please avoid working directly on the master branch but create a new feature branch: $ git branch <new_feature> Switch to the new feature branch by executing $ git checkout <new_feature>","title":"2. Creating a new feature branch"},{"location":"CONTRIBUTING/#3-developing-the-new-feature-bug-fix","text":"Now it's time to modify existing code or to contribute new code to the project.","title":"3. Developing the new feature / bug fix"},{"location":"CONTRIBUTING/#4-testing-your-code","text":"Add the respective unit tests and check if they pass: $ PYTHONPATH='.' pytest ./mlxtend ---with-coverage","title":"4. Testing your code"},{"location":"CONTRIBUTING/#5-documenting-changes","text":"Please add an entry to the mlxtend/docs/sources/changelog.md file. If it is a new feature, it would also be nice if you could update the documentation in appropriate location in mlxtend/sources .","title":"5. Documenting changes"},{"location":"CONTRIBUTING/#6-committing-changes","text":"When you are ready to commit the changes, please provide a meaningful commit message: $ git add <modifies_files> # or `git add .` $ git commit -m '<meaningful commit message>'","title":"6. Committing changes"},{"location":"CONTRIBUTING/#7-optional-squashing-commits","text":"If you made multiple smaller commits, it would be nice if you could group them into a larger, summarizing commit. First, list your recent commit via Note Due to the improved GitHub UI, this is no longer necessary/encouraged. $ git log which will list the commits from newest to oldest in the following format by default: commit 046e3af8a9127df8eac879454f029937c8a31c41 Author: rasbt <mail@sebastianraschka.com> Date: Tue Nov 24 03:46:37 2015 -0500 fixed setup.py commit c3c00f6ba0e8f48bbe1c9081b8ae3817e57ecc5c Author: rasbt <mail@sebastianraschka.com> Date: Tue Nov 24 03:04:39 2015 -0500 documented feature x commit d87934fe8726c46f0b166d6290a3bf38915d6e75 Author: rasbt <mail@sebastianraschka.com> Date: Tue Nov 24 02:44:45 2015 -0500 added support for feature x Assuming that it would make sense to group these 3 commits into one, we can execute $ git rebase -i HEAD~3 which will bring our default git editor with the following contents: pick d87934f added support for feature x pick c3c00f6 documented feature x pick 046e3af fixed setup.py Since c3c00f6 and 046e3af are related to the original commit of feature x , let's keep the d87934f and squash the 2 following commits into this initial one by changes the lines to pick d87934f added support for feature x squash c3c00f6 documented feature x squash 046e3af fixed setup.py Now, save the changes in your editor. Now, quitting the editor will apply the rebase changes, and the editor will open a second time, prompting you to enter a new commit message. In this case, we could enter support for feature x to summarize the contributions.","title":"7. Optional: squashing commits"},{"location":"CONTRIBUTING/#8-uploading-changes","text":"Push your changes to a topic branch to the git server by executing: $ git push origin <feature_branch>","title":"8. Uploading changes"},{"location":"CONTRIBUTING/#9-submitting-a-pull-request","text":"Go to your GitHub repository online, select the new feature branch, and submit a new pull request:","title":"9. Submitting a pull request"},{"location":"CONTRIBUTING/#notes-for-developers","text":"","title":"Notes for Developers"},{"location":"CONTRIBUTING/#building-the-documentation","text":"The documentation is built via MkDocs ; to ensure that the documentation is rendered correctly, you can view the documentation locally by executing mkdocs serve from the mlxtend/docs directory. For example, ~/github/mlxtend/docs$ mkdocs serve","title":"Building the documentation"},{"location":"CONTRIBUTING/#1-building-the-api-documentation","text":"To build the API documentation, navigate to mlxtend/docs and execute the make_api.py file from this directory via ~/github/mlxtend/docs$ python make_api.py This should place the API documentation into the correct directories into the two directories: mlxtend/docs/sources/api_modules mlxtend/docs/sources/api_subpackes","title":"1. Building the API documentation"},{"location":"CONTRIBUTING/#2-editing-the-user-guide","text":"The documents containing code examples for the \"User Guide\" are generated from IPython Notebook files. In order to convert a IPython notebook file to markdown after editing, please follow the following steps: Modify or edit the existing notebook. Execute all cells in the current notebook and make sure that no errors occur. Convert the notebook to markdown using the ipynb2markdown.py converter ~/github/mlxtend/docs$ python ipynb2markdown.py --ipynb ./sources/user_guide/subpackage/notebookname.ipynb Note If you are adding a new document, please also include it in the pages section in the mlxtend/docs/mkdocs.yml file.","title":"2. Editing the User Guide"},{"location":"CONTRIBUTING/#3-building-static-html-files-of-the-documentation","text":"First, please check the documenation via localhost (https://127.0.0.1:8000/): ~/github/mlxtend/docs$ mkdocs serve Next, build the static HTML files of the mlxtend documentation via ~/github/mlxtend/docs$ mkdocs build --clean To deploy the documentation, execute ~/github/mlxtend/docs$ mkdocs gh-deploy --clean","title":"3. Building static HTML files of the documentation"},{"location":"CONTRIBUTING/#4-generate-a-pdf-of-the-documentation","text":"To generate a PDF version of the documentation, simply cd into the mlxtend/docs directory and execute: python md2pdf.py","title":"4. Generate a PDF of the documentation"},{"location":"CONTRIBUTING/#uploading-a-new-version-to-pypi","text":"","title":"Uploading a new version to PyPI"},{"location":"CONTRIBUTING/#1-creating-a-new-testing-environment","text":"Assuming we are using conda , create a new python environment via $ conda create -n 'mlxtend-testing' python=3 numpy scipy pandas Next, activate the environment by executing $ source activate mlxtend-testing","title":"1. Creating a new testing environment"},{"location":"CONTRIBUTING/#2-installing-the-package-from-local-files","text":"Test the installation by executing the following from within the mlxtend main directory: $ pip install . -e the --record files.txt flag will create a files.txt file listing the locations where these files will be installed. Try to import the package to see if it works, for example, by executing $ python -c 'import mlxtend; print(mlxtend.__file__)' If everything seems to be fine, remove the installation via $ cat files.txt | xargs rm -rf ; rm files.txt Next, test if pip is able to install the packages. First, navigate to a different directory, and from there, install the package: $ pip uninstall $ pip install mlxtend and uninstall it again $ pip uninstall mlxtend","title":"2. Installing the package from local files"},{"location":"CONTRIBUTING/#3-deploying-the-package","text":"Consider deploying the package to the PyPI test server first. The setup instructions can be found here . First , install Twine if you don't have it already installed. E.g., use the following to install all recommended packages: $ python -m pip install twine build Second , create the distribution. This by default creates an sdist and wheel in the ./dist directory. $ python -m build Install the wheel and sdist to make sure they work. The distributions file names will change with each version. python -m pip install ./dist/mlxtend-0.23.0.dev0.tar.gz --force-reinstall python -m pip install ./dist/mlxtend-0.23.0.dev0-py3-none-any.whl --force-reinstall python -m pip uninstall mlxtend Third , upload the packages to the test server: $ twine upload --repository-url https://upload.pypi.org/legacy dist/ Then, install it and see if it works: $ pip install -i https://testpypi.python.org/pypi mlxtend Next, uninstall it again as follows: $ pip uninstall mlxtend Fourth , after this dry-run succeeded, repeat this process using the \"real\" PyPI: $ python -m twine upload dist/*","title":"3. Deploying the package"},{"location":"CONTRIBUTING/#4-removing-the-virtual-environment","text":"Finally, to cleanup our local drive, remove the virtual testing environment via $ conda remove --name 'mlxtend-testing' --all Note : if you get an error like HTTPError: 403 Forbidden from https://upload.pypi.org/legacy/ make sure you have an up to date version of twine installed (helped me to uninstall in conda and install via pip.)","title":"4. Removing the virtual environment"},{"location":"CONTRIBUTING/#5-updating-the-conda-forge-recipe","text":"Once a new version of mlxtend has been uploaded to PyPI, update the conda-forge build recipe at https://github.com/conda-forge/mlxtend-feedstock by changing the version number in the recipe/meta.yaml file appropriately.","title":"5. Updating the conda-forge recipe"},{"location":"Code-of-Conduct/","text":"Contributor Covenant Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity, and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language; Being respectful of differing viewpoints and experiences; Gracefully accepting constructive criticism; Focusing on what is best for the community; Showing empathy towards other community members. Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances; Trolling, gaslighting, insulting/derogatory comments, and personal or political attacks; Public or private harassment; Publishing others' private information, such as a physical or electronic address, without explicit permission; Other conduct which could reasonably be considered inappropriate in a professional setting. Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within the project and public spaces when an individual represents the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or other unacceptable behavior may be reported by contacting the project team at mail@sebastianraschka.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality concerning the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"Code-of-Conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"Code-of-Conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to make participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity, and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"Code-of-Conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language; Being respectful of differing viewpoints and experiences; Gracefully accepting constructive criticism; Focusing on what is best for the community; Showing empathy towards other community members. Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances; Trolling, gaslighting, insulting/derogatory comments, and personal or political attacks; Public or private harassment; Publishing others' private information, such as a physical or electronic address, without explicit permission; Other conduct which could reasonably be considered inappropriate in a professional setting.","title":"Our Standards"},{"location":"Code-of-Conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"Code-of-Conduct/#scope","text":"This Code of Conduct applies both within the project and public spaces when an individual represents the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"Code-of-Conduct/#enforcement","text":"Instances of abusive, harassing, or other unacceptable behavior may be reported by contacting the project team at mail@sebastianraschka.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality concerning the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"Code-of-Conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"USER-GUIDE-INDEX/","text":"User Guide Index classifier Adaline EnsembleVoteClassifier LogisticRegression MultiLayerPerceptron OneRClassifier Perceptron SoftmaxRegression StackingClassifier StackingCVClassifier cluster Kmeans data autompg_data boston_housing_data iris_data loadlocal_mnist make_multiplexer_dataset mnist_data three_blobs_data wine_data evaluate accuracy_score bias_variance_decomp bootstrap bootstrap_point632_score BootstrapOutOfBag cochrans_q combined_ftest_5x2cv confusion_matrix create_counterfactual feature_importance_permutation ftest lift_score mcnemar_table mcnemar_tables mcnemar paired_ttest_5x2cv paired_ttest_kfold_cv paired_ttest_resampled permutation_test PredefinedHoldoutSplit proportion_difference RandomHoldoutSplit scoring feature_extraction LinearDiscriminantAnalysis PrincipalComponentAnalysis RBFKernelPCA feature_selection ColumnSelector ExhaustiveFeatureSelector SequentialFeatureSelector file_io find_filegroups find_files frequent_patterns apriori association_rules fpgrowth fpmax math num_combinations num_permutations plotting category_scatter checkerboard_plot ecdf enrichment_plot heatmap plot_confusion_matrix plot_pca_correlation_graph plot_decision_regions plot_learning_curves plot_linear_regression plot_sequential_feature_selection scatterplotmatrix stacked_barplot preprocessing CopyTransformer DenseTransformer MeanCenterer minmax_scaling one-hot_encoding shuffle_arrays_unison standardize TransactionEncoder regressor LinearRegression StackingCVRegressor StackingRegressor text generalize_names generalize_names_duplcheck tokenizer utils Counter","title":"User Guide Index"},{"location":"USER-GUIDE-INDEX/#user-guide-index","text":"","title":"User Guide Index"},{"location":"USER-GUIDE-INDEX/#classifier","text":"Adaline EnsembleVoteClassifier LogisticRegression MultiLayerPerceptron OneRClassifier Perceptron SoftmaxRegression StackingClassifier StackingCVClassifier","title":"classifier"},{"location":"USER-GUIDE-INDEX/#cluster","text":"Kmeans","title":"cluster"},{"location":"USER-GUIDE-INDEX/#data","text":"autompg_data boston_housing_data iris_data loadlocal_mnist make_multiplexer_dataset mnist_data three_blobs_data wine_data","title":"data"},{"location":"USER-GUIDE-INDEX/#evaluate","text":"accuracy_score bias_variance_decomp bootstrap bootstrap_point632_score BootstrapOutOfBag cochrans_q combined_ftest_5x2cv confusion_matrix create_counterfactual feature_importance_permutation ftest lift_score mcnemar_table mcnemar_tables mcnemar paired_ttest_5x2cv paired_ttest_kfold_cv paired_ttest_resampled permutation_test PredefinedHoldoutSplit proportion_difference RandomHoldoutSplit scoring","title":"evaluate"},{"location":"USER-GUIDE-INDEX/#feature_extraction","text":"LinearDiscriminantAnalysis PrincipalComponentAnalysis RBFKernelPCA","title":"feature_extraction"},{"location":"USER-GUIDE-INDEX/#feature_selection","text":"ColumnSelector ExhaustiveFeatureSelector SequentialFeatureSelector","title":"feature_selection"},{"location":"USER-GUIDE-INDEX/#file_io","text":"find_filegroups find_files","title":"file_io"},{"location":"USER-GUIDE-INDEX/#frequent_patterns","text":"apriori association_rules fpgrowth fpmax","title":"frequent_patterns"},{"location":"USER-GUIDE-INDEX/#math","text":"num_combinations num_permutations","title":"math"},{"location":"USER-GUIDE-INDEX/#plotting","text":"category_scatter checkerboard_plot ecdf enrichment_plot heatmap plot_confusion_matrix plot_pca_correlation_graph plot_decision_regions plot_learning_curves plot_linear_regression plot_sequential_feature_selection scatterplotmatrix stacked_barplot","title":"plotting"},{"location":"USER-GUIDE-INDEX/#preprocessing","text":"CopyTransformer DenseTransformer MeanCenterer minmax_scaling one-hot_encoding shuffle_arrays_unison standardize TransactionEncoder","title":"preprocessing"},{"location":"USER-GUIDE-INDEX/#regressor","text":"LinearRegression StackingCVRegressor StackingRegressor","title":"regressor"},{"location":"USER-GUIDE-INDEX/#text","text":"generalize_names generalize_names_duplcheck tokenizer","title":"text"},{"location":"USER-GUIDE-INDEX/#utils","text":"Counter","title":"utils"},{"location":"USER_GUIDE_INDEX/","text":"User Guide Index classifier Adaline EnsembleVoteClassifier LogisticRegression MultiLayerPerceptron OneRClassifier Perceptron SoftmaxRegression StackingClassifier StackingCVClassifier cluster Kmeans data autompg_data boston_housing_data iris_data loadlocal_mnist make_multiplexer_dataset mnist_data three_blobs_data wine_data evaluate accuracy_score bias_variance_decomp bootstrap bootstrap_point632_score BootstrapOutOfBag cochrans_q combined_ftest_5x2cv confusion_matrix create_counterfactual feature_importance_permutation ftest GroupTimeSeriesSplit lift_score mcnemar_table mcnemar_tables mcnemar paired_ttest_5x2cv paired_ttest_kfold_cv paired_ttest_resampled permutation_test PredefinedHoldoutSplit proportion_difference RandomHoldoutSplit scoring feature_extraction LinearDiscriminantAnalysis PrincipalComponentAnalysis RBFKernelPCA feature_selection ColumnSelector ExhaustiveFeatureSelector SequentialFeatureSelector file_io find_filegroups find_files frequent_patterns apriori association_rules fpgrowth fpmax math num_combinations num_permutations plotting category_scatter checkerboard_plot ecdf enrichment_plot heatmap plot_confusion_matrix plot_pca_correlation_graph plot_decision_regions plot_learning_curves plot_linear_regression plot_sequential_feature_selection scatterplotmatrix scatter_hist stacked_barplot preprocessing CopyTransformer DenseTransformer MeanCenterer minmax_scaling one-hot_encoding shuffle_arrays_unison standardize TransactionEncoder regressor LinearRegression StackingCVRegressor StackingRegressor text generalize_names generalize_names_duplcheck tokenizer utils Counter","title":"User Guide Index"},{"location":"USER_GUIDE_INDEX/#user-guide-index","text":"","title":"User Guide Index"},{"location":"USER_GUIDE_INDEX/#classifier","text":"Adaline EnsembleVoteClassifier LogisticRegression MultiLayerPerceptron OneRClassifier Perceptron SoftmaxRegression StackingClassifier StackingCVClassifier","title":"classifier"},{"location":"USER_GUIDE_INDEX/#cluster","text":"Kmeans","title":"cluster"},{"location":"USER_GUIDE_INDEX/#data","text":"autompg_data boston_housing_data iris_data loadlocal_mnist make_multiplexer_dataset mnist_data three_blobs_data wine_data","title":"data"},{"location":"USER_GUIDE_INDEX/#evaluate","text":"accuracy_score bias_variance_decomp bootstrap bootstrap_point632_score BootstrapOutOfBag cochrans_q combined_ftest_5x2cv confusion_matrix create_counterfactual feature_importance_permutation ftest GroupTimeSeriesSplit lift_score mcnemar_table mcnemar_tables mcnemar paired_ttest_5x2cv paired_ttest_kfold_cv paired_ttest_resampled permutation_test PredefinedHoldoutSplit proportion_difference RandomHoldoutSplit scoring","title":"evaluate"},{"location":"USER_GUIDE_INDEX/#feature_extraction","text":"LinearDiscriminantAnalysis PrincipalComponentAnalysis RBFKernelPCA","title":"feature_extraction"},{"location":"USER_GUIDE_INDEX/#feature_selection","text":"ColumnSelector ExhaustiveFeatureSelector SequentialFeatureSelector","title":"feature_selection"},{"location":"USER_GUIDE_INDEX/#file_io","text":"find_filegroups find_files","title":"file_io"},{"location":"USER_GUIDE_INDEX/#frequent_patterns","text":"apriori association_rules fpgrowth fpmax","title":"frequent_patterns"},{"location":"USER_GUIDE_INDEX/#math","text":"num_combinations num_permutations","title":"math"},{"location":"USER_GUIDE_INDEX/#plotting","text":"category_scatter checkerboard_plot ecdf enrichment_plot heatmap plot_confusion_matrix plot_pca_correlation_graph plot_decision_regions plot_learning_curves plot_linear_regression plot_sequential_feature_selection scatterplotmatrix scatter_hist stacked_barplot","title":"plotting"},{"location":"USER_GUIDE_INDEX/#preprocessing","text":"CopyTransformer DenseTransformer MeanCenterer minmax_scaling one-hot_encoding shuffle_arrays_unison standardize TransactionEncoder","title":"preprocessing"},{"location":"USER_GUIDE_INDEX/#regressor","text":"LinearRegression StackingCVRegressor StackingRegressor","title":"regressor"},{"location":"USER_GUIDE_INDEX/#text","text":"generalize_names generalize_names_duplcheck tokenizer","title":"text"},{"location":"USER_GUIDE_INDEX/#utils","text":"Counter","title":"utils"},{"location":"cite/","text":"Citing mlxtend If you use mlxtend as part of your workflow in a scientific publication, please consider citing the mlxtend repository with the following DOI: Raschka, Sebastian (2018) MLxtend: Providing machine learning and data science utilities and extensions to Python's scientific computing stack . J Open Source Softw 3(24). @article{raschkas_2018_mlxtend, author = {Sebastian Raschka}, title = {MLxtend: Providing machine learning and data science utilities and extensions to Python\u2019s scientific computing stack}, journal = {The Journal of Open Source Software}, volume = {3}, number = {24}, month = apr, year = 2018, publisher = {The Open Journal}, doi = {10.21105/joss.00638}, url = {https://joss.theoj.org/papers/10.21105/joss.00638} }","title":"Citing Mlxtend"},{"location":"cite/#citing-mlxtend","text":"If you use mlxtend as part of your workflow in a scientific publication, please consider citing the mlxtend repository with the following DOI: Raschka, Sebastian (2018) MLxtend: Providing machine learning and data science utilities and extensions to Python's scientific computing stack . J Open Source Softw 3(24). @article{raschkas_2018_mlxtend, author = {Sebastian Raschka}, title = {MLxtend: Providing machine learning and data science utilities and extensions to Python\u2019s scientific computing stack}, journal = {The Journal of Open Source Software}, volume = {3}, number = {24}, month = apr, year = 2018, publisher = {The Open Journal}, doi = {10.21105/joss.00638}, url = {https://joss.theoj.org/papers/10.21105/joss.00638} }","title":"Citing mlxtend"},{"location":"contributors/","text":"Contributors For the current list of contributors to mlxtend, please see the GitHub contributor page at https://github.com/rasbt/mlxtend/graphs/contributors .","title":"Contributors"},{"location":"contributors/#contributors","text":"For the current list of contributors to mlxtend, please see the GitHub contributor page at https://github.com/rasbt/mlxtend/graphs/contributors .","title":"Contributors"},{"location":"discuss/","text":"Discuss Any questions or comments about mlxtend? Join the mlxtend mailing list on Google Groups!","title":"Discuss"},{"location":"discuss/#discuss","text":"Any questions or comments about mlxtend? Join the mlxtend mailing list on Google Groups!","title":"Discuss"},{"location":"installation/","text":"Installing mlxtend PyPI To install mlxtend, just execute pip install mlxtend Alternatively, you download the package manually from the Python Package Index https://pypi.python.org/pypi/mlxtend , unzip it, navigate into the package, and use the following command from inside the mlxtend folder: pip install . Upgrading via pip To upgrade an existing version of mlxtend from PyPI, execute pip install mlxtend --upgrade --no-deps Please note that the dependencies (NumPy and SciPy) will also be upgraded if you omit the --no-deps flag; use the --no-deps (\"no dependencies\") flag if you don't want this. Installing mlxtend from the source distribution In rare cases, users reported problems on certain systems with the default pip installation command, which installs mlxtend from the binary distribution (\"wheels\") on PyPI. If you should encounter similar problems, you could try to install mlxtend from the source distribution instead via pip install --no-binary :all: mlxtend Also, I would appreciate it if you could report any issues that occur when using pip install mlxtend in hope that we can fix these in future releases. Conda The mlxtend package is also available through conda forge . To install mlxtend using conda, use the following command: conda install mlxtend --channel conda-forge or simply conda install mlxtend if you added conda-forge to your channels ( conda config --add channels conda-forge ). Dev Version The mlxtend version on PyPI may always one step behind; you can install the latest development version from the GitHub repository by executing pip install git+git://github.com/rasbt/mlxtend.git Or, you can fork the GitHub repository from https://github.com/rasbt/mlxtend and install mlxtend from your local drive via pip install .","title":"Installation"},{"location":"installation/#installing-mlxtend","text":"","title":"Installing mlxtend"},{"location":"installation/#pypi","text":"To install mlxtend, just execute pip install mlxtend Alternatively, you download the package manually from the Python Package Index https://pypi.python.org/pypi/mlxtend , unzip it, navigate into the package, and use the following command from inside the mlxtend folder: pip install .","title":"PyPI"},{"location":"installation/#upgrading-via-pip","text":"To upgrade an existing version of mlxtend from PyPI, execute pip install mlxtend --upgrade --no-deps Please note that the dependencies (NumPy and SciPy) will also be upgraded if you omit the --no-deps flag; use the --no-deps (\"no dependencies\") flag if you don't want this.","title":"Upgrading via pip"},{"location":"installation/#installing-mlxtend-from-the-source-distribution","text":"In rare cases, users reported problems on certain systems with the default pip installation command, which installs mlxtend from the binary distribution (\"wheels\") on PyPI. If you should encounter similar problems, you could try to install mlxtend from the source distribution instead via pip install --no-binary :all: mlxtend Also, I would appreciate it if you could report any issues that occur when using pip install mlxtend in hope that we can fix these in future releases.","title":"Installing mlxtend from the source distribution"},{"location":"installation/#conda","text":"The mlxtend package is also available through conda forge . To install mlxtend using conda, use the following command: conda install mlxtend --channel conda-forge or simply conda install mlxtend if you added conda-forge to your channels ( conda config --add channels conda-forge ).","title":"Conda"},{"location":"installation/#dev-version","text":"The mlxtend version on PyPI may always one step behind; you can install the latest development version from the GitHub repository by executing pip install git+git://github.com/rasbt/mlxtend.git Or, you can fork the GitHub repository from https://github.com/rasbt/mlxtend and install mlxtend from your local drive via pip install .","title":"Dev Version"},{"location":"license/","text":"This project is released under a permissive new BSD open source license and commercially usable. There is no warranty; not even for merchantability or fitness for a particular purpose. In addition, you may use, copy, modify, and redistribute all artistic creative works (figures and images) included in this distribution under the directory according to the terms and conditions of the Creative Commons Attribution 4.0 International License. (Computer-generated graphics such as the plots produced by matplotlib fall under the BSD license mentioned above). new BSD License New BSD License Copyright (c) 2014-2024, Sebastian Raschka. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of mlxtend nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. Creative Commons Attribution 4.0 International License mlxtend documentation figures are licensed under a Creative Commons Attribution 4.0 International License. https://creativecommons.org/licenses/by-sa/4.0/ . You are free to: Share \u2014 copy and redistribute the material in any medium or format Adapt \u2014 remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.","title":"License"},{"location":"license/#new-bsd-license","text":"New BSD License Copyright (c) 2014-2024, Sebastian Raschka. All rights reserved. Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met: Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. Neither the name of mlxtend nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission. THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","title":"new BSD License"},{"location":"license/#creative-commons-attribution-40-international-license","text":"mlxtend documentation figures are licensed under a Creative Commons Attribution 4.0 International License. https://creativecommons.org/licenses/by-sa/4.0/ .","title":"Creative Commons Attribution 4.0 International License"},{"location":"license/#you-are-free-to","text":"Share \u2014 copy and redistribute the material in any medium or format Adapt \u2014 remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms.","title":"You are free to:"},{"location":"license/#under-the-following-terms","text":"Attribution \u2014 You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. No additional restrictions \u2014 You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits.","title":"Under the following terms:"},{"location":"api_modules/mlxtend.classifier/Adaline/","text":"Adaline Adaline(eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0) ADAptive LInear NEuron classifier. Note that this implementation of Adaline expects binary class labels in {0, 1}. Parameters eta : float (default: 0.01) solver rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. minibatches : int (default: None) The number of minibatches for gradient-based optimization. If None: Normal Equations (closed-form solution) If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent (SGD) online learning If 1 < minibatches < len(y): SGD Minibatch learning random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr if not solver='normal equation' 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list Sum of squared errors after each epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/Adaline/ Methods fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Adaline"},{"location":"api_modules/mlxtend.classifier/Adaline/#adaline","text":"Adaline(eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0) ADAptive LInear NEuron classifier. Note that this implementation of Adaline expects binary class labels in {0, 1}. Parameters eta : float (default: 0.01) solver rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. minibatches : int (default: None) The number of minibatches for gradient-based optimization. If None: Normal Equations (closed-form solution) If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent (SGD) online learning If 1 < minibatches < len(y): SGD Minibatch learning random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr if not solver='normal equation' 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list Sum of squared errors after each epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/Adaline/","title":"Adaline"},{"location":"api_modules/mlxtend.classifier/Adaline/#methods","text":"fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Methods"},{"location":"api_modules/mlxtend.classifier/EnsembleVoteClassifier/","text":"EnsembleVoteClassifier EnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0, use_clones=True, fit_base_estimators=True) Soft Voting/Majority Rule classifier for scikit-learn estimators. Parameters clfs : array-like, shape = [n_classifiers] A list of classifiers. Invoking the fit method on the VotingClassifier will fit clones of those original classifiers be stored in the class attribute if use_clones=True (default) and fit_base_estimators=True (default). voting : str, {'hard', 'soft'} (default='hard') If 'hard', uses predicted class labels for majority rule voting. Else if 'soft', predicts the class label based on the argmax of the sums of the predicted probalities, which is recommended for an ensemble of well-calibrated classifiers. weights : array-like, shape = [n_classifiers], optional (default= None ) Sequence of weights ( float or int ) to weight the occurances of predicted class labels ( hard voting) or class probabilities before averaging ( soft voting). Uses uniform weights if None . verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the clf being fitted - verbose=2 : Prints info about the parameters of the clf being fitted - verbose>2 : Changes verbose param of the underlying clf to self.verbose - 2 use_clones : bool (default: True) Clones the classifiers for stacking classification if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Hence, if use_clones=True, the original input classifiers will remain unmodified upon using the StackingClassifier's fit method. Setting use_clones=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. fit_base_estimators : bool (default: True) Refits classifiers in clfs if True; uses references to the clfs , otherwise (assumes that the classifiers were already fit). Note: fit_base_estimators=False will enforce use_clones to be False, and is incompatible to most scikit-learn wrappers! For instance, if any form of cross-validation is performed this would require the re-fitting classifiers to training folds, which would raise a NotFitterError if fit_base_estimators=False. (New in mlxtend v0.6.) Attributes classes_ : array-like, shape = [n_predictions] clf : array-like, shape = [n_predictions] The input classifiers; may be overwritten if use_clones=False clf_ : array-like, shape = [n_predictions] Fitted input classifiers; clones if use_clones=True Examples ``` >>> import numpy as np >>> from sklearn.linear_model import LogisticRegression >>> from sklearn.naive_bayes import GaussianNB >>> from sklearn.ensemble import RandomForestClassifier >>> from mlxtend.sklearn import EnsembleVoteClassifier >>> clf1 = LogisticRegression(random_seed=1) >>> clf2 = RandomForestClassifier(random_seed=1) >>> clf3 = GaussianNB() >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> y = np.array([1, 1, 1, 2, 2, 2]) >>> eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], ... voting='hard', verbose=1) >>> eclf1 = eclf1.fit(X, y) >>> print(eclf1.predict(X)) [1 1 1 2 2 2] >>> eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft') >>> eclf2 = eclf2.fit(X, y) >>> print(eclf2.predict(X)) [1 1 1 2 2 2] >>> eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], ... voting='soft', weights=[2,1,1]) >>> eclf3 = eclf3.fit(X, y) >>> print(eclf3.predict(X)) [1 1 1 2 2 2] >>> For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/ ``` Methods fit(X, y, sample_weight=None) Learn weight coefficients from training data for each classifier. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict class labels for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns maj : array-like, shape = [n_samples] Predicted class labels. predict_proba(X) Predict class probabilities for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns avg : array-like, shape = [n_samples, n_classes] Weighted average probability for each class per sample. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_fit_request(self: mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. set_score_request(self: mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object. transform(X) Return class labels or probabilities for X for each estimator. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns If voting='soft'`` : array-like = [n_classifiers, n_samples, n_classes] Class probabilties calculated by each classifier. If voting='hard'`` : array-like = [n_classifiers, n_samples] Class labels predicted by each classifier.","title":"EnsembleVoteClassifier"},{"location":"api_modules/mlxtend.classifier/EnsembleVoteClassifier/#ensemblevoteclassifier","text":"EnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0, use_clones=True, fit_base_estimators=True) Soft Voting/Majority Rule classifier for scikit-learn estimators. Parameters clfs : array-like, shape = [n_classifiers] A list of classifiers. Invoking the fit method on the VotingClassifier will fit clones of those original classifiers be stored in the class attribute if use_clones=True (default) and fit_base_estimators=True (default). voting : str, {'hard', 'soft'} (default='hard') If 'hard', uses predicted class labels for majority rule voting. Else if 'soft', predicts the class label based on the argmax of the sums of the predicted probalities, which is recommended for an ensemble of well-calibrated classifiers. weights : array-like, shape = [n_classifiers], optional (default= None ) Sequence of weights ( float or int ) to weight the occurances of predicted class labels ( hard voting) or class probabilities before averaging ( soft voting). Uses uniform weights if None . verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the clf being fitted - verbose=2 : Prints info about the parameters of the clf being fitted - verbose>2 : Changes verbose param of the underlying clf to self.verbose - 2 use_clones : bool (default: True) Clones the classifiers for stacking classification if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Hence, if use_clones=True, the original input classifiers will remain unmodified upon using the StackingClassifier's fit method. Setting use_clones=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. fit_base_estimators : bool (default: True) Refits classifiers in clfs if True; uses references to the clfs , otherwise (assumes that the classifiers were already fit). Note: fit_base_estimators=False will enforce use_clones to be False, and is incompatible to most scikit-learn wrappers! For instance, if any form of cross-validation is performed this would require the re-fitting classifiers to training folds, which would raise a NotFitterError if fit_base_estimators=False. (New in mlxtend v0.6.) Attributes classes_ : array-like, shape = [n_predictions] clf : array-like, shape = [n_predictions] The input classifiers; may be overwritten if use_clones=False clf_ : array-like, shape = [n_predictions] Fitted input classifiers; clones if use_clones=True Examples ``` >>> import numpy as np >>> from sklearn.linear_model import LogisticRegression >>> from sklearn.naive_bayes import GaussianNB >>> from sklearn.ensemble import RandomForestClassifier >>> from mlxtend.sklearn import EnsembleVoteClassifier >>> clf1 = LogisticRegression(random_seed=1) >>> clf2 = RandomForestClassifier(random_seed=1) >>> clf3 = GaussianNB() >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> y = np.array([1, 1, 1, 2, 2, 2]) >>> eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], ... voting='hard', verbose=1) >>> eclf1 = eclf1.fit(X, y) >>> print(eclf1.predict(X)) [1 1 1 2 2 2] >>> eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft') >>> eclf2 = eclf2.fit(X, y) >>> print(eclf2.predict(X)) [1 1 1 2 2 2] >>> eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], ... voting='soft', weights=[2,1,1]) >>> eclf3 = eclf3.fit(X, y) >>> print(eclf3.predict(X)) [1 1 1 2 2 2] >>> For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/ ```","title":"EnsembleVoteClassifier"},{"location":"api_modules/mlxtend.classifier/EnsembleVoteClassifier/#methods","text":"fit(X, y, sample_weight=None) Learn weight coefficients from training data for each classifier. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict class labels for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns maj : array-like, shape = [n_samples] Predicted class labels. predict_proba(X) Predict class probabilities for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns avg : array-like, shape = [n_samples, n_classes] Weighted average probability for each class per sample. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_fit_request(self: mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. set_score_request(self: mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object. transform(X) Return class labels or probabilities for X for each estimator. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns If voting='soft'`` : array-like = [n_classifiers, n_samples, n_classes] Class probabilties calculated by each classifier. If voting='hard'`` : array-like = [n_classifiers, n_samples] Class labels predicted by each classifier.","title":"Methods"},{"location":"api_modules/mlxtend.classifier/LogisticRegression/","text":"LogisticRegression LogisticRegression(eta=0.01, epochs=50, l2_lambda=0.0, minibatches=1, random_seed=None, print_progress=0) Logistic regression classifier. Note that this implementation of Logistic Regression expects binary class labels in {0, 1}. Parameters eta : float (default: 0.01) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. l2_lambda : float Regularization parameter for L2 regularization. No regularization if l2_lambda=0.0. minibatches : int (default: 1) The number of minibatches for gradient-based optimization. If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent (SGD) online learning If 1 < minibatches < len(y): SGD Minibatch learning random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list List of floats with cross_entropy cost (sgd or gd) for every epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/LogisticRegression/ Methods fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. predict_proba(X) Predict class probabilities of X from the net input. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns Class 1 probability : float score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"LogisticRegression"},{"location":"api_modules/mlxtend.classifier/LogisticRegression/#logisticregression","text":"LogisticRegression(eta=0.01, epochs=50, l2_lambda=0.0, minibatches=1, random_seed=None, print_progress=0) Logistic regression classifier. Note that this implementation of Logistic Regression expects binary class labels in {0, 1}. Parameters eta : float (default: 0.01) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. l2_lambda : float Regularization parameter for L2 regularization. No regularization if l2_lambda=0.0. minibatches : int (default: 1) The number of minibatches for gradient-based optimization. If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent (SGD) online learning If 1 < minibatches < len(y): SGD Minibatch learning random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list List of floats with cross_entropy cost (sgd or gd) for every epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/LogisticRegression/","title":"LogisticRegression"},{"location":"api_modules/mlxtend.classifier/LogisticRegression/#methods","text":"fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. predict_proba(X) Predict class probabilities of X from the net input. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns Class 1 probability : float score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Methods"},{"location":"api_modules/mlxtend.classifier/MultiLayerPerceptron/","text":"MultiLayerPerceptron MultiLayerPerceptron(eta=0.5, epochs=50, hidden_layers=[50], n_classes=None, momentum=0.0, l1=0.0, l2=0.0, dropout=1.0, decrease_const=0.0, minibatches=1, random_seed=None, print_progress=0) Multi-layer perceptron classifier with logistic sigmoid activations Parameters eta : float (default: 0.5) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. hidden_layers : list (default: [50]) Number of units per hidden layer. By default 50 units in the first hidden layer. At the moment only 1 hidden layer is supported n_classes : int (default: None) A positive integer to declare the number of class labels if not all class labels are present in a partial training set. Gets the number of class labels automatically if None. l1 : float (default: 0.0) L1 regularization strength l2 : float (default: 0.0) L2 regularization strength momentum : float (default: 0.0) Momentum constant. Factor multiplied with the gradient of the previous epoch t-1 to improve learning speed w(t) := w(t) - (grad(t) + momentum * grad(t-1)) decrease_const : float (default: 0.0) Decrease constant. Shrinks the learning rate after each epoch via eta / (1 + epoch*decrease_const) minibatches : int (default: 1) Divide the training data into k minibatches for accelerated stochastic gradient descent learning. Gradient Descent Learning if minibatches = 1 Stochastic Gradient Descent learning if minibatches = len(y) Minibatch learning if minibatches > 1 random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape=[n_features, n_classes] Weights after fitting. b_ : 1D-array, shape=[n_classes] Bias units after fitting. cost_ : list List of floats; the mean categorical cross entropy cost after each epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/MultiLayerPerceptron/ Methods fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. predict_proba(X) Predict class probabilities of X from the net input. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns Class probabilties : array-like, shape= [n_samples, n_classes] score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"MultiLayerPerceptron"},{"location":"api_modules/mlxtend.classifier/MultiLayerPerceptron/#multilayerperceptron","text":"MultiLayerPerceptron(eta=0.5, epochs=50, hidden_layers=[50], n_classes=None, momentum=0.0, l1=0.0, l2=0.0, dropout=1.0, decrease_const=0.0, minibatches=1, random_seed=None, print_progress=0) Multi-layer perceptron classifier with logistic sigmoid activations Parameters eta : float (default: 0.5) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. hidden_layers : list (default: [50]) Number of units per hidden layer. By default 50 units in the first hidden layer. At the moment only 1 hidden layer is supported n_classes : int (default: None) A positive integer to declare the number of class labels if not all class labels are present in a partial training set. Gets the number of class labels automatically if None. l1 : float (default: 0.0) L1 regularization strength l2 : float (default: 0.0) L2 regularization strength momentum : float (default: 0.0) Momentum constant. Factor multiplied with the gradient of the previous epoch t-1 to improve learning speed w(t) := w(t) - (grad(t) + momentum * grad(t-1)) decrease_const : float (default: 0.0) Decrease constant. Shrinks the learning rate after each epoch via eta / (1 + epoch*decrease_const) minibatches : int (default: 1) Divide the training data into k minibatches for accelerated stochastic gradient descent learning. Gradient Descent Learning if minibatches = 1 Stochastic Gradient Descent learning if minibatches = len(y) Minibatch learning if minibatches > 1 random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape=[n_features, n_classes] Weights after fitting. b_ : 1D-array, shape=[n_classes] Bias units after fitting. cost_ : list List of floats; the mean categorical cross entropy cost after each epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/MultiLayerPerceptron/","title":"MultiLayerPerceptron"},{"location":"api_modules/mlxtend.classifier/MultiLayerPerceptron/#methods","text":"fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. predict_proba(X) Predict class probabilities of X from the net input. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns Class probabilties : array-like, shape= [n_samples, n_classes] score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Methods"},{"location":"api_modules/mlxtend.classifier/OneRClassifier/","text":"OneRClassifier OneRClassifier(resolve_ties='first') OneR (One Rule) Classifier. Parameters resolve_ties : str (default: 'first') Option for how to resolve ties if two or more features have the same error. Options are - 'first' (default): chooses first feature in the list, i.e., feature with the lower column index. - 'chi-squared': performs a chi-squared test for each feature against the target and selects the feature with the lowest p-value. Attributes self.classes_labels_ : array-like, shape = [n_labels] Array containing the unique class labels found in the training set. self.feature_idx_ : int The index of the rules' feature based on the column in the training set. self.p_value_ : float The p value for a given feature. Only available after calling fit when the OneR attribute resolve_ties = 'chi-squared' is set. self.prediction_dict_ : dict Dictionary containing information about the feature's (self.feature_idx_) rules and total error. E.g., {'total error': 37, 'rules (value: class)': {0: 0, 1: 2}} means the total error is 37, and the rules are \"if feature value == 0 classify as 0\" and \"if feature value == 1 classify as 2\". (And classify as class 1 otherwise.) For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/OneRClassifier/ Methods fit(X, y) Learn rule from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns self : object get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. predict(X) Predict class labels for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns maj : array-like, shape = [n_samples] Predicted class labels. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. set_score_request(self: mlxtend.classifier.oner.OneRClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.oner.OneRClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object.","title":"OneRClassifier"},{"location":"api_modules/mlxtend.classifier/OneRClassifier/#onerclassifier","text":"OneRClassifier(resolve_ties='first') OneR (One Rule) Classifier. Parameters resolve_ties : str (default: 'first') Option for how to resolve ties if two or more features have the same error. Options are - 'first' (default): chooses first feature in the list, i.e., feature with the lower column index. - 'chi-squared': performs a chi-squared test for each feature against the target and selects the feature with the lowest p-value. Attributes self.classes_labels_ : array-like, shape = [n_labels] Array containing the unique class labels found in the training set. self.feature_idx_ : int The index of the rules' feature based on the column in the training set. self.p_value_ : float The p value for a given feature. Only available after calling fit when the OneR attribute resolve_ties = 'chi-squared' is set. self.prediction_dict_ : dict Dictionary containing information about the feature's (self.feature_idx_) rules and total error. E.g., {'total error': 37, 'rules (value: class)': {0: 0, 1: 2}} means the total error is 37, and the rules are \"if feature value == 0 classify as 0\" and \"if feature value == 1 classify as 2\". (And classify as class 1 otherwise.) For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/OneRClassifier/","title":"OneRClassifier"},{"location":"api_modules/mlxtend.classifier/OneRClassifier/#methods","text":"fit(X, y) Learn rule from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns self : object get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. predict(X) Predict class labels for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns maj : array-like, shape = [n_samples] Predicted class labels. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. set_score_request(self: mlxtend.classifier.oner.OneRClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.oner.OneRClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object.","title":"Methods"},{"location":"api_modules/mlxtend.classifier/Perceptron/","text":"Perceptron Perceptron(eta=0.1, epochs=50, random_seed=None, print_progress=0) Perceptron classifier. Note that this implementation of the Perceptron expects binary class labels in {0, 1}. Parameters eta : float (default: 0.1) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Number of passes over the training dataset. Prior to each epoch, the dataset is shuffled to prevent cycles. random_seed : int Random state for initializing random weights and shuffling. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list Number of misclassifications in every epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/Perceptron/ Methods fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Perceptron"},{"location":"api_modules/mlxtend.classifier/Perceptron/#perceptron","text":"Perceptron(eta=0.1, epochs=50, random_seed=None, print_progress=0) Perceptron classifier. Note that this implementation of the Perceptron expects binary class labels in {0, 1}. Parameters eta : float (default: 0.1) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Number of passes over the training dataset. Prior to each epoch, the dataset is shuffled to prevent cycles. random_seed : int Random state for initializing random weights and shuffling. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list Number of misclassifications in every epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/Perceptron/","title":"Perceptron"},{"location":"api_modules/mlxtend.classifier/Perceptron/#methods","text":"fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Methods"},{"location":"api_modules/mlxtend.classifier/SoftmaxRegression/","text":"SoftmaxRegression SoftmaxRegression(eta=0.01, epochs=50, l2=0.0, minibatches=1, n_classes=None, random_seed=None, print_progress=0) Softmax regression classifier. Parameters eta : float (default: 0.01) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. l2 : float Regularization parameter for L2 regularization. No regularization if l2=0.0. minibatches : int (default: 1) The number of minibatches for gradient-based optimization. If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent (SGD) online learning If 1 < minibatches < len(y): SGD Minibatch learning n_classes : int (default: None) A positive integer to declare the number of class labels if not all class labels are present in a partial training set. Gets the number of class labels automatically if None. random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list List of floats, the average cross_entropy for each epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/ Methods fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. predict_proba(X) Predict class probabilities of X from the net input. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns Class probabilties : array-like, shape= [n_samples, n_classes] score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"SoftmaxRegression"},{"location":"api_modules/mlxtend.classifier/SoftmaxRegression/#softmaxregression","text":"SoftmaxRegression(eta=0.01, epochs=50, l2=0.0, minibatches=1, n_classes=None, random_seed=None, print_progress=0) Softmax regression classifier. Parameters eta : float (default: 0.01) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. l2 : float Regularization parameter for L2 regularization. No regularization if l2=0.0. minibatches : int (default: 1) The number of minibatches for gradient-based optimization. If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent (SGD) online learning If 1 < minibatches < len(y): SGD Minibatch learning n_classes : int (default: None) A positive integer to declare the number of class labels if not all class labels are present in a partial training set. Gets the number of class labels automatically if None. random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list List of floats, the average cross_entropy for each epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/","title":"SoftmaxRegression"},{"location":"api_modules/mlxtend.classifier/SoftmaxRegression/#methods","text":"fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. predict_proba(X) Predict class probabilities of X from the net input. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns Class probabilties : array-like, shape= [n_samples, n_classes] score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Methods"},{"location":"api_modules/mlxtend.classifier/StackingCVClassifier/","text":"StackingCVClassifier StackingCVClassifier(classifiers, meta_classifier, use_probas=False, drop_proba_col=None, cv=2, shuffle=True, random_state=None, stratify=True, verbose=0, use_features_in_secondary=False, store_train_meta_features=False, use_clones=True, n_jobs=None, pre_dispatch='2 n_jobs')* A 'Stacking Cross-Validation' classifier for scikit-learn estimators. New in mlxtend v0.4.3 Parameters classifiers : array-like, shape = [n_classifiers] A list of classifiers. Invoking the fit method on the StackingCVClassifer will fit clones of these original classifiers that will be stored in the class attribute self.clfs_ if use_clones=True . meta_classifier : object The meta-classifier to be fitted on the ensemble of classifiers use_probas : bool (default: False) If True, trains meta-classifier based on predicted probabilities instead of class labels. drop_proba_col : string (default: None) Drops extra \"probability\" column in the feature set, because it is redundant: p(y_c) = 1 - p(y_1) + p(y_2) + ... + p(y_{c-1}). This can be useful for meta-classifiers that are sensitive to perfectly collinear features. If 'last', drops last probability column. If 'first', drops first probability column. Only relevant if use_probas=True . cv : int, cross-validation generator or an iterable, optional (default: 2) Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 2-fold cross validation, - integer, to specify the number of folds in a (Stratified)KFold , - An object to be used as a cross-validation generator. - An iterable yielding train, test splits. For integer/None inputs, it will use either a KFold or StratifiedKFold cross validation depending the value of stratify argument. shuffle : bool (default: True) If True, and the cv argument is integer, the training data will be shuffled at fitting stage prior to cross-validation. If the cv argument is a specific cross validation technique, this argument is omitted. random_state : int, RandomState instance or None, optional (default: None) Constrols the randomness of the cv splitter. Used when cv is integer and shuffle=True . New in v0.16.0. stratify : bool (default: True) If True, and the cv argument is integer it will follow a stratified K-Fold cross validation technique. If the cv argument is a specific cross validation technique, this argument is omitted. verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the regressor being fitted and which fold is currently being used for fitting - verbose=2 : Prints info about the parameters of the regressor being fitted - verbose>2 : Changes verbose param of the underlying regressor to self.verbose - 2 use_features_in_secondary : bool (default: False) If True, the meta-classifier will be trained both on the predictions of the original classifiers and the original dataset. If False, the meta-classifier will be trained only on the predictions of the original classifiers. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-classifier stored in the self.train_meta_features_ array, which can be accessed after calling fit . use_clones : bool (default: True) Clones the classifiers for stacking classification if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Hence, if use_clones=True, the original input classifiers will remain unmodified upon using the StackingCVClassifier's fit method. Setting use_clones=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. n_jobs : int or None, optional (default=None) The number of CPUs to use to do the computation. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. for more details. New in v0.16.0. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs - An int, giving the exact number of total jobs that are spawned - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' New in v0.16.0. Attributes clfs_ : list, shape=[n_classifiers] Fitted classifiers (clones of the original classifiers) meta_clf_ : estimator Fitted meta-classifier (clone of the original meta-estimator) train_meta_features : numpy array, shape = [n_samples, n_classifiers] meta-features for training data, where n_samples is the number of samples in training data and n_classifiers is the number of classfiers. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/ Methods decision_function(X) Predict class confidence scores for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns scores : shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes). Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. fit(X, y, groups=None, sample_weight=None) Fit ensemble classifers and the meta-classifier. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : numpy array, shape = [n_samples] Target values. groups : numpy array/None, shape = [n_samples] The group that each sample belongs to. This is used by specific folding strategies such as GroupKFold() sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict target values for X. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns labels : array-like, shape = [n_samples] Predicted class labels. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, n_classifiers] Returns the meta-features for test data. predict_proba(X) Predict class probabilities for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns proba : array-like, shape = [n_samples, n_classes] or a list of n_outputs of such arrays if n_outputs > 1. Probability for each class per sample. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_fit_request(self: mlxtend.classifier.stacking_cv_classification.StackingCVClassifier, , groups: Union[bool, NoneType, str] = ' UNCHANGED ', sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_cv_classification.StackingCVClassifier* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.classifier.stacking_cv_classification.StackingCVClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_cv_classification.StackingCVClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object. Properties named_classifiers None","title":"StackingCVClassifier"},{"location":"api_modules/mlxtend.classifier/StackingCVClassifier/#stackingcvclassifier","text":"StackingCVClassifier(classifiers, meta_classifier, use_probas=False, drop_proba_col=None, cv=2, shuffle=True, random_state=None, stratify=True, verbose=0, use_features_in_secondary=False, store_train_meta_features=False, use_clones=True, n_jobs=None, pre_dispatch='2 n_jobs')* A 'Stacking Cross-Validation' classifier for scikit-learn estimators. New in mlxtend v0.4.3 Parameters classifiers : array-like, shape = [n_classifiers] A list of classifiers. Invoking the fit method on the StackingCVClassifer will fit clones of these original classifiers that will be stored in the class attribute self.clfs_ if use_clones=True . meta_classifier : object The meta-classifier to be fitted on the ensemble of classifiers use_probas : bool (default: False) If True, trains meta-classifier based on predicted probabilities instead of class labels. drop_proba_col : string (default: None) Drops extra \"probability\" column in the feature set, because it is redundant: p(y_c) = 1 - p(y_1) + p(y_2) + ... + p(y_{c-1}). This can be useful for meta-classifiers that are sensitive to perfectly collinear features. If 'last', drops last probability column. If 'first', drops first probability column. Only relevant if use_probas=True . cv : int, cross-validation generator or an iterable, optional (default: 2) Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 2-fold cross validation, - integer, to specify the number of folds in a (Stratified)KFold , - An object to be used as a cross-validation generator. - An iterable yielding train, test splits. For integer/None inputs, it will use either a KFold or StratifiedKFold cross validation depending the value of stratify argument. shuffle : bool (default: True) If True, and the cv argument is integer, the training data will be shuffled at fitting stage prior to cross-validation. If the cv argument is a specific cross validation technique, this argument is omitted. random_state : int, RandomState instance or None, optional (default: None) Constrols the randomness of the cv splitter. Used when cv is integer and shuffle=True . New in v0.16.0. stratify : bool (default: True) If True, and the cv argument is integer it will follow a stratified K-Fold cross validation technique. If the cv argument is a specific cross validation technique, this argument is omitted. verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the regressor being fitted and which fold is currently being used for fitting - verbose=2 : Prints info about the parameters of the regressor being fitted - verbose>2 : Changes verbose param of the underlying regressor to self.verbose - 2 use_features_in_secondary : bool (default: False) If True, the meta-classifier will be trained both on the predictions of the original classifiers and the original dataset. If False, the meta-classifier will be trained only on the predictions of the original classifiers. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-classifier stored in the self.train_meta_features_ array, which can be accessed after calling fit . use_clones : bool (default: True) Clones the classifiers for stacking classification if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Hence, if use_clones=True, the original input classifiers will remain unmodified upon using the StackingCVClassifier's fit method. Setting use_clones=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. n_jobs : int or None, optional (default=None) The number of CPUs to use to do the computation. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. for more details. New in v0.16.0. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs - An int, giving the exact number of total jobs that are spawned - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' New in v0.16.0. Attributes clfs_ : list, shape=[n_classifiers] Fitted classifiers (clones of the original classifiers) meta_clf_ : estimator Fitted meta-classifier (clone of the original meta-estimator) train_meta_features : numpy array, shape = [n_samples, n_classifiers] meta-features for training data, where n_samples is the number of samples in training data and n_classifiers is the number of classfiers. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/","title":"StackingCVClassifier"},{"location":"api_modules/mlxtend.classifier/StackingCVClassifier/#methods","text":"decision_function(X) Predict class confidence scores for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns scores : shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes). Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. fit(X, y, groups=None, sample_weight=None) Fit ensemble classifers and the meta-classifier. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : numpy array, shape = [n_samples] Target values. groups : numpy array/None, shape = [n_samples] The group that each sample belongs to. This is used by specific folding strategies such as GroupKFold() sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict target values for X. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns labels : array-like, shape = [n_samples] Predicted class labels. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, n_classifiers] Returns the meta-features for test data. predict_proba(X) Predict class probabilities for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns proba : array-like, shape = [n_samples, n_classes] or a list of n_outputs of such arrays if n_outputs > 1. Probability for each class per sample. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_fit_request(self: mlxtend.classifier.stacking_cv_classification.StackingCVClassifier, , groups: Union[bool, NoneType, str] = ' UNCHANGED ', sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_cv_classification.StackingCVClassifier* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.classifier.stacking_cv_classification.StackingCVClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_cv_classification.StackingCVClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object.","title":"Methods"},{"location":"api_modules/mlxtend.classifier/StackingCVClassifier/#properties","text":"named_classifiers None","title":"Properties"},{"location":"api_modules/mlxtend.classifier/StackingClassifier/","text":"StackingClassifier StackingClassifier(classifiers, meta_classifier, use_probas=False, drop_proba_col=None, average_probas=False, verbose=0, use_features_in_secondary=False, store_train_meta_features=False, use_clones=True, fit_base_estimators=True) A Stacking classifier for scikit-learn estimators for classification. Parameters classifiers : array-like, shape = [n_classifiers] A list of classifiers. Invoking the fit method on the StackingClassifer will fit clones of these original classifiers that will be stored in the class attribute self.clfs_ if use_clones=True (default) and fit_base_estimators=True (default). meta_classifier : object The meta-classifier to be fitted on the ensemble of classifiers use_probas : bool (default: False) If True, trains meta-classifier based on predicted probabilities instead of class labels. drop_proba_col : string (default: None) Drops extra \"probability\" column in the feature set, because it is redundant: p(y_c) = 1 - p(y_1) + p(y_2) + ... + p(y_{c-1}). This can be useful for meta-classifiers that are sensitive to perfectly collinear features. If 'last', drops last probability column. If 'first', drops first probability column. Only relevant if use_probas=True . average_probas : bool (default: False) Averages the probabilities as meta features if True . Only relevant if use_probas=True . verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the regressor being fitted - verbose=2 : Prints info about the parameters of the regressor being fitted - verbose>2 : Changes verbose param of the underlying regressor to self.verbose - 2 use_features_in_secondary : bool (default: False) If True, the meta-classifier will be trained both on the predictions of the original classifiers and the original dataset. If False, the meta-classifier will be trained only on the predictions of the original classifiers. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-classifier stored in the self.train_meta_features_ array, which can be accessed after calling fit . use_clones : bool (default: True) Clones the classifiers for stacking classification if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Hence, if use_clones=True, the original input classifiers will remain unmodified upon using the StackingClassifier's fit method. Setting use_clones=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. fit_base_estimators: bool (default: True) Refits classifiers in classifiers if True; uses references to the classifiers , otherwise (assumes that the classifiers were already fit). Note: fit_base_estimators=False will enforce use_clones to be False, and is incompatible to most scikit-learn wrappers! For instance, if any form of cross-validation is performed this would require the re-fitting classifiers to training folds, which would raise a NotFitterError if fit_base_estimators=False. (New in mlxtend v0.6.) Attributes clfs_ : list, shape=[n_classifiers] Fitted classifiers (clones of the original classifiers) meta_clf_ : estimator Fitted meta-classifier (clone of the original meta-estimator) train_meta_features : numpy array, shape = [n_samples, n_classifiers] meta-features for training data, where n_samples is the number of samples in training data and n_classifiers is the number of classfiers. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/ Methods decision_function(X) Predict class confidence scores for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns scores : shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes). Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. fit(X, y, sample_weight=None) Fit ensemble classifers and the meta-classifier. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] or [n_samples, n_outputs] Target values. sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict target values for X. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns labels : array-like, shape = [n_samples] Predicted class labels. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, n_classifiers] Returns the meta-features for test data. predict_proba(X) Predict class probabilities for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns proba : array-like, shape = [n_samples, n_classes] or a list of n_outputs of such arrays if n_outputs > 1. Probability for each class per sample. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_fit_request(self: mlxtend.classifier.stacking_classification.StackingClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_classification.StackingClassifier* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.classifier.stacking_classification.StackingClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_classification.StackingClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object. Properties named_classifiers None","title":"StackingClassifier"},{"location":"api_modules/mlxtend.classifier/StackingClassifier/#stackingclassifier","text":"StackingClassifier(classifiers, meta_classifier, use_probas=False, drop_proba_col=None, average_probas=False, verbose=0, use_features_in_secondary=False, store_train_meta_features=False, use_clones=True, fit_base_estimators=True) A Stacking classifier for scikit-learn estimators for classification. Parameters classifiers : array-like, shape = [n_classifiers] A list of classifiers. Invoking the fit method on the StackingClassifer will fit clones of these original classifiers that will be stored in the class attribute self.clfs_ if use_clones=True (default) and fit_base_estimators=True (default). meta_classifier : object The meta-classifier to be fitted on the ensemble of classifiers use_probas : bool (default: False) If True, trains meta-classifier based on predicted probabilities instead of class labels. drop_proba_col : string (default: None) Drops extra \"probability\" column in the feature set, because it is redundant: p(y_c) = 1 - p(y_1) + p(y_2) + ... + p(y_{c-1}). This can be useful for meta-classifiers that are sensitive to perfectly collinear features. If 'last', drops last probability column. If 'first', drops first probability column. Only relevant if use_probas=True . average_probas : bool (default: False) Averages the probabilities as meta features if True . Only relevant if use_probas=True . verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the regressor being fitted - verbose=2 : Prints info about the parameters of the regressor being fitted - verbose>2 : Changes verbose param of the underlying regressor to self.verbose - 2 use_features_in_secondary : bool (default: False) If True, the meta-classifier will be trained both on the predictions of the original classifiers and the original dataset. If False, the meta-classifier will be trained only on the predictions of the original classifiers. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-classifier stored in the self.train_meta_features_ array, which can be accessed after calling fit . use_clones : bool (default: True) Clones the classifiers for stacking classification if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Hence, if use_clones=True, the original input classifiers will remain unmodified upon using the StackingClassifier's fit method. Setting use_clones=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. fit_base_estimators: bool (default: True) Refits classifiers in classifiers if True; uses references to the classifiers , otherwise (assumes that the classifiers were already fit). Note: fit_base_estimators=False will enforce use_clones to be False, and is incompatible to most scikit-learn wrappers! For instance, if any form of cross-validation is performed this would require the re-fitting classifiers to training folds, which would raise a NotFitterError if fit_base_estimators=False. (New in mlxtend v0.6.) Attributes clfs_ : list, shape=[n_classifiers] Fitted classifiers (clones of the original classifiers) meta_clf_ : estimator Fitted meta-classifier (clone of the original meta-estimator) train_meta_features : numpy array, shape = [n_samples, n_classifiers] meta-features for training data, where n_samples is the number of samples in training data and n_classifiers is the number of classfiers. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/","title":"StackingClassifier"},{"location":"api_modules/mlxtend.classifier/StackingClassifier/#methods","text":"decision_function(X) Predict class confidence scores for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns scores : shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes). Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. fit(X, y, sample_weight=None) Fit ensemble classifers and the meta-classifier. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] or [n_samples, n_outputs] Target values. sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict target values for X. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns labels : array-like, shape = [n_samples] Predicted class labels. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, n_classifiers] Returns the meta-features for test data. predict_proba(X) Predict class probabilities for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns proba : array-like, shape = [n_samples, n_classes] or a list of n_outputs of such arrays if n_outputs > 1. Probability for each class per sample. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_fit_request(self: mlxtend.classifier.stacking_classification.StackingClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_classification.StackingClassifier* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.classifier.stacking_classification.StackingClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_classification.StackingClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object.","title":"Methods"},{"location":"api_modules/mlxtend.classifier/StackingClassifier/#properties","text":"named_classifiers None","title":"Properties"},{"location":"api_modules/mlxtend.cluster/Kmeans/","text":"Kmeans Kmeans(k, max_iter=10, convergence_tolerance=1e-05, random_seed=None, print_progress=0) K-means clustering class. Added in 0.4.1dev Parameters k : int Number of clusters max_iter : int (default: 10) Number of iterations during cluster assignment. Cluster re-assignment stops automatically when the algorithm converged. convergence_tolerance : float (default: 1e-05) Compares current centroids with centroids of the previous iteration using the given tolerance (a small positive float)to determine if the algorithm converged early. random_seed : int (default: None) Set random state for the initial centroid assignment. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Iterations elapsed 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes centroids_ : 2d-array, shape={k, n_features} Feature values of the k cluster centroids. custers_ : dictionary The cluster assignments stored as a Python dictionary; the dictionary keys denote the cluster indeces and the items are Python lists of the sample indices that were assigned to each cluster. iterations_ : int Number of iterations until convergence. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/Kmeans/ Methods fit(X, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Kmeans"},{"location":"api_modules/mlxtend.cluster/Kmeans/#kmeans","text":"Kmeans(k, max_iter=10, convergence_tolerance=1e-05, random_seed=None, print_progress=0) K-means clustering class. Added in 0.4.1dev Parameters k : int Number of clusters max_iter : int (default: 10) Number of iterations during cluster assignment. Cluster re-assignment stops automatically when the algorithm converged. convergence_tolerance : float (default: 1e-05) Compares current centroids with centroids of the previous iteration using the given tolerance (a small positive float)to determine if the algorithm converged early. random_seed : int (default: None) Set random state for the initial centroid assignment. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Iterations elapsed 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes centroids_ : 2d-array, shape={k, n_features} Feature values of the k cluster centroids. custers_ : dictionary The cluster assignments stored as a Python dictionary; the dictionary keys denote the cluster indeces and the items are Python lists of the sample indices that were assigned to each cluster. iterations_ : int Number of iterations until convergence. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/Kmeans/","title":"Kmeans"},{"location":"api_modules/mlxtend.cluster/Kmeans/#methods","text":"fit(X, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Methods"},{"location":"api_modules/mlxtend.data/autompg_data/","text":"autompg_data autompg_data() Auto MPG dataset. Source : https://archive.ics.uci.edu/ml/datasets/Auto+MPG Number of samples : 392 Continuous target variable : mpg Dataset Attributes: 1) cylinders: multi-valued discrete 2) displacement: continuous 3) horsepower: continuous 4) weight: continuous 5) acceleration: continuous 6) model year: multi-valued discrete 7) origin: multi-valued discrete 8) car name: string (unique for each instance) Returns X, y : [n_samples, n_features], [n_targets] X is the feature matrix with 392 auto samples as rows and 8 feature columns (6 rows with NaNs removed). y is a 1-dimensional array of the target MPG values. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/autompg_data/","title":"Autompg data"},{"location":"api_modules/mlxtend.data/autompg_data/#autompg_data","text":"autompg_data() Auto MPG dataset. Source : https://archive.ics.uci.edu/ml/datasets/Auto+MPG Number of samples : 392 Continuous target variable : mpg Dataset Attributes: 1) cylinders: multi-valued discrete 2) displacement: continuous 3) horsepower: continuous 4) weight: continuous 5) acceleration: continuous 6) model year: multi-valued discrete 7) origin: multi-valued discrete 8) car name: string (unique for each instance) Returns X, y : [n_samples, n_features], [n_targets] X is the feature matrix with 392 auto samples as rows and 8 feature columns (6 rows with NaNs removed). y is a 1-dimensional array of the target MPG values. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/autompg_data/","title":"autompg_data"},{"location":"api_modules/mlxtend.data/boston_housing_data/","text":"boston_housing_data boston_housing_data() Boston Housing dataset. Source : https://archive.ics.uci.edu/ml/datasets/Housing Number of samples : 506 Continuous target variable : MEDV MEDV = Median value of owner-occupied homes in $1000's Dataset Attributes: 1) CRIM per capita crime rate by town 2) ZN proportion of residential land zoned for lots over 25,000 sq.ft. 3) INDUS proportion of non-retail business acres per town 4) CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) 5) NOX nitric oxides concentration (parts per 10 million) 6) RM average number of rooms per dwelling 7) AGE proportion of owner-occupied units built prior to 1940 8) DIS weighted distances to five Boston employment centres 9) RAD index of accessibility to radial highways 10) TAX full-value property-tax rate per $10,000 11) PTRATIO pupil-teacher ratio by town 12) B 1000(Bk - 0.63)^2 where Bk is the prop. of b. by town 13) LSTAT % lower status of the population Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 506 housing samples as rows and 13 feature columns. y is a 1-dimensional array of the continuous target variable MEDV Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/boston_housing_data/","title":"Boston housing data"},{"location":"api_modules/mlxtend.data/boston_housing_data/#boston_housing_data","text":"boston_housing_data() Boston Housing dataset. Source : https://archive.ics.uci.edu/ml/datasets/Housing Number of samples : 506 Continuous target variable : MEDV MEDV = Median value of owner-occupied homes in $1000's Dataset Attributes: 1) CRIM per capita crime rate by town 2) ZN proportion of residential land zoned for lots over 25,000 sq.ft. 3) INDUS proportion of non-retail business acres per town 4) CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) 5) NOX nitric oxides concentration (parts per 10 million) 6) RM average number of rooms per dwelling 7) AGE proportion of owner-occupied units built prior to 1940 8) DIS weighted distances to five Boston employment centres 9) RAD index of accessibility to radial highways 10) TAX full-value property-tax rate per $10,000 11) PTRATIO pupil-teacher ratio by town 12) B 1000(Bk - 0.63)^2 where Bk is the prop. of b. by town 13) LSTAT % lower status of the population Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 506 housing samples as rows and 13 feature columns. y is a 1-dimensional array of the continuous target variable MEDV Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/boston_housing_data/","title":"boston_housing_data"},{"location":"api_modules/mlxtend.data/iris_data/","text":"iris_data iris_data(version='uci') Iris flower dataset. Source : https://archive.ics.uci.edu/ml/datasets/Iris Number of samples : 150 Class labels : {0, 1, 2}, distribution: [50, 50, 50] 0 = setosa, 1 = versicolor, 2 = virginica. Dataset Attributes: 1) sepal length [cm] 2) sepal width [cm] 3) petal length [cm] 4) petal width [cm] Parameters version : string, optional (default: 'uci'). Version to use {'uci', 'corrected'}. 'uci' loads the dataset as deposited on the UCI machine learning repository, and 'corrected' provides the version that is consistent with Fisher's original paper. See Note for details. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 150 flower samples as rows, and 4 feature columns sepal length, sepal width, petal length, and petal width. y is a 1-dimensional array of the class labels {0, 1, 2} Note The Iris dataset (originally collected by Edgar Anderson) and available in UCI's machine learning repository is different from the Iris dataset described in the original paper by R.A. Fisher [1]). Precisely, there are two data points (row number 34 and 37) in UCI's Machine Learning repository are different from the origianlly published Iris dataset. Also, the original version of the Iris Dataset, which can be loaded via version='corrected' is the same as the one in R. [1] . A. Fisher (1936). \"The use of multiple measurements in taxonomic problems\". Annals of Eugenics. 7 (2): 179\u2013188 Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/iris_data/","title":"Iris data"},{"location":"api_modules/mlxtend.data/iris_data/#iris_data","text":"iris_data(version='uci') Iris flower dataset. Source : https://archive.ics.uci.edu/ml/datasets/Iris Number of samples : 150 Class labels : {0, 1, 2}, distribution: [50, 50, 50] 0 = setosa, 1 = versicolor, 2 = virginica. Dataset Attributes: 1) sepal length [cm] 2) sepal width [cm] 3) petal length [cm] 4) petal width [cm] Parameters version : string, optional (default: 'uci'). Version to use {'uci', 'corrected'}. 'uci' loads the dataset as deposited on the UCI machine learning repository, and 'corrected' provides the version that is consistent with Fisher's original paper. See Note for details. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 150 flower samples as rows, and 4 feature columns sepal length, sepal width, petal length, and petal width. y is a 1-dimensional array of the class labels {0, 1, 2} Note The Iris dataset (originally collected by Edgar Anderson) and available in UCI's machine learning repository is different from the Iris dataset described in the original paper by R.A. Fisher [1]). Precisely, there are two data points (row number 34 and 37) in UCI's Machine Learning repository are different from the origianlly published Iris dataset. Also, the original version of the Iris Dataset, which can be loaded via version='corrected' is the same as the one in R. [1] . A. Fisher (1936). \"The use of multiple measurements in taxonomic problems\". Annals of Eugenics. 7 (2): 179\u2013188 Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/iris_data/","title":"iris_data"},{"location":"api_modules/mlxtend.data/loadlocal_mnist/","text":"loadlocal_mnist loadlocal_mnist(images_path, labels_path) Read MNIST from ubyte files. Parameters images_path : str path to the test or train MNIST ubyte file labels_path : str path to the test or train MNIST class labels file Returns images : [n_samples, n_pixels] numpy.array Pixel values of the images. labels : [n_samples] numpy array Target class labels Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/loadlocal_mnist/","title":"Loadlocal mnist"},{"location":"api_modules/mlxtend.data/loadlocal_mnist/#loadlocal_mnist","text":"loadlocal_mnist(images_path, labels_path) Read MNIST from ubyte files. Parameters images_path : str path to the test or train MNIST ubyte file labels_path : str path to the test or train MNIST class labels file Returns images : [n_samples, n_pixels] numpy.array Pixel values of the images. labels : [n_samples] numpy array Target class labels Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/loadlocal_mnist/","title":"loadlocal_mnist"},{"location":"api_modules/mlxtend.data/make_multiplexer_dataset/","text":"make_multiplexer_dataset make_multiplexer_dataset(address_bits=2, sample_size=100, positive_class_ratio=0.5, shuffle=False, random_seed=None) Function to create a binary n-bit multiplexer dataset. New in mlxtend v0.9 Parameters address_bits : int (default: 2) A positive integer that determines the number of address bits in the multiplexer, which in turn determine the n-bit capacity of the multiplexer and therefore the number of features. The number of features is determined by the number of address bits. For example, 2 address bits will result in a 6 bit multiplexer and consequently 6 features (2 + 2^2 = 6). If address_bits=3 , then this results in an 11-bit multiplexer as (2 + 2^3 = 11) with 11 features. sample_size : int (default: 100) The total number of samples generated. positive_class_ratio : float (default: 0.5) The fraction (a float between 0 and 1) of samples in the sample_size d dataset that have class label 1. If positive_class_ratio=0.5 (default), then the ratio of class 0 and class 1 samples is perfectly balanced. shuffle : Bool (default: False) Whether or not to shuffle the features and labels. If False (default), the samples are returned in sorted order starting with sample_size /2 samples with class label 0 and followed by sample_size /2 samples with class label 1. random_seed : int (default: None) Random seed used for generating the multiplexer samples and shuffling. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with the number of samples equal to sample_size . The number of features is determined by the number of address bits. For instance, 2 address bits will result in a 6 bit multiplexer and consequently 6 features (2 + 2^2 = 6). All features are binary (values in {0, 1}). y is a 1-dimensional array of class labels in {0, 1}. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/make_multiplexer_dataset","title":"Make multiplexer dataset"},{"location":"api_modules/mlxtend.data/make_multiplexer_dataset/#make_multiplexer_dataset","text":"make_multiplexer_dataset(address_bits=2, sample_size=100, positive_class_ratio=0.5, shuffle=False, random_seed=None) Function to create a binary n-bit multiplexer dataset. New in mlxtend v0.9 Parameters address_bits : int (default: 2) A positive integer that determines the number of address bits in the multiplexer, which in turn determine the n-bit capacity of the multiplexer and therefore the number of features. The number of features is determined by the number of address bits. For example, 2 address bits will result in a 6 bit multiplexer and consequently 6 features (2 + 2^2 = 6). If address_bits=3 , then this results in an 11-bit multiplexer as (2 + 2^3 = 11) with 11 features. sample_size : int (default: 100) The total number of samples generated. positive_class_ratio : float (default: 0.5) The fraction (a float between 0 and 1) of samples in the sample_size d dataset that have class label 1. If positive_class_ratio=0.5 (default), then the ratio of class 0 and class 1 samples is perfectly balanced. shuffle : Bool (default: False) Whether or not to shuffle the features and labels. If False (default), the samples are returned in sorted order starting with sample_size /2 samples with class label 0 and followed by sample_size /2 samples with class label 1. random_seed : int (default: None) Random seed used for generating the multiplexer samples and shuffling. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with the number of samples equal to sample_size . The number of features is determined by the number of address bits. For instance, 2 address bits will result in a 6 bit multiplexer and consequently 6 features (2 + 2^2 = 6). All features are binary (values in {0, 1}). y is a 1-dimensional array of class labels in {0, 1}. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/make_multiplexer_dataset","title":"make_multiplexer_dataset"},{"location":"api_modules/mlxtend.data/mnist_data/","text":"mnist_data mnist_data() 5000 samples from the MNIST handwritten digits dataset. Data Source : https://yann.lecun.com/exdb/mnist/ Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 5000 image samples as rows, each row consists of 28x28 pixels that were unrolled into 784 pixel feature vectors. y contains the 10 unique class labels 0-9. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/mnist_data/","title":"Mnist data"},{"location":"api_modules/mlxtend.data/mnist_data/#mnist_data","text":"mnist_data() 5000 samples from the MNIST handwritten digits dataset. Data Source : https://yann.lecun.com/exdb/mnist/ Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 5000 image samples as rows, each row consists of 28x28 pixels that were unrolled into 784 pixel feature vectors. y contains the 10 unique class labels 0-9. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/mnist_data/","title":"mnist_data"},{"location":"api_modules/mlxtend.data/three_blobs_data/","text":"three_blobs_data three_blobs_data() A random dataset of 3 2D blobs for clustering. Number of samples : 150 Suggested labels : {0, 1, 2}, distribution: [50, 50, 50] Returns X, y : [n_samples, n_features], [n_cluster_labels] X is the feature matrix with 159 samples as rows and 2 feature columns. y is a 1-dimensional array of the 3 suggested cluster labels 0, 1, 2 Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/three_blobs_data","title":"Three blobs data"},{"location":"api_modules/mlxtend.data/three_blobs_data/#three_blobs_data","text":"three_blobs_data() A random dataset of 3 2D blobs for clustering. Number of samples : 150 Suggested labels : {0, 1, 2}, distribution: [50, 50, 50] Returns X, y : [n_samples, n_features], [n_cluster_labels] X is the feature matrix with 159 samples as rows and 2 feature columns. y is a 1-dimensional array of the 3 suggested cluster labels 0, 1, 2 Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/three_blobs_data","title":"three_blobs_data"},{"location":"api_modules/mlxtend.data/wine_data/","text":"wine_data wine_data() Wine dataset. Source : https://archive.ics.uci.edu/ml/datasets/Wine Number of samples : 178 Class labels : {0, 1, 2}, distribution: [59, 71, 48] Dataset Attributes: 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted wines 13) Proline Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 178 wine samples as rows and 13 feature columns. y is a 1-dimensional array of the 3 class labels 0, 1, 2 Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/wine_data","title":"Wine data"},{"location":"api_modules/mlxtend.data/wine_data/#wine_data","text":"wine_data() Wine dataset. Source : https://archive.ics.uci.edu/ml/datasets/Wine Number of samples : 178 Class labels : {0, 1, 2}, distribution: [59, 71, 48] Dataset Attributes: 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted wines 13) Proline Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 178 wine samples as rows and 13 feature columns. y is a 1-dimensional array of the 3 class labels 0, 1, 2 Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/wine_data","title":"wine_data"},{"location":"api_modules/mlxtend.evaluate/BootstrapOutOfBag/","text":"BootstrapOutOfBag BootstrapOutOfBag(n_splits=200, random_seed=None) Parameters n_splits : int (default=200) Number of bootstrap iterations. Must be larger than 1. random_seed : int (default=None) If int, random_seed is the seed used by the random number generator. Returns train_idx : ndarray The training set indices for that split. test_idx : ndarray The testing set indices for that split. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/BootstrapOutOfBag/ Methods get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator Parameters X : object Always ignored, exists for compatibility with scikit-learn. y : object Always ignored, exists for compatibility with scikit-learn. groups : object Always ignored, exists for compatibility with scikit-learn. Returns n_splits : int Returns the number of splitting iterations in the cross-validator. split(X, y=None, groups=None) y : array-like or None (default: None) Argument is not used and only included as parameter for compatibility, similar to KFold in scikit-learn. groups : array-like or None (default: None) Argument is not used and only included as parameter for compatibility, similar to KFold in scikit-learn.","title":"BootstrapOutOfBag"},{"location":"api_modules/mlxtend.evaluate/BootstrapOutOfBag/#bootstrapoutofbag","text":"BootstrapOutOfBag(n_splits=200, random_seed=None) Parameters n_splits : int (default=200) Number of bootstrap iterations. Must be larger than 1. random_seed : int (default=None) If int, random_seed is the seed used by the random number generator. Returns train_idx : ndarray The training set indices for that split. test_idx : ndarray The testing set indices for that split. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/BootstrapOutOfBag/","title":"BootstrapOutOfBag"},{"location":"api_modules/mlxtend.evaluate/BootstrapOutOfBag/#methods","text":"get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator Parameters X : object Always ignored, exists for compatibility with scikit-learn. y : object Always ignored, exists for compatibility with scikit-learn. groups : object Always ignored, exists for compatibility with scikit-learn. Returns n_splits : int Returns the number of splitting iterations in the cross-validator. split(X, y=None, groups=None) y : array-like or None (default: None) Argument is not used and only included as parameter for compatibility, similar to KFold in scikit-learn. groups : array-like or None (default: None) Argument is not used and only included as parameter for compatibility, similar to KFold in scikit-learn.","title":"Methods"},{"location":"api_modules/mlxtend.evaluate/GroupTimeSeriesSplit/","text":"GroupTimeSeriesSplit GroupTimeSeriesSplit(test_size, train_size=None, n_splits=None, gap_size=0, shift_size=1, window_type='rolling') Group time series cross-validator. Parameters test_size : int Size of test dataset. train_size : int (default=None) Size of train dataset. n_splits : int (default=None) Number of the splits. gap_size : int (default=0) Gap size between train and test datasets. shift_size : int (default=1) Step to shift for the next fold. window_type : str (default=\"rolling\") Type of the window. Possible values: \"rolling\", \"expanding\". Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/GroupTimeSeriesSplit/ Methods get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator. Parameters X : object Always ignored, exists for compatibility. y : object Always ignored, exists for compatibility. groups : object Always ignored, exists for compatibility. Returns n_splits : int Returns the number of splitting iterations in the cross-validator. split(X, y=None, groups=None) Generate indices to split data into training and test set. Parameters X : array-like Training data. y : array-like (default=None) Always ignored, exists for compatibility. groups : array-like (default=None) Array with group names or sequence numbers. Yields train : ndarray The training set indices for that split. test : ndarray The testing set indices for that split.","title":"GroupTimeSeriesSplit"},{"location":"api_modules/mlxtend.evaluate/GroupTimeSeriesSplit/#grouptimeseriessplit","text":"GroupTimeSeriesSplit(test_size, train_size=None, n_splits=None, gap_size=0, shift_size=1, window_type='rolling') Group time series cross-validator. Parameters test_size : int Size of test dataset. train_size : int (default=None) Size of train dataset. n_splits : int (default=None) Number of the splits. gap_size : int (default=0) Gap size between train and test datasets. shift_size : int (default=1) Step to shift for the next fold. window_type : str (default=\"rolling\") Type of the window. Possible values: \"rolling\", \"expanding\". Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/GroupTimeSeriesSplit/","title":"GroupTimeSeriesSplit"},{"location":"api_modules/mlxtend.evaluate/GroupTimeSeriesSplit/#methods","text":"get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator. Parameters X : object Always ignored, exists for compatibility. y : object Always ignored, exists for compatibility. groups : object Always ignored, exists for compatibility. Returns n_splits : int Returns the number of splitting iterations in the cross-validator. split(X, y=None, groups=None) Generate indices to split data into training and test set. Parameters X : array-like Training data. y : array-like (default=None) Always ignored, exists for compatibility. groups : array-like (default=None) Array with group names or sequence numbers. Yields train : ndarray The training set indices for that split. test : ndarray The testing set indices for that split.","title":"Methods"},{"location":"api_modules/mlxtend.evaluate/PredefinedHoldoutSplit/","text":"PredefinedHoldoutSplit PredefinedHoldoutSplit(valid_indices) Train/Validation set splitter for sklearn's GridSearchCV etc. Uses user-specified train/validation set indices to split a dataset into train/validation sets using user-defined or random indices. Parameters valid_indices : array-like, shape (num_examples,) Indices of the training examples in the training set to be used for validation. All other indices in the training set are used to for a training subset for model fitting. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/PredefinedHoldoutSplit/ Methods get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator Parameters X : object Always ignored, exists for compatibility. y : object Always ignored, exists for compatibility. groups : object Always ignored, exists for compatibility. Returns n_splits : 1 Returns the number of splitting iterations in the cross-validator. Always returns 1. split(X, y, groups=None) Generate indices to split data into training and test set. Parameters X : array-like, shape (num_examples, num_features) Training data, where num_examples is the number of examples and num_features is the number of features. y : array-like, shape (num_examples,) The target variable for supervised learning problems. Stratification is done based on the y labels. groups : object Always ignored, exists for compatibility. Yields train_index : ndarray The training set indices for that split. valid_index : ndarray The validation set indices for that split.","title":"PredefinedHoldoutSplit"},{"location":"api_modules/mlxtend.evaluate/PredefinedHoldoutSplit/#predefinedholdoutsplit","text":"PredefinedHoldoutSplit(valid_indices) Train/Validation set splitter for sklearn's GridSearchCV etc. Uses user-specified train/validation set indices to split a dataset into train/validation sets using user-defined or random indices. Parameters valid_indices : array-like, shape (num_examples,) Indices of the training examples in the training set to be used for validation. All other indices in the training set are used to for a training subset for model fitting. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/PredefinedHoldoutSplit/","title":"PredefinedHoldoutSplit"},{"location":"api_modules/mlxtend.evaluate/PredefinedHoldoutSplit/#methods","text":"get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator Parameters X : object Always ignored, exists for compatibility. y : object Always ignored, exists for compatibility. groups : object Always ignored, exists for compatibility. Returns n_splits : 1 Returns the number of splitting iterations in the cross-validator. Always returns 1. split(X, y, groups=None) Generate indices to split data into training and test set. Parameters X : array-like, shape (num_examples, num_features) Training data, where num_examples is the number of examples and num_features is the number of features. y : array-like, shape (num_examples,) The target variable for supervised learning problems. Stratification is done based on the y labels. groups : object Always ignored, exists for compatibility. Yields train_index : ndarray The training set indices for that split. valid_index : ndarray The validation set indices for that split.","title":"Methods"},{"location":"api_modules/mlxtend.evaluate/RandomHoldoutSplit/","text":"RandomHoldoutSplit RandomHoldoutSplit(valid_size=0.5, random_seed=None, stratify=False) Train/Validation set splitter for sklearn's GridSearchCV etc. Provides train/validation set indices to split a dataset into train/validation sets using random indices. Parameters valid_size : float (default: 0.5) Proportion of examples that being assigned as validation examples. 1- valid_size will then automatically be assigned as training set examples. random_seed : int (default: None) The random seed for splitting the data into training and validation set partitions. stratify : bool (default: False) True or False, whether to perform a stratified split or not Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/RandomHoldoutSplit/ Methods get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator Parameters X : object Always ignored, exists for compatibility. y : object Always ignored, exists for compatibility. groups : object Always ignored, exists for compatibility. Returns n_splits : 1 Returns the number of splitting iterations in the cross-validator. Always returns 1. split(X, y, groups=None) Generate indices to split data into training and test set. Parameters X : array-like, shape (num_examples, num_features) Training data, where num_examples is the number of training examples and num_features is the number of features. y : array-like, shape (num_examples,) The target variable for supervised learning problems. Stratification is done based on the y labels. groups : object Always ignored, exists for compatibility. Yields train_index : ndarray The training set indices for that split. valid_index : ndarray The validation set indices for that split.","title":"RandomHoldoutSplit"},{"location":"api_modules/mlxtend.evaluate/RandomHoldoutSplit/#randomholdoutsplit","text":"RandomHoldoutSplit(valid_size=0.5, random_seed=None, stratify=False) Train/Validation set splitter for sklearn's GridSearchCV etc. Provides train/validation set indices to split a dataset into train/validation sets using random indices. Parameters valid_size : float (default: 0.5) Proportion of examples that being assigned as validation examples. 1- valid_size will then automatically be assigned as training set examples. random_seed : int (default: None) The random seed for splitting the data into training and validation set partitions. stratify : bool (default: False) True or False, whether to perform a stratified split or not Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/RandomHoldoutSplit/","title":"RandomHoldoutSplit"},{"location":"api_modules/mlxtend.evaluate/RandomHoldoutSplit/#methods","text":"get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator Parameters X : object Always ignored, exists for compatibility. y : object Always ignored, exists for compatibility. groups : object Always ignored, exists for compatibility. Returns n_splits : 1 Returns the number of splitting iterations in the cross-validator. Always returns 1. split(X, y, groups=None) Generate indices to split data into training and test set. Parameters X : array-like, shape (num_examples, num_features) Training data, where num_examples is the number of training examples and num_features is the number of features. y : array-like, shape (num_examples,) The target variable for supervised learning problems. Stratification is done based on the y labels. groups : object Always ignored, exists for compatibility. Yields train_index : ndarray The training set indices for that split. valid_index : ndarray The validation set indices for that split.","title":"Methods"},{"location":"api_modules/mlxtend.evaluate/accuracy_score/","text":"accuracy_score accuracy_score(y_target, y_predicted, method='standard', pos_label=1, normalize=True) General accuracy function for supervised learning. Parameters y_target : array-like, shape=[n_values] True class labels or target values. y_predicted : array-like, shape=[n_values] Predicted class labels or target values. method : str, 'standard' by default. The chosen method for accuracy computation. If set to 'standard', computes overall accuracy. If set to 'binary', computes accuracy for class pos_label. If set to 'average', computes average per-class (balanced) accuracy. If set to 'balanced', computes the scikit-learn-style balanced accuracy. pos_label : str or int, 1 by default. The class whose accuracy score is to be reported. Used only when method is set to 'binary' normalize : bool, True by default. If True, returns fraction of correctly classified samples. If False, returns number of correctly classified samples. Returns score: float Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/accuracy_score/","title":"Accuracy score"},{"location":"api_modules/mlxtend.evaluate/accuracy_score/#accuracy_score","text":"accuracy_score(y_target, y_predicted, method='standard', pos_label=1, normalize=True) General accuracy function for supervised learning. Parameters y_target : array-like, shape=[n_values] True class labels or target values. y_predicted : array-like, shape=[n_values] Predicted class labels or target values. method : str, 'standard' by default. The chosen method for accuracy computation. If set to 'standard', computes overall accuracy. If set to 'binary', computes accuracy for class pos_label. If set to 'average', computes average per-class (balanced) accuracy. If set to 'balanced', computes the scikit-learn-style balanced accuracy. pos_label : str or int, 1 by default. The class whose accuracy score is to be reported. Used only when method is set to 'binary' normalize : bool, True by default. If True, returns fraction of correctly classified samples. If False, returns number of correctly classified samples. Returns score: float Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/accuracy_score/","title":"accuracy_score"},{"location":"api_modules/mlxtend.evaluate/bias_variance_decomp/","text":"bias_variance_decomp bias_variance_decomp(estimator, X_train, y_train, X_test, y_test, loss='0-1_loss', num_rounds=200, random_seed=None, fit_params) estimator : object A classifier or regressor object or class implementing both a fit and predict method similar to the scikit-learn API. X_train : array-like, shape=(num_examples, num_features) A training dataset for drawing the bootstrap samples to carry out the bias-variance decomposition. y_train : array-like, shape=(num_examples) Targets (class labels, continuous values in case of regression) associated with the X_train examples. X_test : array-like, shape=(num_examples, num_features) The test dataset for computing the average loss, bias, and variance. y_test : array-like, shape=(num_examples) Targets (class labels, continuous values in case of regression) associated with the X_test examples. loss : str (default='0-1_loss') Loss function for performing the bias-variance decomposition. Currently allowed values are '0-1_loss' and 'mse'. num_rounds : int (default=200) Number of bootstrap rounds (sampling from the training set) for performing the bias-variance decomposition. Each bootstrap sample has the same size as the original training set. random_seed : int (default=None) Random seed for the bootstrap sampling used for the bias-variance decomposition. fit_params : additional parameters Additional parameters to be passed to the .fit() function of the estimator when it is fit to the bootstrap samples. Returns avg_expected_loss, avg_bias, avg_var : returns the average expected average bias, and average bias (all floats), where the average is computed over the data points in the test set. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/","title":"Bias variance decomp"},{"location":"api_modules/mlxtend.evaluate/bias_variance_decomp/#bias_variance_decomp","text":"bias_variance_decomp(estimator, X_train, y_train, X_test, y_test, loss='0-1_loss', num_rounds=200, random_seed=None, fit_params) estimator : object A classifier or regressor object or class implementing both a fit and predict method similar to the scikit-learn API. X_train : array-like, shape=(num_examples, num_features) A training dataset for drawing the bootstrap samples to carry out the bias-variance decomposition. y_train : array-like, shape=(num_examples) Targets (class labels, continuous values in case of regression) associated with the X_train examples. X_test : array-like, shape=(num_examples, num_features) The test dataset for computing the average loss, bias, and variance. y_test : array-like, shape=(num_examples) Targets (class labels, continuous values in case of regression) associated with the X_test examples. loss : str (default='0-1_loss') Loss function for performing the bias-variance decomposition. Currently allowed values are '0-1_loss' and 'mse'. num_rounds : int (default=200) Number of bootstrap rounds (sampling from the training set) for performing the bias-variance decomposition. Each bootstrap sample has the same size as the original training set. random_seed : int (default=None) Random seed for the bootstrap sampling used for the bias-variance decomposition. fit_params : additional parameters Additional parameters to be passed to the .fit() function of the estimator when it is fit to the bootstrap samples. Returns avg_expected_loss, avg_bias, avg_var : returns the average expected average bias, and average bias (all floats), where the average is computed over the data points in the test set. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/","title":"bias_variance_decomp"},{"location":"api_modules/mlxtend.evaluate/bootstrap/","text":"bootstrap bootstrap(x, func, num_rounds=1000, ci=0.95, ddof=1, seed=None) Implements the ordinary nonparametric bootstrap Parameters x : NumPy array, shape=(n_samples, [n_columns]) An one or multidimensional array of data records func : A function which computes a statistic that is used to compute the bootstrap replicates (the statistic computed from the bootstrap samples). This function must return a scalar value. For example, np.mean or np.median would be an acceptable argument for func if x is a 1-dimensional array or vector. num_rounds : int (default=1000) The number of bootstrap samples to draw where each bootstrap sample has the same number of records as the original dataset. ci : int (default=0.95) An integer in the range (0, 1) that represents the confidence level for computing the confidence interval. For example, ci=0.95 (default) will compute the 95% confidence interval from the bootstrap replicates. ddof : int The delta degrees of freedom used when computing the standard error. seed : int or None (default=None) Random seed for generating bootstrap samples. Returns original, standard_error, (lower_ci, upper_ci) : tuple Returns the statistic of the original sample ( original ), the standard error of the estimate, and the respective confidence interval bounds. Examples ``` >>> from mlxtend.evaluate import bootstrap >>> rng = np.random.RandomState(123) >>> x = rng.normal(loc=5., size=100) >>> original, std_err, ci_bounds = bootstrap(x, ... num_rounds=1000, ... func=np.mean, ... ci=0.95, ... seed=123) >>> print('Mean: %.2f, SE: +/- %.2f, CI95: [%.2f, %.2f]' % (original, ... std_err, ... ci_bounds[0], ... ci_bounds[1])) Mean: 5.03, SE: +/- 0.11, CI95: [4.80, 5.26] >>> For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap/ ```","title":"Bootstrap"},{"location":"api_modules/mlxtend.evaluate/bootstrap/#bootstrap","text":"bootstrap(x, func, num_rounds=1000, ci=0.95, ddof=1, seed=None) Implements the ordinary nonparametric bootstrap Parameters x : NumPy array, shape=(n_samples, [n_columns]) An one or multidimensional array of data records func : A function which computes a statistic that is used to compute the bootstrap replicates (the statistic computed from the bootstrap samples). This function must return a scalar value. For example, np.mean or np.median would be an acceptable argument for func if x is a 1-dimensional array or vector. num_rounds : int (default=1000) The number of bootstrap samples to draw where each bootstrap sample has the same number of records as the original dataset. ci : int (default=0.95) An integer in the range (0, 1) that represents the confidence level for computing the confidence interval. For example, ci=0.95 (default) will compute the 95% confidence interval from the bootstrap replicates. ddof : int The delta degrees of freedom used when computing the standard error. seed : int or None (default=None) Random seed for generating bootstrap samples. Returns original, standard_error, (lower_ci, upper_ci) : tuple Returns the statistic of the original sample ( original ), the standard error of the estimate, and the respective confidence interval bounds. Examples ``` >>> from mlxtend.evaluate import bootstrap >>> rng = np.random.RandomState(123) >>> x = rng.normal(loc=5., size=100) >>> original, std_err, ci_bounds = bootstrap(x, ... num_rounds=1000, ... func=np.mean, ... ci=0.95, ... seed=123) >>> print('Mean: %.2f, SE: +/- %.2f, CI95: [%.2f, %.2f]' % (original, ... std_err, ... ci_bounds[0], ... ci_bounds[1])) Mean: 5.03, SE: +/- 0.11, CI95: [4.80, 5.26] >>> For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap/ ```","title":"bootstrap"},{"location":"api_modules/mlxtend.evaluate/bootstrap_point632_score/","text":"bootstrap_point632_score bootstrap_point632_score(estimator, X, y, n_splits=200, method='.632', scoring_func=None, predict_proba=False, random_seed=None, clone_estimator=True, fit_params) Implementation of the .632 [1] and .632+ [2] bootstrap for supervised learning References: - [1] Efron, Bradley. 1983. \"Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation.\" Journal of the American Statistical Association 78 (382): 316. doi:10.2307/2288636. - [2] Efron, Bradley, and Robert Tibshirani. 1997. \"Improvements on Cross-Validation: The .632+ Bootstrap Method.\" Journal of the American Statistical Association 92 (438): 548. doi:10.2307/2965703. Parameters estimator : object An estimator for classification or regression that follows the scikit-learn API and implements \"fit\" and \"predict\" methods. X : array-like The data to fit. Can be, for example a list, or an array at least 2d. y : array-like, optional, default: None The target variable to try to predict in the case of supervised learning. n_splits : int (default=200) Number of bootstrap iterations. Must be larger than 1. method : str (default='.632') The bootstrap method, which can be either - 1) '.632' bootstrap (default) - 2) '.632+' bootstrap - 3) 'oob' (regular out-of-bag, no weighting) for comparison studies. scoring_func : callable, Score function (or loss function) with signature scoring_func(y, y_pred, **kwargs) . If none, uses classification accuracy if the estimator is a classifier and mean squared error if the estimator is a regressor. predict_proba : bool Whether to use the predict_proba function for the estimator argument. This is to be used in conjunction with scoring_func which takes in probability values instead of actual predictions. For example, if the scoring_func is :meth: sklearn.metrics.roc_auc_score , then use predict_proba=True . Note that this requires estimator to have predict_proba method implemented. random_seed : int (default=None) If int, random_seed is the seed used by the random number generator. clone_estimator : bool (default=True) Clones the estimator if true, otherwise fits the original. fit_params : additional parameters Additional parameters to be passed to the .fit() function of the estimator when it is fit to the bootstrap samples. Returns scores : array of float, shape=(len(list(n_splits)),) Array of scores of the estimator for each bootstrap replicate. Examples >>> from sklearn import datasets, linear_model >>> from mlxtend.evaluate import bootstrap_point632_score >>> iris = datasets.load_iris() >>> X = iris.data >>> y = iris.target >>> lr = linear_model.LogisticRegression() >>> scores = bootstrap_point632_score(lr, X, y) >>> acc = np.mean(scores) >>> print('Accuracy:', acc) 0.953023146884 >>> lower = np.percentile(scores, 2.5) >>> upper = np.percentile(scores, 97.5) >>> print('95%% Confidence interval: [%.2f, %.2f]' % (lower, upper)) 95% Confidence interval: [0.90, 0.98] For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap_point632_score/","title":"Bootstrap point632 score"},{"location":"api_modules/mlxtend.evaluate/bootstrap_point632_score/#bootstrap_point632_score","text":"bootstrap_point632_score(estimator, X, y, n_splits=200, method='.632', scoring_func=None, predict_proba=False, random_seed=None, clone_estimator=True, fit_params) Implementation of the .632 [1] and .632+ [2] bootstrap for supervised learning References: - [1] Efron, Bradley. 1983. \"Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation.\" Journal of the American Statistical Association 78 (382): 316. doi:10.2307/2288636. - [2] Efron, Bradley, and Robert Tibshirani. 1997. \"Improvements on Cross-Validation: The .632+ Bootstrap Method.\" Journal of the American Statistical Association 92 (438): 548. doi:10.2307/2965703. Parameters estimator : object An estimator for classification or regression that follows the scikit-learn API and implements \"fit\" and \"predict\" methods. X : array-like The data to fit. Can be, for example a list, or an array at least 2d. y : array-like, optional, default: None The target variable to try to predict in the case of supervised learning. n_splits : int (default=200) Number of bootstrap iterations. Must be larger than 1. method : str (default='.632') The bootstrap method, which can be either - 1) '.632' bootstrap (default) - 2) '.632+' bootstrap - 3) 'oob' (regular out-of-bag, no weighting) for comparison studies. scoring_func : callable, Score function (or loss function) with signature scoring_func(y, y_pred, **kwargs) . If none, uses classification accuracy if the estimator is a classifier and mean squared error if the estimator is a regressor. predict_proba : bool Whether to use the predict_proba function for the estimator argument. This is to be used in conjunction with scoring_func which takes in probability values instead of actual predictions. For example, if the scoring_func is :meth: sklearn.metrics.roc_auc_score , then use predict_proba=True . Note that this requires estimator to have predict_proba method implemented. random_seed : int (default=None) If int, random_seed is the seed used by the random number generator. clone_estimator : bool (default=True) Clones the estimator if true, otherwise fits the original. fit_params : additional parameters Additional parameters to be passed to the .fit() function of the estimator when it is fit to the bootstrap samples. Returns scores : array of float, shape=(len(list(n_splits)),) Array of scores of the estimator for each bootstrap replicate. Examples >>> from sklearn import datasets, linear_model >>> from mlxtend.evaluate import bootstrap_point632_score >>> iris = datasets.load_iris() >>> X = iris.data >>> y = iris.target >>> lr = linear_model.LogisticRegression() >>> scores = bootstrap_point632_score(lr, X, y) >>> acc = np.mean(scores) >>> print('Accuracy:', acc) 0.953023146884 >>> lower = np.percentile(scores, 2.5) >>> upper = np.percentile(scores, 97.5) >>> print('95%% Confidence interval: [%.2f, %.2f]' % (lower, upper)) 95% Confidence interval: [0.90, 0.98] For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap_point632_score/","title":"bootstrap_point632_score"},{"location":"api_modules/mlxtend.evaluate/cochrans_q/","text":"cochrans_q cochrans_q(y_target, y_model_predictions)* Cochran's Q test to compare 2 or more models. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. *y_model_predictions : array-likes, shape=[n_samples] Variable number of 2 or more arrays that contain the predicted class labels from models as 1D NumPy array. Returns q, p : float or None, float Returns the Q (chi-squared) value and the p-value Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/cochrans_q/","title":"Cochrans q"},{"location":"api_modules/mlxtend.evaluate/cochrans_q/#cochrans_q","text":"cochrans_q(y_target, y_model_predictions)* Cochran's Q test to compare 2 or more models. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. *y_model_predictions : array-likes, shape=[n_samples] Variable number of 2 or more arrays that contain the predicted class labels from models as 1D NumPy array. Returns q, p : float or None, float Returns the Q (chi-squared) value and the p-value Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/cochrans_q/","title":"cochrans_q"},{"location":"api_modules/mlxtend.evaluate/combined_ftest_5x2cv/","text":"combined_ftest_5x2cv combined_ftest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None) Implements the 5x2cv combined F test proposed by Alpaydin 1999, to compare the performance of two models. Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. random_seed : int or None (default: None) Random seed for creating the test/train splits. Returns f : float The F-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/combined_ftest_5x2cv/","title":"Combined ftest 5x2cv"},{"location":"api_modules/mlxtend.evaluate/combined_ftest_5x2cv/#combined_ftest_5x2cv","text":"combined_ftest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None) Implements the 5x2cv combined F test proposed by Alpaydin 1999, to compare the performance of two models. Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. random_seed : int or None (default: None) Random seed for creating the test/train splits. Returns f : float The F-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/combined_ftest_5x2cv/","title":"combined_ftest_5x2cv"},{"location":"api_modules/mlxtend.evaluate/confusion_matrix/","text":"confusion_matrix confusion_matrix(y_target, y_predicted, binary=False, positive_label=1) Compute a confusion matrix/contingency table. Parameters y_target : array-like, shape=[n_samples] True class labels. y_predicted : array-like, shape=[n_samples] Predicted class labels. binary : bool (default: False) Maps a multi-class problem onto a binary confusion matrix, where the positive class is 1 and all other classes are 0. positive_label : int (default: 1) Class label of the positive class. Returns mat : array-like, shape=[n_classes, n_classes] Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix/","title":"Confusion matrix"},{"location":"api_modules/mlxtend.evaluate/confusion_matrix/#confusion_matrix","text":"confusion_matrix(y_target, y_predicted, binary=False, positive_label=1) Compute a confusion matrix/contingency table. Parameters y_target : array-like, shape=[n_samples] True class labels. y_predicted : array-like, shape=[n_samples] Predicted class labels. binary : bool (default: False) Maps a multi-class problem onto a binary confusion matrix, where the positive class is 1 and all other classes are 0. positive_label : int (default: 1) Class label of the positive class. Returns mat : array-like, shape=[n_classes, n_classes] Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix/","title":"confusion_matrix"},{"location":"api_modules/mlxtend.evaluate/create_counterfactual/","text":"create_counterfactual create_counterfactual(x_reference, y_desired, model, X_dataset, y_desired_proba=None, lammbda=0.1, random_seed=None) Implementation of the counterfactual method by Wachter et al. 2017 References: - Wachter, S., Mittelstadt, B., & Russell, C. (2017). Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harv. JL & Tech., 31, 841., https://arxiv.org/abs/1711.00399 Parameters x_reference : array-like, shape=[m_features] The data instance (training example) to be explained. y_desired : int The desired class label for x_reference . model : estimator A (scikit-learn) estimator implementing .predict() and/or predict_proba() . - If model supports predict_proba() , then this is used by default for the first loss term, (lambda * model.predict[_proba](x_counterfact) - y_desired[_proba])^2 - Otherwise, method will fall back to predict . X_dataset : array-like, shape=[n_examples, m_features] A (training) dataset for picking the initial counterfactual as initial value for starting the optimization procedure. y_desired_proba : float (default: None) A float within the range [0, 1] designating the desired class probability for y_desired . - If y_desired_proba=None (default), the first loss term is (lambda * model(x_counterfact) - y_desired)^2 where y_desired is a class label - If y_desired_proba is not None, the first loss term is (lambda * model(x_counterfact) - y_desired_proba)^2 lammbda : Weighting parameter for the first loss term, (lambda * model(x_counterfact) - y_desired[_proba])^2 random_seed : int (default=None) If int, random_seed is the seed used by the random number generator for selecting the inital counterfactual from X_dataset .","title":"Create counterfactual"},{"location":"api_modules/mlxtend.evaluate/create_counterfactual/#create_counterfactual","text":"create_counterfactual(x_reference, y_desired, model, X_dataset, y_desired_proba=None, lammbda=0.1, random_seed=None) Implementation of the counterfactual method by Wachter et al. 2017 References: - Wachter, S., Mittelstadt, B., & Russell, C. (2017). Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harv. JL & Tech., 31, 841., https://arxiv.org/abs/1711.00399 Parameters x_reference : array-like, shape=[m_features] The data instance (training example) to be explained. y_desired : int The desired class label for x_reference . model : estimator A (scikit-learn) estimator implementing .predict() and/or predict_proba() . - If model supports predict_proba() , then this is used by default for the first loss term, (lambda * model.predict[_proba](x_counterfact) - y_desired[_proba])^2 - Otherwise, method will fall back to predict . X_dataset : array-like, shape=[n_examples, m_features] A (training) dataset for picking the initial counterfactual as initial value for starting the optimization procedure. y_desired_proba : float (default: None) A float within the range [0, 1] designating the desired class probability for y_desired . - If y_desired_proba=None (default), the first loss term is (lambda * model(x_counterfact) - y_desired)^2 where y_desired is a class label - If y_desired_proba is not None, the first loss term is (lambda * model(x_counterfact) - y_desired_proba)^2 lammbda : Weighting parameter for the first loss term, (lambda * model(x_counterfact) - y_desired[_proba])^2 random_seed : int (default=None) If int, random_seed is the seed used by the random number generator for selecting the inital counterfactual from X_dataset .","title":"create_counterfactual"},{"location":"api_modules/mlxtend.evaluate/feature_importance_permutation/","text":"feature_importance_permutation feature_importance_permutation(X, y, predict_method, metric, num_rounds=1, feature_groups=None, seed=None) Feature importance imputation via permutation importance Parameters X : NumPy array, shape = [n_samples, n_features] Dataset, where n_samples is the number of samples and n_features is the number of features. y : NumPy array, shape = [n_samples] Target values. predict_method : prediction function A callable function that predicts the target values from X. metric : str, callable The metric for evaluating the feature importance through permutation. By default, the strings 'accuracy' is recommended for classifiers and the string 'r2' is recommended for regressors. Optionally, a custom scoring function (e.g., metric=scoring_func ) that accepts two arguments, y_true and y_pred, which have similar shape to the y array. num_rounds : int (default=1) Number of rounds the feature columns are permuted to compute the permutation importance. feature_groups : list or None (default=None) Optional argument for treating certain features as a group. For example [1, 2, [3, 4, 5]] , which can be useful for interpretability, for example, if features 3, 4, 5 are one-hot encoded features. seed : int or None (default=None) Random seed for permuting the feature columns. Returns mean_importance_vals, all_importance_vals : NumPy arrays. The first array, mean_importance_vals has shape [n_features, ] and contains the importance values for all features. The shape of the second array is [n_features, num_rounds] and contains the feature importance for each repetition. If num_rounds=1, it contains the same values as the first array, mean_importance_vals. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/","title":"Feature importance permutation"},{"location":"api_modules/mlxtend.evaluate/feature_importance_permutation/#feature_importance_permutation","text":"feature_importance_permutation(X, y, predict_method, metric, num_rounds=1, feature_groups=None, seed=None) Feature importance imputation via permutation importance Parameters X : NumPy array, shape = [n_samples, n_features] Dataset, where n_samples is the number of samples and n_features is the number of features. y : NumPy array, shape = [n_samples] Target values. predict_method : prediction function A callable function that predicts the target values from X. metric : str, callable The metric for evaluating the feature importance through permutation. By default, the strings 'accuracy' is recommended for classifiers and the string 'r2' is recommended for regressors. Optionally, a custom scoring function (e.g., metric=scoring_func ) that accepts two arguments, y_true and y_pred, which have similar shape to the y array. num_rounds : int (default=1) Number of rounds the feature columns are permuted to compute the permutation importance. feature_groups : list or None (default=None) Optional argument for treating certain features as a group. For example [1, 2, [3, 4, 5]] , which can be useful for interpretability, for example, if features 3, 4, 5 are one-hot encoded features. seed : int or None (default=None) Random seed for permuting the feature columns. Returns mean_importance_vals, all_importance_vals : NumPy arrays. The first array, mean_importance_vals has shape [n_features, ] and contains the importance values for all features. The shape of the second array is [n_features, num_rounds] and contains the feature importance for each repetition. If num_rounds=1, it contains the same values as the first array, mean_importance_vals. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/","title":"feature_importance_permutation"},{"location":"api_modules/mlxtend.evaluate/ftest/","text":"ftest ftest(y_target, y_model_predictions)* F-Test test to compare 2 or more models. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. *y_model_predictions : array-likes, shape=[n_samples] Variable number of 2 or more arrays that contain the predicted class labels from models as 1D NumPy array. Returns f, p : float or None, float Returns the F-value and the p-value Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/ftest/","title":"Ftest"},{"location":"api_modules/mlxtend.evaluate/ftest/#ftest","text":"ftest(y_target, y_model_predictions)* F-Test test to compare 2 or more models. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. *y_model_predictions : array-likes, shape=[n_samples] Variable number of 2 or more arrays that contain the predicted class labels from models as 1D NumPy array. Returns f, p : float or None, float Returns the F-value and the p-value Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/ftest/","title":"ftest"},{"location":"api_modules/mlxtend.evaluate/lift_score/","text":"lift_score lift_score(y_target, y_predicted, binary=True, positive_label=1) Lift measures the degree to which the predictions of a classification model are better than randomly-generated predictions. The in terms of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN), the lift score is computed as: [ TP / (TP+FP) ] / [ (TP+FN) / (TP+TN+FP+FN) ] Parameters y_target : array-like, shape=[n_samples] True class labels. y_predicted : array-like, shape=[n_samples] Predicted class labels. binary : bool (default: True) Maps a multi-class problem onto a binary, where the positive class is 1 and all other classes are 0. positive_label : int (default: 0) Class label of the positive class. Returns score : float Lift score in the range [0, infinity] Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/lift_score/","title":"Lift score"},{"location":"api_modules/mlxtend.evaluate/lift_score/#lift_score","text":"lift_score(y_target, y_predicted, binary=True, positive_label=1) Lift measures the degree to which the predictions of a classification model are better than randomly-generated predictions. The in terms of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN), the lift score is computed as: [ TP / (TP+FP) ] / [ (TP+FN) / (TP+TN+FP+FN) ] Parameters y_target : array-like, shape=[n_samples] True class labels. y_predicted : array-like, shape=[n_samples] Predicted class labels. binary : bool (default: True) Maps a multi-class problem onto a binary, where the positive class is 1 and all other classes are 0. positive_label : int (default: 0) Class label of the positive class. Returns score : float Lift score in the range [0, infinity] Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/lift_score/","title":"lift_score"},{"location":"api_modules/mlxtend.evaluate/mcnemar/","text":"mcnemar mcnemar(ary, corrected=True, exact=False) McNemar test for paired nominal data Parameters ary : array-like, shape=[2, 2] 2 x 2 contigency table (as returned by evaluate.mcnemar_table), where a: ary[0, 0]: # of samples that both models predicted correctly b: ary[0, 1]: # of samples that model 1 got right and model 2 got wrong c: ary[1, 0]: # of samples that model 2 got right and model 1 got wrong d: aryCell [1, 1]: # of samples that both models predicted incorrectly corrected : array-like, shape=[n_samples] (default: True) Uses Edward's continuity correction for chi-squared if True exact : bool, (default: False) If True , uses an exact binomial test comparing b to a binomial distribution with n = b + c and p = 0.5. It is highly recommended to use exact=True for sample sizes < 25 since chi-squared is not well-approximated by the chi-squared distribution! Returns chi2, p : float or None, float Returns the chi-squared value and the p-value; if exact=True (default: False ), chi2 is None Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar/","title":"Mcnemar"},{"location":"api_modules/mlxtend.evaluate/mcnemar/#mcnemar","text":"mcnemar(ary, corrected=True, exact=False) McNemar test for paired nominal data Parameters ary : array-like, shape=[2, 2] 2 x 2 contigency table (as returned by evaluate.mcnemar_table), where a: ary[0, 0]: # of samples that both models predicted correctly b: ary[0, 1]: # of samples that model 1 got right and model 2 got wrong c: ary[1, 0]: # of samples that model 2 got right and model 1 got wrong d: aryCell [1, 1]: # of samples that both models predicted incorrectly corrected : array-like, shape=[n_samples] (default: True) Uses Edward's continuity correction for chi-squared if True exact : bool, (default: False) If True , uses an exact binomial test comparing b to a binomial distribution with n = b + c and p = 0.5. It is highly recommended to use exact=True for sample sizes < 25 since chi-squared is not well-approximated by the chi-squared distribution! Returns chi2, p : float or None, float Returns the chi-squared value and the p-value; if exact=True (default: False ), chi2 is None Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar/","title":"mcnemar"},{"location":"api_modules/mlxtend.evaluate/mcnemar_table/","text":"mcnemar_table mcnemar_table(y_target, y_model1, y_model2) Compute a 2x2 contigency table for McNemar's test. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. y_model1 : array-like, shape=[n_samples] Predicted class labels from model as 1D NumPy array. y_model2 : array-like, shape=[n_samples] Predicted class labels from model 2 as 1D NumPy array. Returns tb : array-like, shape=[2, 2] 2x2 contingency table with the following contents: a: tb[0, 0]: # of samples that both models predicted correctly b: tb[0, 1]: # of samples that model 1 got right and model 2 got wrong c: tb[1, 0]: # of samples that model 2 got right and model 1 got wrong d: tb[1, 1]: # of samples that both models predicted incorrectly Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar_table/","title":"Mcnemar table"},{"location":"api_modules/mlxtend.evaluate/mcnemar_table/#mcnemar_table","text":"mcnemar_table(y_target, y_model1, y_model2) Compute a 2x2 contigency table for McNemar's test. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. y_model1 : array-like, shape=[n_samples] Predicted class labels from model as 1D NumPy array. y_model2 : array-like, shape=[n_samples] Predicted class labels from model 2 as 1D NumPy array. Returns tb : array-like, shape=[2, 2] 2x2 contingency table with the following contents: a: tb[0, 0]: # of samples that both models predicted correctly b: tb[0, 1]: # of samples that model 1 got right and model 2 got wrong c: tb[1, 0]: # of samples that model 2 got right and model 1 got wrong d: tb[1, 1]: # of samples that both models predicted incorrectly Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar_table/","title":"mcnemar_table"},{"location":"api_modules/mlxtend.evaluate/mcnemar_tables/","text":"mcnemar_tables mcnemar_tables(y_target, y_model_predictions)* Compute multiple 2x2 contigency tables for McNemar's test or Cochran's Q test. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. y_model_predictions : array-like, shape=[n_samples] Predicted class labels for a model. Returns tables : dict Dictionary of NumPy arrays with shape=[2, 2]. Each dictionary key names the two models to be compared based on the order the models were passed as *y_model_predictions . The number of dictionary entries is equal to the number of pairwise combinations between the m models, i.e., \"m choose 2.\" For example the following target array (containing the true labels) and 3 models y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) y_mod0 = np.array([0, 1, 0, 0, 0, 1, 1, 0, 0, 0]) y_mod1 = np.array([0, 0, 1, 1, 0, 1, 1, 0, 0, 0]) y_mod2 = np.array([0, 1, 1, 1, 0, 1, 0, 0, 0, 0]) would result in the following dictionary: {'model_0 vs model_1': array([[ 4., 1.], [ 2., 3.]]), 'model_0 vs model_2': array([[ 3., 0.], [ 3., 4.]]), 'model_1 vs model_2': array([[ 3., 0.], [ 2., 5.]])} Each array is structured in the following way: tb[0, 0]: # of samples that both models predicted correctly tb[0, 1]: # of samples that model a got right and model b got wrong tb[1, 0]: # of samples that model b got right and model a got wrong tb[1, 1]: # of samples that both models predicted incorrectly Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar_tables/","title":"Mcnemar tables"},{"location":"api_modules/mlxtend.evaluate/mcnemar_tables/#mcnemar_tables","text":"mcnemar_tables(y_target, y_model_predictions)* Compute multiple 2x2 contigency tables for McNemar's test or Cochran's Q test. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. y_model_predictions : array-like, shape=[n_samples] Predicted class labels for a model. Returns tables : dict Dictionary of NumPy arrays with shape=[2, 2]. Each dictionary key names the two models to be compared based on the order the models were passed as *y_model_predictions . The number of dictionary entries is equal to the number of pairwise combinations between the m models, i.e., \"m choose 2.\" For example the following target array (containing the true labels) and 3 models y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) y_mod0 = np.array([0, 1, 0, 0, 0, 1, 1, 0, 0, 0]) y_mod1 = np.array([0, 0, 1, 1, 0, 1, 1, 0, 0, 0]) y_mod2 = np.array([0, 1, 1, 1, 0, 1, 0, 0, 0, 0]) would result in the following dictionary: {'model_0 vs model_1': array([[ 4., 1.], [ 2., 3.]]), 'model_0 vs model_2': array([[ 3., 0.], [ 3., 4.]]), 'model_1 vs model_2': array([[ 3., 0.], [ 2., 5.]])} Each array is structured in the following way: tb[0, 0]: # of samples that both models predicted correctly tb[0, 1]: # of samples that model a got right and model b got wrong tb[1, 0]: # of samples that model b got right and model a got wrong tb[1, 1]: # of samples that both models predicted incorrectly Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar_tables/","title":"mcnemar_tables"},{"location":"api_modules/mlxtend.evaluate/paired_ttest_5x2cv/","text":"paired_ttest_5x2cv paired_ttest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None) Implements the 5x2cv paired t test proposed by Dieterrich (1998) to compare the performance of two models. Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. random_seed : int or None (default: None) Random seed for creating the test/train splits. Returns t : float The t-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_5x2cv/","title":"Paired ttest 5x2cv"},{"location":"api_modules/mlxtend.evaluate/paired_ttest_5x2cv/#paired_ttest_5x2cv","text":"paired_ttest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None) Implements the 5x2cv paired t test proposed by Dieterrich (1998) to compare the performance of two models. Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. random_seed : int or None (default: None) Random seed for creating the test/train splits. Returns t : float The t-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_5x2cv/","title":"paired_ttest_5x2cv"},{"location":"api_modules/mlxtend.evaluate/paired_ttest_kfold_cv/","text":"paired_ttest_kfold_cv paired_ttest_kfold_cv(estimator1, estimator2, X, y, cv=10, scoring=None, shuffle=False, random_seed=None) Implements the k-fold paired t test procedure to compare the performance of two models. Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. cv : int (default: 10) Number of splits and iteration for the cross-validation procedure scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. shuffle : bool (default: True) Whether to shuffle the dataset for generating the k-fold splits. random_seed : int or None (default: None) Random seed for shuffling the dataset for generating the k-fold splits. Ignored if shuffle=False. Returns t : float The t-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_kfold_cv/","title":"Paired ttest kfold cv"},{"location":"api_modules/mlxtend.evaluate/paired_ttest_kfold_cv/#paired_ttest_kfold_cv","text":"paired_ttest_kfold_cv(estimator1, estimator2, X, y, cv=10, scoring=None, shuffle=False, random_seed=None) Implements the k-fold paired t test procedure to compare the performance of two models. Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. cv : int (default: 10) Number of splits and iteration for the cross-validation procedure scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. shuffle : bool (default: True) Whether to shuffle the dataset for generating the k-fold splits. random_seed : int or None (default: None) Random seed for shuffling the dataset for generating the k-fold splits. Ignored if shuffle=False. Returns t : float The t-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_kfold_cv/","title":"paired_ttest_kfold_cv"},{"location":"api_modules/mlxtend.evaluate/paired_ttest_resampled/","text":"paired_ttest_resampled paired_ttest_resampled(estimator1, estimator2, X, y, num_rounds=30, test_size=0.3, scoring=None, random_seed=None) Implements the resampled paired t test procedure to compare the performance of two models (also called k-hold-out paired t test). Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. num_rounds : int (default: 30) Number of resampling iterations (i.e., train/test splits) test_size : float or int (default: 0.3) If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to use as a test set. If int, represents the absolute number of test exsamples. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. random_seed : int or None (default: None) Random seed for creating the test/train splits. Returns t : float The t-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_resampled/","title":"Paired ttest resampled"},{"location":"api_modules/mlxtend.evaluate/paired_ttest_resampled/#paired_ttest_resampled","text":"paired_ttest_resampled(estimator1, estimator2, X, y, num_rounds=30, test_size=0.3, scoring=None, random_seed=None) Implements the resampled paired t test procedure to compare the performance of two models (also called k-hold-out paired t test). Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. num_rounds : int (default: 30) Number of resampling iterations (i.e., train/test splits) test_size : float or int (default: 0.3) If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to use as a test set. If int, represents the absolute number of test exsamples. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. random_seed : int or None (default: None) Random seed for creating the test/train splits. Returns t : float The t-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_resampled/","title":"paired_ttest_resampled"},{"location":"api_modules/mlxtend.evaluate/permutation_test/","text":"permutation_test permutation_test(x, y, func='x_mean != y_mean', method='exact', num_rounds=1000, seed=None, paired=False) Nonparametric permutation test Parameters x : list or numpy array with shape (n_datapoints,) A list or 1D numpy array of the first sample (e.g., the treatment group). y : list or numpy array with shape (n_datapoints,) A list or 1D numpy array of the second sample (e.g., the control group). func : custom function or str (default: 'x_mean != y_mean') function to compute the statistic for the permutation test. - If 'x_mean != y_mean', uses func=lambda x, y: np.abs(np.mean(x) - np.mean(y))) for a two-sided test. - If 'x_mean > y_mean', uses func=lambda x, y: np.mean(x) - np.mean(y)) for a one-sided test. - If 'x_mean < y_mean', uses func=lambda x, y: np.mean(y) - np.mean(x)) for a one-sided test. method : 'approximate' or 'exact' (default: 'exact') If 'exact' (default), all possible permutations are considered. If 'approximate' the number of drawn samples is given by num_rounds . Note that 'exact' is typically not feasible unless the dataset size is relatively small. paired : bool If True, a paired test is performed by only exchanging each datapoint with its associate. num_rounds : int (default: 1000) The number of permutation samples if method='approximate' . seed : int or None (default: None) The random seed for generating permutation samples if method='approximate' . Returns p-value under the null hypothesis Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/permutation_test/","title":"Permutation test"},{"location":"api_modules/mlxtend.evaluate/permutation_test/#permutation_test","text":"permutation_test(x, y, func='x_mean != y_mean', method='exact', num_rounds=1000, seed=None, paired=False) Nonparametric permutation test Parameters x : list or numpy array with shape (n_datapoints,) A list or 1D numpy array of the first sample (e.g., the treatment group). y : list or numpy array with shape (n_datapoints,) A list or 1D numpy array of the second sample (e.g., the control group). func : custom function or str (default: 'x_mean != y_mean') function to compute the statistic for the permutation test. - If 'x_mean != y_mean', uses func=lambda x, y: np.abs(np.mean(x) - np.mean(y))) for a two-sided test. - If 'x_mean > y_mean', uses func=lambda x, y: np.mean(x) - np.mean(y)) for a one-sided test. - If 'x_mean < y_mean', uses func=lambda x, y: np.mean(y) - np.mean(x)) for a one-sided test. method : 'approximate' or 'exact' (default: 'exact') If 'exact' (default), all possible permutations are considered. If 'approximate' the number of drawn samples is given by num_rounds . Note that 'exact' is typically not feasible unless the dataset size is relatively small. paired : bool If True, a paired test is performed by only exchanging each datapoint with its associate. num_rounds : int (default: 1000) The number of permutation samples if method='approximate' . seed : int or None (default: None) The random seed for generating permutation samples if method='approximate' . Returns p-value under the null hypothesis Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/permutation_test/","title":"permutation_test"},{"location":"api_modules/mlxtend.evaluate/proportion_difference/","text":"proportion_difference proportion_difference(proportion_1, proportion_2, n_1, n_2=None) Computes the test statistic and p-value for a difference of proportions test. Parameters proportion_1 : float The first proportion proportion_2 : float The second proportion n_1 : int The sample size of the first test sample n_2 : int or None (default=None) The sample size of the second test sample. If None , n_1 = n_2 . Returns z, p : float or None, float Returns the z-score and the p-value Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/proportion_difference/","title":"Proportion difference"},{"location":"api_modules/mlxtend.evaluate/proportion_difference/#proportion_difference","text":"proportion_difference(proportion_1, proportion_2, n_1, n_2=None) Computes the test statistic and p-value for a difference of proportions test. Parameters proportion_1 : float The first proportion proportion_2 : float The second proportion n_1 : int The sample size of the first test sample n_2 : int or None (default=None) The sample size of the second test sample. If None , n_1 = n_2 . Returns z, p : float or None, float Returns the z-score and the p-value Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/proportion_difference/","title":"proportion_difference"},{"location":"api_modules/mlxtend.evaluate/scoring/","text":"scoring scoring(y_target, y_predicted, metric='error', positive_label=1, unique_labels='auto') Compute a scoring metric for supervised learning. Parameters y_target : array-like, shape=[n_values] True class labels or target values. y_predicted : array-like, shape=[n_values] Predicted class labels or target values. metric : str (default: 'error') Performance metric: 'accuracy': (TP + TN)/(FP + FN + TP + TN) = 1-ERR 'average per-class accuracy': Average per-class accuracy 'average per-class error': Average per-class error 'balanced per-class accuracy': Average per-class accuracy 'balanced per-class error': Average per-class error 'error': (TP + TN)/(FP+ FN + TP + TN) = 1-ACC 'false_positive_rate': FP/N = FP/(FP + TN) 'true_positive_rate': TP/P = TP/(FN + TP) 'true_negative_rate': TN/N = TN/(FP + TN) 'precision': TP/(TP + FP) 'recall': equal to 'true_positive_rate' 'sensitivity': equal to 'true_positive_rate' or 'recall' 'specificity': equal to 'true_negative_rate' 'f1': 2 * (PRE * REC)/(PRE + REC) 'matthews_corr_coef': (TP TN - FP FN) / (sqrt{(TP + FP)( TP + FN )( TN + FP )( TN + FN )}) Where: [TP: True positives, TN = True negatives, TN: True negatives, FN = False negatives] positive_label : int (default: 1) Label of the positive class for binary classification metrics. unique_labels : str or array-like (default: 'auto') If 'auto', deduces the unique class labels from y_target Returns score : float Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/scoring/","title":"Scoring"},{"location":"api_modules/mlxtend.evaluate/scoring/#scoring","text":"scoring(y_target, y_predicted, metric='error', positive_label=1, unique_labels='auto') Compute a scoring metric for supervised learning. Parameters y_target : array-like, shape=[n_values] True class labels or target values. y_predicted : array-like, shape=[n_values] Predicted class labels or target values. metric : str (default: 'error') Performance metric: 'accuracy': (TP + TN)/(FP + FN + TP + TN) = 1-ERR 'average per-class accuracy': Average per-class accuracy 'average per-class error': Average per-class error 'balanced per-class accuracy': Average per-class accuracy 'balanced per-class error': Average per-class error 'error': (TP + TN)/(FP+ FN + TP + TN) = 1-ACC 'false_positive_rate': FP/N = FP/(FP + TN) 'true_positive_rate': TP/P = TP/(FN + TP) 'true_negative_rate': TN/N = TN/(FP + TN) 'precision': TP/(TP + FP) 'recall': equal to 'true_positive_rate' 'sensitivity': equal to 'true_positive_rate' or 'recall' 'specificity': equal to 'true_negative_rate' 'f1': 2 * (PRE * REC)/(PRE + REC) 'matthews_corr_coef': (TP TN - FP FN) / (sqrt{(TP + FP)( TP + FN )( TN + FP )( TN + FN )}) Where: [TP: True positives, TN = True negatives, TN: True negatives, FN = False negatives] positive_label : int (default: 1) Label of the positive class for binary classification metrics. unique_labels : str or array-like (default: 'auto') If 'auto', deduces the unique class labels from y_target Returns score : float Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/scoring/","title":"scoring"},{"location":"api_modules/mlxtend.feature_extraction/LinearDiscriminantAnalysis/","text":"LinearDiscriminantAnalysis LinearDiscriminantAnalysis(n_discriminants=None) Linear Discriminant Analysis Class Parameters n_discriminants : int (default: None) The number of discrimants for transformation. Keeps the original dimensions of the dataset if None . Attributes w_ : array-like, shape=[n_features, n_discriminants] Projection matrix e_vals_ : array-like, shape=[n_features] Eigenvalues in sorted order. e_vecs_ : array-like, shape=[n_features] Eigenvectors in sorted order. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_extraction/LinearDiscriminantAnalysis/ Methods fit(X, y, n_classes=None) Fit the LDA model with X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. n_classes : int (default: None) A positive integer to declare the number of class labels if not all class labels are present in a partial training set. Gets the number of class labels automatically if None. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause transform(X) Apply the linear transformation on X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_projected : np.ndarray, shape = [n_samples, n_discriminants] Projected training vectors.","title":"LinearDiscriminantAnalysis"},{"location":"api_modules/mlxtend.feature_extraction/LinearDiscriminantAnalysis/#lineardiscriminantanalysis","text":"LinearDiscriminantAnalysis(n_discriminants=None) Linear Discriminant Analysis Class Parameters n_discriminants : int (default: None) The number of discrimants for transformation. Keeps the original dimensions of the dataset if None . Attributes w_ : array-like, shape=[n_features, n_discriminants] Projection matrix e_vals_ : array-like, shape=[n_features] Eigenvalues in sorted order. e_vecs_ : array-like, shape=[n_features] Eigenvectors in sorted order. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_extraction/LinearDiscriminantAnalysis/","title":"LinearDiscriminantAnalysis"},{"location":"api_modules/mlxtend.feature_extraction/LinearDiscriminantAnalysis/#methods","text":"fit(X, y, n_classes=None) Fit the LDA model with X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. n_classes : int (default: None) A positive integer to declare the number of class labels if not all class labels are present in a partial training set. Gets the number of class labels automatically if None. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause transform(X) Apply the linear transformation on X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_projected : np.ndarray, shape = [n_samples, n_discriminants] Projected training vectors.","title":"Methods"},{"location":"api_modules/mlxtend.feature_extraction/PrincipalComponentAnalysis/","text":"PrincipalComponentAnalysis PrincipalComponentAnalysis(n_components=None, solver='svd', whitening=False) Principal Component Analysis Class Parameters n_components : int (default: None) The number of principal components for transformation. Keeps the original dimensions of the dataset if None . solver : str (default: 'svd') Method for performing the matrix decomposition. {'eigen', 'svd'} whitening : bool (default: False) Performs whitening such that the covariance matrix of the transformed data will be the identity matrix. Attributes w_ : array-like, shape=[n_features, n_components] Projection matrix e_vals_ : array-like, shape=[n_features] Eigenvalues in sorted order. e_vecs_ : array-like, shape=[n_features] Eigenvectors in sorted order. e_vals_normalized_ : array-like, shape=[n_features] Normalized eigen values such that they sum up to 1. This is equal to what's often referred to as \"explained variance ratios.\" loadings_ : array_like, shape=[n_features, n_features] The factor loadings of the original variables onto the principal components. The columns are the principal components, and the rows are the features loadings. For instance, the first column contains the loadings onto the first principal component. Note that the signs may be flipped depending on whether you use the 'eigen' or 'svd' solver; this does not affect the interpretation of the loadings though. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_extraction/PrincipalComponentAnalysis/ Methods fit(X, y=None) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause transform(X) Apply the linear transformation on X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_projected : np.ndarray, shape = [n_samples, n_components] Projected training vectors.","title":"PrincipalComponentAnalysis"},{"location":"api_modules/mlxtend.feature_extraction/PrincipalComponentAnalysis/#principalcomponentanalysis","text":"PrincipalComponentAnalysis(n_components=None, solver='svd', whitening=False) Principal Component Analysis Class Parameters n_components : int (default: None) The number of principal components for transformation. Keeps the original dimensions of the dataset if None . solver : str (default: 'svd') Method for performing the matrix decomposition. {'eigen', 'svd'} whitening : bool (default: False) Performs whitening such that the covariance matrix of the transformed data will be the identity matrix. Attributes w_ : array-like, shape=[n_features, n_components] Projection matrix e_vals_ : array-like, shape=[n_features] Eigenvalues in sorted order. e_vecs_ : array-like, shape=[n_features] Eigenvectors in sorted order. e_vals_normalized_ : array-like, shape=[n_features] Normalized eigen values such that they sum up to 1. This is equal to what's often referred to as \"explained variance ratios.\" loadings_ : array_like, shape=[n_features, n_features] The factor loadings of the original variables onto the principal components. The columns are the principal components, and the rows are the features loadings. For instance, the first column contains the loadings onto the first principal component. Note that the signs may be flipped depending on whether you use the 'eigen' or 'svd' solver; this does not affect the interpretation of the loadings though. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_extraction/PrincipalComponentAnalysis/","title":"PrincipalComponentAnalysis"},{"location":"api_modules/mlxtend.feature_extraction/PrincipalComponentAnalysis/#methods","text":"fit(X, y=None) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause transform(X) Apply the linear transformation on X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_projected : np.ndarray, shape = [n_samples, n_components] Projected training vectors.","title":"Methods"},{"location":"api_modules/mlxtend.feature_extraction/RBFKernelPCA/","text":"RBFKernelPCA RBFKernelPCA(gamma=15.0, n_components=None, copy_X=True) RBF Kernel Principal Component Analysis for dimensionality reduction. Parameters gamma : float (default: 15.0) Free parameter (coefficient) of the RBF kernel. n_components : int (default: None) The number of principal components for transformation. Keeps the original dimensions of the dataset if None . copy_X : bool (default: True) Copies training data, which is required to compute the projection of new data via the transform method. Uses a reference to X if False. Attributes e_vals_ : array-like, shape=[n_features] Eigenvalues in sorted order. e_vecs_ : array-like, shape=[n_features] Eigenvectors in sorted order. X_projected_ : array-like, shape=[n_samples, n_components] Training samples projected along the component axes. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_extraction/RBFKernelPCA/ Methods fit(X) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause transform(X) Apply the non-linear transformation on X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_projected : np.ndarray, shape = [n_samples, n_components] Projected training vectors.","title":"RBFKernelPCA"},{"location":"api_modules/mlxtend.feature_extraction/RBFKernelPCA/#rbfkernelpca","text":"RBFKernelPCA(gamma=15.0, n_components=None, copy_X=True) RBF Kernel Principal Component Analysis for dimensionality reduction. Parameters gamma : float (default: 15.0) Free parameter (coefficient) of the RBF kernel. n_components : int (default: None) The number of principal components for transformation. Keeps the original dimensions of the dataset if None . copy_X : bool (default: True) Copies training data, which is required to compute the projection of new data via the transform method. Uses a reference to X if False. Attributes e_vals_ : array-like, shape=[n_features] Eigenvalues in sorted order. e_vecs_ : array-like, shape=[n_features] Eigenvectors in sorted order. X_projected_ : array-like, shape=[n_samples, n_components] Training samples projected along the component axes. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_extraction/RBFKernelPCA/","title":"RBFKernelPCA"},{"location":"api_modules/mlxtend.feature_extraction/RBFKernelPCA/#methods","text":"fit(X) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause transform(X) Apply the non-linear transformation on X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_projected : np.ndarray, shape = [n_samples, n_components] Projected training vectors.","title":"Methods"},{"location":"api_modules/mlxtend.feature_selection/ColumnSelector/","text":"ColumnSelector ColumnSelector(cols=None, drop_axis=False) Object for selecting specific columns from a data set. Parameters cols : array-like (default: None) A list specifying the feature indices to be selected. For example, [1, 4, 5] to select the 2nd, 5th, and 6th feature columns, and ['A','C','D'] to select the name of feature columns A, C and D. If None, returns all columns in the array. drop_axis : bool (default=False) Drops last axis if True and the only one column is selected. This is useful, e.g., when the ColumnSelector is used for selecting only one column and the resulting array should be fed to e.g., a scikit-learn column selector. E.g., instead of returning an array with shape (n_samples, 1), drop_axis=True will return an aray with shape (n_samples,). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_selection/ColumnSelector/ Methods fit(X, y=None) Mock method. Does nothing. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns self fit_transform(X, y=None) Return a slice of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_slice : shape = [n_samples, k_features] Subset of the feature space where k_features <= n_features get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X, y=None) Return a slice of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_slice : shape = [n_samples, k_features] Subset of the feature space where k_features <= n_features","title":"ColumnSelector"},{"location":"api_modules/mlxtend.feature_selection/ColumnSelector/#columnselector","text":"ColumnSelector(cols=None, drop_axis=False) Object for selecting specific columns from a data set. Parameters cols : array-like (default: None) A list specifying the feature indices to be selected. For example, [1, 4, 5] to select the 2nd, 5th, and 6th feature columns, and ['A','C','D'] to select the name of feature columns A, C and D. If None, returns all columns in the array. drop_axis : bool (default=False) Drops last axis if True and the only one column is selected. This is useful, e.g., when the ColumnSelector is used for selecting only one column and the resulting array should be fed to e.g., a scikit-learn column selector. E.g., instead of returning an array with shape (n_samples, 1), drop_axis=True will return an aray with shape (n_samples,). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_selection/ColumnSelector/","title":"ColumnSelector"},{"location":"api_modules/mlxtend.feature_selection/ColumnSelector/#methods","text":"fit(X, y=None) Mock method. Does nothing. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns self fit_transform(X, y=None) Return a slice of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_slice : shape = [n_samples, k_features] Subset of the feature space where k_features <= n_features get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X, y=None) Return a slice of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_slice : shape = [n_samples, k_features] Subset of the feature space where k_features <= n_features","title":"Methods"},{"location":"api_modules/mlxtend.feature_selection/ExhaustiveFeatureSelector/","text":"ExhaustiveFeatureSelector ExhaustiveFeatureSelector(estimator, min_features=1, max_features=1, print_progress=True, scoring='accuracy', cv=5, n_jobs=1, pre_dispatch='2 n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)* Exhaustive Feature Selection for Classification and Regression. (new in v0.4.3) Parameters estimator : scikit-learn classifier or regressor min_features : int (default: 1) Minumum number of features to select max_features : int (default: 1) Maximum number of features to select. If parameter feature_groups is not None, the number of features is equal to the number of feature groups, i.e. len(feature_groups) . For example, if feature_groups = [[0], [1], [2, 3], [4]] , then the max_features value cannot exceed 4. print_progress : bool (default: True) Prints progress as the number of epochs to stderr. scoring : str, (default='accuracy') Scoring metric in {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'r2'} for regressors, or a callable object or function with signature scorer(estimator, X, y) . cv : int (default: 5) Scikit-learn cross-validation generator or int . If estimator is a classifier (or y consists of integer class labels), stratified k-fold is performed, and regular k-fold cross-validation otherwise. No cross-validation if cv is None, False, or 0. n_jobs : int (default: 1) The number of CPUs to use for evaluating different feature subsets in parallel. -1 means 'all CPUs'. pre_dispatch : int, or string (default: '2*n_jobs') Controls the number of jobs that get dispatched during parallel execution if n_jobs > 1 or n_jobs=-1 . Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs clone_estimator : bool (default: True) Clones estimator if True; works with the original estimator instance if False. Set to False if the estimator doesn't implement scikit-learn's set_params and get_params methods. In addition, it is required to set cv=0, and n_jobs=1. fixed_features : tuple (default: None) If not None , the feature indices provided as a tuple will be regarded as fixed by the feature selector. For example, if fixed_features=(1, 3, 7) , the 2nd, 4th, and 8th feature are guaranteed to be present in the solution. Note that if fixed_features is not None , make sure that the number of features to be selected is greater than len(fixed_features) . In other words, ensure that k_features > len(fixed_features) . feature_groups : list or None (default: None) Optional argument for treating certain features as a group. This means, the features within a group are always selected together, never split. For example, feature_groups=[[1], [2], [3, 4, 5]] specifies 3 feature groups.In this case, possible feature selection results with k_features=2 are [[1], [2] , [[1], [3, 4, 5]] , or [[2], [3, 4, 5]] . Feature groups can be useful for interpretability, for example, if features 3, 4, 5 are one-hot encoded features. (For more details, please read the notes at the bottom of this docstring). New in mlxtend v. 0.21.0. Attributes best_idx_ : array-like, shape = [n_predictions] Feature Indices of the selected feature subsets. best_feature_names_ : array-like, shape = [n_predictions] Feature names of the selected feature subsets. If pandas DataFrames are used in the fit method, the feature names correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. New in v 0.13.0. best_score_ : float Cross validation average score of the selected subset. subsets_ : dict A dictionary of selected feature subsets during the exhaustive selection, where the dictionary keys are the lengths k of these feature subsets. The dictionary values are dictionaries themselves with the following keys: 'feature_idx' (tuple of indices of the feature subset) 'feature_names' (tuple of feature names of the feat. subset) 'cv_scores' (list individual cross-validation scores) 'avg_score' (average cross-validation score) Note that if pandas DataFrames are used in the fit method, the 'feature_names' correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. The 'feature_names' is new in v. 0.13.0. Notes (1) If parameter feature_groups is not None, the number of features is equal to the number of feature groups, i.e. len(feature_groups) . For example, if feature_groups = [[0], [1], [2, 3], [4]] , then the max_features value cannot exceed 4. (2) Although two or more individual features may be considered as one group throughout the feature-selection process, it does not mean the individual features of that group have the same impact on the outcome. For instance, in linear regression, the coefficient of the feature 2 and 3 can be different even if they are considered as one group in feature_groups. (3) If both fixed_features and feature_groups are specified, ensure that each feature group contains the fixed_features selection. E.g., for a 3-feature set fixed_features=[0, 1] and feature_groups=[[0, 1], [2]] is valid; fixed_features=[0, 1] and feature_groups=[[0], [1, 2]] is not valid. (4) In case of KeyboardInterrupt, the dictionary subsets may not be completed. If user is still interested in getting the best score, they can use method `finalize_fit`. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/ Methods finalize_fit() None fit(X, y, groups=None, fit_params) Perform feature selection and learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : dict of string -> object, optional Parameters to pass to to the fit method of classifier. Returns self : object fit_transform(X, y, groups=None, fit_params) Fit to training data and return the best selected features from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : dict of string -> object, optional Parameters to pass to to the fit method of classifier. Returns Feature subset of X, shape={n_samples, k_features} get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_metric_dict(confidence_interval=0.95) Return metric dictionary Parameters confidence_interval : float (default: 0.95) A positive float between 0.0 and 1.0 to compute the confidence interval bounds of the CV score averages. Returns Dictionary with items where each dictionary value is a list with the number of iterations (number of feature subsets) as its length. The dictionary keys corresponding to these lists are as follows: 'feature_idx': tuple of the indices of the feature subset 'cv_scores': list with individual CV scores 'avg_score': of CV average scores 'std_dev': standard deviation of the CV score average 'std_err': standard error of the CV score average 'ci_bound': confidence interval bound of the CV score average get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_fit_request(self: mlxtend.feature_selection.exhaustive_feature_selector.ExhaustiveFeatureSelector, , groups: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.feature_selection.exhaustive_feature_selector.ExhaustiveFeatureSelector* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . Returns self : object The updated object. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X) Return the best selected features from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. Returns Feature subset of X, shape={n_samples, k_features}","title":"ExhaustiveFeatureSelector"},{"location":"api_modules/mlxtend.feature_selection/ExhaustiveFeatureSelector/#exhaustivefeatureselector","text":"ExhaustiveFeatureSelector(estimator, min_features=1, max_features=1, print_progress=True, scoring='accuracy', cv=5, n_jobs=1, pre_dispatch='2 n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)* Exhaustive Feature Selection for Classification and Regression. (new in v0.4.3) Parameters estimator : scikit-learn classifier or regressor min_features : int (default: 1) Minumum number of features to select max_features : int (default: 1) Maximum number of features to select. If parameter feature_groups is not None, the number of features is equal to the number of feature groups, i.e. len(feature_groups) . For example, if feature_groups = [[0], [1], [2, 3], [4]] , then the max_features value cannot exceed 4. print_progress : bool (default: True) Prints progress as the number of epochs to stderr. scoring : str, (default='accuracy') Scoring metric in {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'r2'} for regressors, or a callable object or function with signature scorer(estimator, X, y) . cv : int (default: 5) Scikit-learn cross-validation generator or int . If estimator is a classifier (or y consists of integer class labels), stratified k-fold is performed, and regular k-fold cross-validation otherwise. No cross-validation if cv is None, False, or 0. n_jobs : int (default: 1) The number of CPUs to use for evaluating different feature subsets in parallel. -1 means 'all CPUs'. pre_dispatch : int, or string (default: '2*n_jobs') Controls the number of jobs that get dispatched during parallel execution if n_jobs > 1 or n_jobs=-1 . Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs clone_estimator : bool (default: True) Clones estimator if True; works with the original estimator instance if False. Set to False if the estimator doesn't implement scikit-learn's set_params and get_params methods. In addition, it is required to set cv=0, and n_jobs=1. fixed_features : tuple (default: None) If not None , the feature indices provided as a tuple will be regarded as fixed by the feature selector. For example, if fixed_features=(1, 3, 7) , the 2nd, 4th, and 8th feature are guaranteed to be present in the solution. Note that if fixed_features is not None , make sure that the number of features to be selected is greater than len(fixed_features) . In other words, ensure that k_features > len(fixed_features) . feature_groups : list or None (default: None) Optional argument for treating certain features as a group. This means, the features within a group are always selected together, never split. For example, feature_groups=[[1], [2], [3, 4, 5]] specifies 3 feature groups.In this case, possible feature selection results with k_features=2 are [[1], [2] , [[1], [3, 4, 5]] , or [[2], [3, 4, 5]] . Feature groups can be useful for interpretability, for example, if features 3, 4, 5 are one-hot encoded features. (For more details, please read the notes at the bottom of this docstring). New in mlxtend v. 0.21.0. Attributes best_idx_ : array-like, shape = [n_predictions] Feature Indices of the selected feature subsets. best_feature_names_ : array-like, shape = [n_predictions] Feature names of the selected feature subsets. If pandas DataFrames are used in the fit method, the feature names correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. New in v 0.13.0. best_score_ : float Cross validation average score of the selected subset. subsets_ : dict A dictionary of selected feature subsets during the exhaustive selection, where the dictionary keys are the lengths k of these feature subsets. The dictionary values are dictionaries themselves with the following keys: 'feature_idx' (tuple of indices of the feature subset) 'feature_names' (tuple of feature names of the feat. subset) 'cv_scores' (list individual cross-validation scores) 'avg_score' (average cross-validation score) Note that if pandas DataFrames are used in the fit method, the 'feature_names' correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. The 'feature_names' is new in v. 0.13.0. Notes (1) If parameter feature_groups is not None, the number of features is equal to the number of feature groups, i.e. len(feature_groups) . For example, if feature_groups = [[0], [1], [2, 3], [4]] , then the max_features value cannot exceed 4. (2) Although two or more individual features may be considered as one group throughout the feature-selection process, it does not mean the individual features of that group have the same impact on the outcome. For instance, in linear regression, the coefficient of the feature 2 and 3 can be different even if they are considered as one group in feature_groups. (3) If both fixed_features and feature_groups are specified, ensure that each feature group contains the fixed_features selection. E.g., for a 3-feature set fixed_features=[0, 1] and feature_groups=[[0, 1], [2]] is valid; fixed_features=[0, 1] and feature_groups=[[0], [1, 2]] is not valid. (4) In case of KeyboardInterrupt, the dictionary subsets may not be completed. If user is still interested in getting the best score, they can use method `finalize_fit`. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/","title":"ExhaustiveFeatureSelector"},{"location":"api_modules/mlxtend.feature_selection/ExhaustiveFeatureSelector/#methods","text":"finalize_fit() None fit(X, y, groups=None, fit_params) Perform feature selection and learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : dict of string -> object, optional Parameters to pass to to the fit method of classifier. Returns self : object fit_transform(X, y, groups=None, fit_params) Fit to training data and return the best selected features from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : dict of string -> object, optional Parameters to pass to to the fit method of classifier. Returns Feature subset of X, shape={n_samples, k_features} get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_metric_dict(confidence_interval=0.95) Return metric dictionary Parameters confidence_interval : float (default: 0.95) A positive float between 0.0 and 1.0 to compute the confidence interval bounds of the CV score averages. Returns Dictionary with items where each dictionary value is a list with the number of iterations (number of feature subsets) as its length. The dictionary keys corresponding to these lists are as follows: 'feature_idx': tuple of the indices of the feature subset 'cv_scores': list with individual CV scores 'avg_score': of CV average scores 'std_dev': standard deviation of the CV score average 'std_err': standard error of the CV score average 'ci_bound': confidence interval bound of the CV score average get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_fit_request(self: mlxtend.feature_selection.exhaustive_feature_selector.ExhaustiveFeatureSelector, , groups: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.feature_selection.exhaustive_feature_selector.ExhaustiveFeatureSelector* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . Returns self : object The updated object. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X) Return the best selected features from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. Returns Feature subset of X, shape={n_samples, k_features}","title":"Methods"},{"location":"api_modules/mlxtend.feature_selection/SequentialFeatureSelector/","text":"SequentialFeatureSelector SequentialFeatureSelector(estimator, k_features=1, forward=True, floating=False, verbose=0, scoring=None, cv=5, n_jobs=1, pre_dispatch='2 n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)* Sequential Feature Selection for Classification and Regression. Parameters estimator : scikit-learn classifier or regressor k_features : int or tuple or str (default: 1) Number of features to select, where k_features < the full feature set. New in 0.4.2: A tuple containing a min and max value can be provided, and the SFS will consider return any feature combination between min and max that scored highest in cross-validation. For example, the tuple (1, 4) will return any combination from 1 up to 4 features instead of a fixed number of features k. New in 0.8.0: A string argument \"best\" or \"parsimonious\". If \"best\" is provided, the feature selector will return the feature subset with the best cross-validation performance. If \"parsimonious\" is provided as an argument, the smallest feature subset that is within one standard error of the cross-validation performance will be selected. forward : bool (default: True) Forward selection if True, backward selection otherwise floating : bool (default: False) Adds a conditional exclusion/inclusion if True. verbose : int (default: 0), level of verbosity to use in logging. If 0, no output, if 1 number of features in current set, if 2 detailed logging i ncluding timestamp and cv scores at step. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. cv : int (default: 5) Integer or iterable yielding train, test splits. If cv is an integer and estimator is a classifier (or y consists of integer class labels) stratified k-fold. Otherwise regular k-fold cross-validation is performed. No cross-validation if cv is None, False, or 0. n_jobs : int (default: 1) The number of CPUs to use for evaluating different feature subsets in parallel. -1 means 'all CPUs'. pre_dispatch : int, or string (default: '2*n_jobs') Controls the number of jobs that get dispatched during parallel execution if n_jobs > 1 or n_jobs=-1 . Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs clone_estimator : bool (default: True) Clones estimator if True; works with the original estimator instance if False. Set to False if the estimator doesn't implement scikit-learn's set_params and get_params methods. In addition, it is required to set cv=0, and n_jobs=1. fixed_features : tuple (default: None) If not None , the feature indices provided as a tuple will be regarded as fixed by the feature selector. For example, if fixed_features=(1, 3, 7) , the 2nd, 4th, and 8th feature are guaranteed to be present in the solution. Note that if fixed_features is not None , make sure that the number of features to be selected is greater than len(fixed_features) . In other words, ensure that k_features > len(fixed_features) . New in mlxtend v. 0.18.0. feature_groups : list or None (default: None) Optional argument for treating certain features as a group. This means, the features within a group are always selected together, never split. For example, feature_groups=[[1], [2], [3, 4, 5]] specifies 3 feature groups. In this case, possible feature selection results with k_features=2 are [[1], [2] , [[1], [3, 4, 5]] , or [[2], [3, 4, 5]] . Feature groups can be useful for interpretability, for example, if features 3, 4, 5 are one-hot encoded features. (For more details, please read the notes at the bottom of this docstring). New in mlxtend v. 0.21.0. Attributes k_feature_idx_ : array-like, shape = [n_predictions] Feature Indices of the selected feature subsets. k_feature_names_ : array-like, shape = [n_predictions] Feature names of the selected feature subsets. If pandas DataFrames are used in the fit method, the feature names correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. New in v 0.13.0. k_score_ : float Cross validation average score of the selected subset. subsets_ : dict A dictionary of selected feature subsets during the sequential selection, where the dictionary keys are the lengths k of these feature subsets. If the parameter feature_groups is not None, the value of key indicates the number of groups that are selected together. The dictionary values are dictionaries themselves with the following keys: 'feature_idx' (tuple of indices of the feature subset) 'feature_names' (tuple of feature names of the feat. subset) 'cv_scores' (list individual cross-validation scores) 'avg_score' (average cross-validation score) Note that if pandas DataFrames are used in the fit method, the 'feature_names' correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. The 'feature_names' is new in v 0.13.0. Notes (1) If parameter feature_groups is not None, the number of features is equal to the number of feature groups, i.e. len(feature_groups) . For example, if feature_groups = [[0], [1], [2, 3], [4]] , then the max_features value cannot exceed 4. (2) Although two or more individual features may be considered as one group throughout the feature-selection process, it does not mean the individual features of that group have the same impact on the outcome. For instance, in linear regression, the coefficient of the feature 2 and 3 can be different even if they are considered as one group in feature_groups. (3) If both fixed_features and feature_groups are specified, ensure that each feature group contains the fixed_features selection. E.g., for a 3-feature set fixed_features=[0, 1] and feature_groups=[[0, 1], [2]] is valid; fixed_features=[0, 1] and feature_groups=[[0], [1, 2]] is not valid. (4) In case of KeyboardInterrupt, the dictionary subsets may not be completed. If user is still interested in getting the best score, they can use method `finalize_fit`. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/ Methods finalize_fit() None fit(X, y, groups=None, fit_params) Perform feature selection and learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. New in v 0.13.0: pandas DataFrames are now also accepted as argument for y. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : various, optional Additional parameters that are being passed to the estimator. For example, sample_weights=weights . Returns self : object fit_transform(X, y, groups=None, fit_params) Fit to training data then reduce X to its most important features. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. New in v 0.13.0: a pandas Series are now also accepted as argument for y. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : various, optional Additional parameters that are being passed to the estimator. For example, sample_weights=weights . Returns Reduced feature subset of X, shape={n_samples, k_features} generate_error_message_k_features(name) None get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_metric_dict(confidence_interval=0.95) Return metric dictionary Parameters confidence_interval : float (default: 0.95) A positive float between 0.0 and 1.0 to compute the confidence interval bounds of the CV score averages. Returns Dictionary with items where each dictionary value is a list with the number of iterations (number of feature subsets) as its length. The dictionary keys corresponding to these lists are as follows: 'feature_idx': tuple of the indices of the feature subset 'cv_scores': list with individual CV scores 'avg_score': of CV average scores 'std_dev': standard deviation of the CV score average 'std_err': standard error of the CV score average 'ci_bound': confidence interval bound of the CV score average get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_fit_request(self: mlxtend.feature_selection.sequential_feature_selector.SequentialFeatureSelector, , groups: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.feature_selection.sequential_feature_selector.SequentialFeatureSelector* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . Returns self : object The updated object. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with get_params() . Returns self transform(X) Reduce X to its most important features. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. Returns Reduced feature subset of X, shape={n_samples, k_features} Properties named_estimators Returns List of named estimator tuples, like [('svc', SVC(...))]","title":"SequentialFeatureSelector"},{"location":"api_modules/mlxtend.feature_selection/SequentialFeatureSelector/#sequentialfeatureselector","text":"SequentialFeatureSelector(estimator, k_features=1, forward=True, floating=False, verbose=0, scoring=None, cv=5, n_jobs=1, pre_dispatch='2 n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)* Sequential Feature Selection for Classification and Regression. Parameters estimator : scikit-learn classifier or regressor k_features : int or tuple or str (default: 1) Number of features to select, where k_features < the full feature set. New in 0.4.2: A tuple containing a min and max value can be provided, and the SFS will consider return any feature combination between min and max that scored highest in cross-validation. For example, the tuple (1, 4) will return any combination from 1 up to 4 features instead of a fixed number of features k. New in 0.8.0: A string argument \"best\" or \"parsimonious\". If \"best\" is provided, the feature selector will return the feature subset with the best cross-validation performance. If \"parsimonious\" is provided as an argument, the smallest feature subset that is within one standard error of the cross-validation performance will be selected. forward : bool (default: True) Forward selection if True, backward selection otherwise floating : bool (default: False) Adds a conditional exclusion/inclusion if True. verbose : int (default: 0), level of verbosity to use in logging. If 0, no output, if 1 number of features in current set, if 2 detailed logging i ncluding timestamp and cv scores at step. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. cv : int (default: 5) Integer or iterable yielding train, test splits. If cv is an integer and estimator is a classifier (or y consists of integer class labels) stratified k-fold. Otherwise regular k-fold cross-validation is performed. No cross-validation if cv is None, False, or 0. n_jobs : int (default: 1) The number of CPUs to use for evaluating different feature subsets in parallel. -1 means 'all CPUs'. pre_dispatch : int, or string (default: '2*n_jobs') Controls the number of jobs that get dispatched during parallel execution if n_jobs > 1 or n_jobs=-1 . Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs clone_estimator : bool (default: True) Clones estimator if True; works with the original estimator instance if False. Set to False if the estimator doesn't implement scikit-learn's set_params and get_params methods. In addition, it is required to set cv=0, and n_jobs=1. fixed_features : tuple (default: None) If not None , the feature indices provided as a tuple will be regarded as fixed by the feature selector. For example, if fixed_features=(1, 3, 7) , the 2nd, 4th, and 8th feature are guaranteed to be present in the solution. Note that if fixed_features is not None , make sure that the number of features to be selected is greater than len(fixed_features) . In other words, ensure that k_features > len(fixed_features) . New in mlxtend v. 0.18.0. feature_groups : list or None (default: None) Optional argument for treating certain features as a group. This means, the features within a group are always selected together, never split. For example, feature_groups=[[1], [2], [3, 4, 5]] specifies 3 feature groups. In this case, possible feature selection results with k_features=2 are [[1], [2] , [[1], [3, 4, 5]] , or [[2], [3, 4, 5]] . Feature groups can be useful for interpretability, for example, if features 3, 4, 5 are one-hot encoded features. (For more details, please read the notes at the bottom of this docstring). New in mlxtend v. 0.21.0. Attributes k_feature_idx_ : array-like, shape = [n_predictions] Feature Indices of the selected feature subsets. k_feature_names_ : array-like, shape = [n_predictions] Feature names of the selected feature subsets. If pandas DataFrames are used in the fit method, the feature names correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. New in v 0.13.0. k_score_ : float Cross validation average score of the selected subset. subsets_ : dict A dictionary of selected feature subsets during the sequential selection, where the dictionary keys are the lengths k of these feature subsets. If the parameter feature_groups is not None, the value of key indicates the number of groups that are selected together. The dictionary values are dictionaries themselves with the following keys: 'feature_idx' (tuple of indices of the feature subset) 'feature_names' (tuple of feature names of the feat. subset) 'cv_scores' (list individual cross-validation scores) 'avg_score' (average cross-validation score) Note that if pandas DataFrames are used in the fit method, the 'feature_names' correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. The 'feature_names' is new in v 0.13.0. Notes (1) If parameter feature_groups is not None, the number of features is equal to the number of feature groups, i.e. len(feature_groups) . For example, if feature_groups = [[0], [1], [2, 3], [4]] , then the max_features value cannot exceed 4. (2) Although two or more individual features may be considered as one group throughout the feature-selection process, it does not mean the individual features of that group have the same impact on the outcome. For instance, in linear regression, the coefficient of the feature 2 and 3 can be different even if they are considered as one group in feature_groups. (3) If both fixed_features and feature_groups are specified, ensure that each feature group contains the fixed_features selection. E.g., for a 3-feature set fixed_features=[0, 1] and feature_groups=[[0, 1], [2]] is valid; fixed_features=[0, 1] and feature_groups=[[0], [1, 2]] is not valid. (4) In case of KeyboardInterrupt, the dictionary subsets may not be completed. If user is still interested in getting the best score, they can use method `finalize_fit`. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/","title":"SequentialFeatureSelector"},{"location":"api_modules/mlxtend.feature_selection/SequentialFeatureSelector/#methods","text":"finalize_fit() None fit(X, y, groups=None, fit_params) Perform feature selection and learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. New in v 0.13.0: pandas DataFrames are now also accepted as argument for y. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : various, optional Additional parameters that are being passed to the estimator. For example, sample_weights=weights . Returns self : object fit_transform(X, y, groups=None, fit_params) Fit to training data then reduce X to its most important features. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. New in v 0.13.0: a pandas Series are now also accepted as argument for y. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : various, optional Additional parameters that are being passed to the estimator. For example, sample_weights=weights . Returns Reduced feature subset of X, shape={n_samples, k_features} generate_error_message_k_features(name) None get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_metric_dict(confidence_interval=0.95) Return metric dictionary Parameters confidence_interval : float (default: 0.95) A positive float between 0.0 and 1.0 to compute the confidence interval bounds of the CV score averages. Returns Dictionary with items where each dictionary value is a list with the number of iterations (number of feature subsets) as its length. The dictionary keys corresponding to these lists are as follows: 'feature_idx': tuple of the indices of the feature subset 'cv_scores': list with individual CV scores 'avg_score': of CV average scores 'std_dev': standard deviation of the CV score average 'std_err': standard error of the CV score average 'ci_bound': confidence interval bound of the CV score average get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_fit_request(self: mlxtend.feature_selection.sequential_feature_selector.SequentialFeatureSelector, , groups: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.feature_selection.sequential_feature_selector.SequentialFeatureSelector* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . Returns self : object The updated object. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with get_params() . Returns self transform(X) Reduce X to its most important features. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. Returns Reduced feature subset of X, shape={n_samples, k_features}","title":"Methods"},{"location":"api_modules/mlxtend.feature_selection/SequentialFeatureSelector/#properties","text":"named_estimators Returns List of named estimator tuples, like [('svc', SVC(...))]","title":"Properties"},{"location":"api_modules/mlxtend.file_io/find_filegroups/","text":"find_filegroups find_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True, rstrip='', ignore_substring=None) Find and collect files from different directories in a python dictionary. Parameters paths : list Paths of the directories to be searched. Dictionary keys are build from the first directory. substring : str (default: '') Substring that all files have to contain to be considered. extensions : list (default: None) None or list of allowed file extensions for each path. If provided, the number of extensions must match the number of paths . validity_check : bool (default: None) If True , checks if all dictionary values have the same number of file paths. Prints a warning and returns an empty dictionary if the validity check failed. ignore_invisible : bool (default: True) If True , ignores invisible files (i.e., files starting with a period). rstrip : str (default: '') If provided, strips characters from right side of the file base names after splitting the extension. Useful to trim different filenames to a common stem. E.g,. \"abc_d.txt\" and \"abc_d_.csv\" would share the stem \"abc_d\" if rstrip is set to \"_\". ignore_substring : str (default: None) Ignores files that contain the specified substring. Returns groups : dict Dictionary of files paths. Keys are the file names found in the first directory listed in paths (without file extension). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/file_io/find_filegroups/","title":"Find filegroups"},{"location":"api_modules/mlxtend.file_io/find_filegroups/#find_filegroups","text":"find_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True, rstrip='', ignore_substring=None) Find and collect files from different directories in a python dictionary. Parameters paths : list Paths of the directories to be searched. Dictionary keys are build from the first directory. substring : str (default: '') Substring that all files have to contain to be considered. extensions : list (default: None) None or list of allowed file extensions for each path. If provided, the number of extensions must match the number of paths . validity_check : bool (default: None) If True , checks if all dictionary values have the same number of file paths. Prints a warning and returns an empty dictionary if the validity check failed. ignore_invisible : bool (default: True) If True , ignores invisible files (i.e., files starting with a period). rstrip : str (default: '') If provided, strips characters from right side of the file base names after splitting the extension. Useful to trim different filenames to a common stem. E.g,. \"abc_d.txt\" and \"abc_d_.csv\" would share the stem \"abc_d\" if rstrip is set to \"_\". ignore_substring : str (default: None) Ignores files that contain the specified substring. Returns groups : dict Dictionary of files paths. Keys are the file names found in the first directory listed in paths (without file extension). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/file_io/find_filegroups/","title":"find_filegroups"},{"location":"api_modules/mlxtend.file_io/find_files/","text":"find_files find_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True, ignore_substring=None) Find files in a directory based on substring matching. Parameters substring : str Substring of the file to be matched. path : str Path where to look. recursive : bool If true, searches subdirectories recursively. check_ext : str If string (e.g., '.txt'), only returns files that match the specified file extension. ignore_invisible : bool If True , ignores invisible files (i.e., files starting with a period). ignore_substring : str Ignores files that contain the specified substring. Returns results : list List of the matched files. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/file_io/find_files/","title":"Find files"},{"location":"api_modules/mlxtend.file_io/find_files/#find_files","text":"find_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True, ignore_substring=None) Find files in a directory based on substring matching. Parameters substring : str Substring of the file to be matched. path : str Path where to look. recursive : bool If true, searches subdirectories recursively. check_ext : str If string (e.g., '.txt'), only returns files that match the specified file extension. ignore_invisible : bool If True , ignores invisible files (i.e., files starting with a period). ignore_substring : str Ignores files that contain the specified substring. Returns results : list List of the matched files. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/file_io/find_files/","title":"find_files"},{"location":"api_modules/mlxtend.frequent_patterns/apriori/","text":"apriori apriori(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0, low_memory=False) Get frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see (https://pandas.pydata.org/pandas-docs/stable/ user_guide/sparse.html#sparse-data-structures) Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minumum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions . use_colnames : bool (default: False) If True , uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Maximum length of the itemsets generated. If None (default) all possible itemsets lengths (under the apriori condition) are evaluated. verbose : int (default: 0) Shows the number of iterations if >= 1 and low_memory is True . If =1 and low_memory is False , shows the number of combinations. low_memory : bool (default: False) If True , uses an iterator to search for combinations above min_support . Note that while low_memory=True should only be used for large dataset if memory resources are limited, because this implementation is approx. 3-6x slower than the default. Returns pandas DataFrame with columns ['support', 'itemsets'] of all itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/","title":"Apriori"},{"location":"api_modules/mlxtend.frequent_patterns/apriori/#apriori","text":"apriori(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0, low_memory=False) Get frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see (https://pandas.pydata.org/pandas-docs/stable/ user_guide/sparse.html#sparse-data-structures) Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minumum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions . use_colnames : bool (default: False) If True , uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Maximum length of the itemsets generated. If None (default) all possible itemsets lengths (under the apriori condition) are evaluated. verbose : int (default: 0) Shows the number of iterations if >= 1 and low_memory is True . If =1 and low_memory is False , shows the number of combinations. low_memory : bool (default: False) If True , uses an iterator to search for combinations above min_support . Note that while low_memory=True should only be used for large dataset if memory resources are limited, because this implementation is approx. 3-6x slower than the default. Returns pandas DataFrame with columns ['support', 'itemsets'] of all itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/","title":"apriori"},{"location":"api_modules/mlxtend.frequent_patterns/association_rules/","text":"association_rules association_rules(df, metric='confidence', min_threshold=0.8, support_only=False) Generates a DataFrame of association rules including the metrics 'score', 'confidence', and 'lift' Parameters df : pandas DataFrame pandas DataFrame of frequent itemsets with columns ['support', 'itemsets'] metric : string (default: 'confidence') Metric to evaluate if a rule is of interest. Automatically set to 'support' if support_only=True . Otherwise, supported metrics are 'support', 'confidence', 'lift', 'leverage', 'conviction' and 'zhangs_metric' These metrics are computed as follows: - support(A->C) = support(A+C) [aka 'support'], range: [0, 1] - confidence(A->C) = support(A+C) / support(A), range: [0, 1] - lift(A->C) = confidence(A->C) / support(C), range: [0, inf] - leverage(A->C) = support(A->C) - support(A)*support(C), range: [-1, 1] - conviction = [1 - support(C)] / [1 - confidence(A->C)], range: [0, inf] - zhangs_metric(A->C) = leverage(A->C) / max(support(A->C)*(1-support(A)), support(A)*(support(C)-support(A->C))) range: [-1,1] min_threshold : float (default: 0.8) Minimal threshold for the evaluation metric, via the metric parameter, to decide whether a candidate rule is of interest. support_only : bool (default: False) Only computes the rule support and fills the other metric columns with NaNs. This is useful if: a) the input DataFrame is incomplete, e.g., does not contain support values for all rule antecedents and consequents b) you simply want to speed up the computation because you don't need the other metrics. Returns pandas DataFrame with columns \"antecedents\" and \"consequents\" that store itemsets, plus the scoring metric columns: \"antecedent support\", \"consequent support\", \"support\", \"confidence\", \"lift\", \"leverage\", \"conviction\" of all rules for which metric(rule) >= min_threshold. Each entry in the \"antecedents\" and \"consequents\" columns are of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/","title":"Association rules"},{"location":"api_modules/mlxtend.frequent_patterns/association_rules/#association_rules","text":"association_rules(df, metric='confidence', min_threshold=0.8, support_only=False) Generates a DataFrame of association rules including the metrics 'score', 'confidence', and 'lift' Parameters df : pandas DataFrame pandas DataFrame of frequent itemsets with columns ['support', 'itemsets'] metric : string (default: 'confidence') Metric to evaluate if a rule is of interest. Automatically set to 'support' if support_only=True . Otherwise, supported metrics are 'support', 'confidence', 'lift', 'leverage', 'conviction' and 'zhangs_metric' These metrics are computed as follows: - support(A->C) = support(A+C) [aka 'support'], range: [0, 1] - confidence(A->C) = support(A+C) / support(A), range: [0, 1] - lift(A->C) = confidence(A->C) / support(C), range: [0, inf] - leverage(A->C) = support(A->C) - support(A)*support(C), range: [-1, 1] - conviction = [1 - support(C)] / [1 - confidence(A->C)], range: [0, inf] - zhangs_metric(A->C) = leverage(A->C) / max(support(A->C)*(1-support(A)), support(A)*(support(C)-support(A->C))) range: [-1,1] min_threshold : float (default: 0.8) Minimal threshold for the evaluation metric, via the metric parameter, to decide whether a candidate rule is of interest. support_only : bool (default: False) Only computes the rule support and fills the other metric columns with NaNs. This is useful if: a) the input DataFrame is incomplete, e.g., does not contain support values for all rule antecedents and consequents b) you simply want to speed up the computation because you don't need the other metrics. Returns pandas DataFrame with columns \"antecedents\" and \"consequents\" that store itemsets, plus the scoring metric columns: \"antecedent support\", \"consequent support\", \"support\", \"confidence\", \"lift\", \"leverage\", \"conviction\" of all rules for which metric(rule) >= min_threshold. Each entry in the \"antecedents\" and \"consequents\" columns are of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/","title":"association_rules"},{"location":"api_modules/mlxtend.frequent_patterns/fpgrowth/","text":"fpgrowth fpgrowth(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0) Get frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#sparse-data-structures. Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minimum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions. use_colnames : bool (default: False) If true, uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Maximum length of the itemsets generated. If None (default) all possible itemsets lengths are evaluated. verbose : int (default: 0) Shows the stages of conditional tree generation. Returns pandas DataFrame with columns ['support', 'itemsets'] of all itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpgrowth/","title":"Fpgrowth"},{"location":"api_modules/mlxtend.frequent_patterns/fpgrowth/#fpgrowth","text":"fpgrowth(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0) Get frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#sparse-data-structures. Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minimum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions. use_colnames : bool (default: False) If true, uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Maximum length of the itemsets generated. If None (default) all possible itemsets lengths are evaluated. verbose : int (default: 0) Shows the stages of conditional tree generation. Returns pandas DataFrame with columns ['support', 'itemsets'] of all itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpgrowth/","title":"fpgrowth"},{"location":"api_modules/mlxtend.frequent_patterns/fpmax/","text":"fpmax fpmax(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0) Get maximal frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see (https://pandas.pydata.org/pandas-docs/stable/ user_guide/sparse.html#sparse-data-structures) Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minimum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions. use_colnames : bool (default: False) If true, uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Given the set of all maximal itemsets, return those that are less than max_len . If None (default) all possible itemsets lengths are evaluated. verbose : int (default: 0) Shows the stages of conditional tree generation. Returns pandas DataFrame with columns ['support', 'itemsets'] of all maximal itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpmax/","title":"Fpmax"},{"location":"api_modules/mlxtend.frequent_patterns/fpmax/#fpmax","text":"fpmax(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0) Get maximal frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see (https://pandas.pydata.org/pandas-docs/stable/ user_guide/sparse.html#sparse-data-structures) Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minimum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions. use_colnames : bool (default: False) If true, uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Given the set of all maximal itemsets, return those that are less than max_len . If None (default) all possible itemsets lengths are evaluated. verbose : int (default: 0) Shows the stages of conditional tree generation. Returns pandas DataFrame with columns ['support', 'itemsets'] of all maximal itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpmax/","title":"fpmax"},{"location":"api_modules/mlxtend.frequent_patterns/hmine/","text":"hmine hmine(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0) -> pandas.core.frame.DataFrame Get frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#sparse-data-structures. Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minimum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions. use_colnames : bool (default: False) If true, uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Maximum length of the itemsets generated. If None (default) all possible itemsets lengths are evaluated. verbose : int (default: 0) Shows the stages of conditional tree generation. Returns pandas DataFrame with columns ['support', 'itemsets'] of all itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/hmine/","title":"Hmine"},{"location":"api_modules/mlxtend.frequent_patterns/hmine/#hmine","text":"hmine(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0) -> pandas.core.frame.DataFrame Get frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#sparse-data-structures. Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minimum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions. use_colnames : bool (default: False) If true, uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Maximum length of the itemsets generated. If None (default) all possible itemsets lengths are evaluated. verbose : int (default: 0) Shows the stages of conditional tree generation. Returns pandas DataFrame with columns ['support', 'itemsets'] of all itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/hmine/","title":"hmine"},{"location":"api_modules/mlxtend.math/factorial/","text":"factorial factorial(n) None","title":"Factorial"},{"location":"api_modules/mlxtend.math/factorial/#factorial","text":"factorial(n) None","title":"factorial"},{"location":"api_modules/mlxtend.math/num_combinations/","text":"num_combinations num_combinations(n, k, with_replacement=False) Function to calculate the number of possible combinations. Parameters n : int Total number of items. k : int Number of elements of the target itemset. with_replacement : bool (default: False) Allows repeated elements if True. Returns comb : int Number of possible combinations. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/num_combinations/","title":"Num combinations"},{"location":"api_modules/mlxtend.math/num_combinations/#num_combinations","text":"num_combinations(n, k, with_replacement=False) Function to calculate the number of possible combinations. Parameters n : int Total number of items. k : int Number of elements of the target itemset. with_replacement : bool (default: False) Allows repeated elements if True. Returns comb : int Number of possible combinations. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/num_combinations/","title":"num_combinations"},{"location":"api_modules/mlxtend.math/num_permutations/","text":"num_permutations num_permutations(n, k, with_replacement=False) Function to calculate the number of possible permutations. Parameters n : int Total number of items. k : int Number of elements of the target itemset. with_replacement : bool Allows repeated elements if True. Returns permut : int Number of possible permutations. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/num_permutations/","title":"Num permutations"},{"location":"api_modules/mlxtend.math/num_permutations/#num_permutations","text":"num_permutations(n, k, with_replacement=False) Function to calculate the number of possible permutations. Parameters n : int Total number of items. k : int Number of elements of the target itemset. with_replacement : bool Allows repeated elements if True. Returns permut : int Number of possible permutations. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/num_permutations/","title":"num_permutations"},{"location":"api_modules/mlxtend.math/vectorspace_dimensionality/","text":"vectorspace_dimensionality vectorspace_dimensionality(ary) Computes the hyper-volume spanned by a vector set Parameters ary : array-like, shape=[num_vectors, num_vectors] An orthogonal set of vectors (arranged as columns in a matrix) Returns dimensions : int An integer indicating the \"dimensionality\" hyper-volume spanned by the vector set Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/vectorspace_dimensionality/","title":"Vectorspace dimensionality"},{"location":"api_modules/mlxtend.math/vectorspace_dimensionality/#vectorspace_dimensionality","text":"vectorspace_dimensionality(ary) Computes the hyper-volume spanned by a vector set Parameters ary : array-like, shape=[num_vectors, num_vectors] An orthogonal set of vectors (arranged as columns in a matrix) Returns dimensions : int An integer indicating the \"dimensionality\" hyper-volume spanned by the vector set Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/vectorspace_dimensionality/","title":"vectorspace_dimensionality"},{"location":"api_modules/mlxtend.math/vectorspace_orthonormalization/","text":"vectorspace_orthonormalization vectorspace_orthonormalization(ary, eps=1e-13) Transforms a set of column vectors to a orthonormal basis. Given a set of orthogonal vectors, this functions converts such column vectors, arranged in a matrix, into orthonormal basis vectors. Parameters ary : array-like, shape=[num_vectors, num_vectors] An orthogonal set of vectors (arranged as columns in a matrix) eps : float (default: 1e-13) A small tolerance value to determine whether the vector norm is zero or not. Returns arr : array-like, shape=[num_vectors, num_vectors] An orthonormal set of vectors (arranged as columns) Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/vectorspace_orthonormalization/","title":"Vectorspace orthonormalization"},{"location":"api_modules/mlxtend.math/vectorspace_orthonormalization/#vectorspace_orthonormalization","text":"vectorspace_orthonormalization(ary, eps=1e-13) Transforms a set of column vectors to a orthonormal basis. Given a set of orthogonal vectors, this functions converts such column vectors, arranged in a matrix, into orthonormal basis vectors. Parameters ary : array-like, shape=[num_vectors, num_vectors] An orthogonal set of vectors (arranged as columns in a matrix) eps : float (default: 1e-13) A small tolerance value to determine whether the vector norm is zero or not. Returns arr : array-like, shape=[num_vectors, num_vectors] An orthonormal set of vectors (arranged as columns) Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/vectorspace_orthonormalization/","title":"vectorspace_orthonormalization"},{"location":"api_modules/mlxtend.plotting/category_scatter/","text":"category_scatter category_scatter(x, y, label_col, data, markers='sxo^v', colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'), alpha=0.7, markersize=20.0, legend_loc='best') Scatter plot to plot categories in different colors/markerstyles. Parameters x : str or int DataFrame column name of the x-axis values or integer for the numpy ndarray column index. y : str DataFrame column name of the y-axis values or integer for the numpy ndarray column index data : Pandas DataFrame object or NumPy ndarray. markers : str Markers that are cycled through the label category. colors : tuple Colors that are cycled through the label category. alpha : float (default: 0.7) Parameter to control the transparency. markersize : float (default` : 20.0) Parameter to control the marker size. legend_loc : str (default: 'best') Location of the plot legend {best, upper left, upper right, lower left, lower right} No legend if legend_loc=False Returns fig : matplotlig.pyplot figure object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/category_scatter/","title":"Category scatter"},{"location":"api_modules/mlxtend.plotting/category_scatter/#category_scatter","text":"category_scatter(x, y, label_col, data, markers='sxo^v', colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'), alpha=0.7, markersize=20.0, legend_loc='best') Scatter plot to plot categories in different colors/markerstyles. Parameters x : str or int DataFrame column name of the x-axis values or integer for the numpy ndarray column index. y : str DataFrame column name of the y-axis values or integer for the numpy ndarray column index data : Pandas DataFrame object or NumPy ndarray. markers : str Markers that are cycled through the label category. colors : tuple Colors that are cycled through the label category. alpha : float (default: 0.7) Parameter to control the transparency. markersize : float (default` : 20.0) Parameter to control the marker size. legend_loc : str (default: 'best') Location of the plot legend {best, upper left, upper right, lower left, lower right} No legend if legend_loc=False Returns fig : matplotlig.pyplot figure object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/category_scatter/","title":"category_scatter"},{"location":"api_modules/mlxtend.plotting/checkerboard_plot/","text":"checkerboard_plot checkerboard_plot(ary, cell_colors=('white', 'black'), font_colors=('black', 'white'), fmt='%.1f', figsize=None, row_labels=None, col_labels=None, fontsize=None) Plot a checkerboard table / heatmap via matplotlib. Parameters ary : array-like, shape = [n, m] A 2D Nnumpy array. cell_colors : tuple or list (default: ('white', 'black')) Tuple or list containing the two colors of the checkerboard pattern. font_colors : tuple or list (default: ('black', 'white')) Font colors corresponding to the cell colors. figsize : tuple (default: (2.5, 2.5)) Height and width of the figure fmt : str (default: '%.1f') Python string formatter for cell values. The default '%.1f' results in floats with 1 digit after the decimal point. Use '%d' to show numbers as integers. row_labels : list (default: None) List of the row labels. Uses the array row indices 0 to n by default. col_labels : list (default: None) List of the column labels. Uses the array column indices 0 to m by default. fontsize : int (default: None) Specifies the font size of the checkerboard table. Uses matplotlib's default if None. Returns fig : matplotlib Figure object. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/checkerboard_plot/","title":"Checkerboard plot"},{"location":"api_modules/mlxtend.plotting/checkerboard_plot/#checkerboard_plot","text":"checkerboard_plot(ary, cell_colors=('white', 'black'), font_colors=('black', 'white'), fmt='%.1f', figsize=None, row_labels=None, col_labels=None, fontsize=None) Plot a checkerboard table / heatmap via matplotlib. Parameters ary : array-like, shape = [n, m] A 2D Nnumpy array. cell_colors : tuple or list (default: ('white', 'black')) Tuple or list containing the two colors of the checkerboard pattern. font_colors : tuple or list (default: ('black', 'white')) Font colors corresponding to the cell colors. figsize : tuple (default: (2.5, 2.5)) Height and width of the figure fmt : str (default: '%.1f') Python string formatter for cell values. The default '%.1f' results in floats with 1 digit after the decimal point. Use '%d' to show numbers as integers. row_labels : list (default: None) List of the row labels. Uses the array row indices 0 to n by default. col_labels : list (default: None) List of the column labels. Uses the array column indices 0 to m by default. fontsize : int (default: None) Specifies the font size of the checkerboard table. Uses matplotlib's default if None. Returns fig : matplotlib Figure object. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/checkerboard_plot/","title":"checkerboard_plot"},{"location":"api_modules/mlxtend.plotting/ecdf/","text":"ecdf ecdf(x, y_label='ECDF', x_label=None, ax=None, percentile=None, ecdf_color=None, ecdf_marker='o', percentile_color='black', percentile_linestyle='--') Plots an Empirical Cumulative Distribution Function Parameters x : array or list, shape=[n_samples,] Array-like object containing the feature values y_label : str (default='ECDF') Text label for the y-axis x_label : str (default=None) Text label for the x-axis ax : matplotlib.axes.Axes (default: None) An existing matplotlib Axes. Creates one if ax=None percentile : float (default=None) Float between 0 and 1 for plotting a percentile threshold line ecdf_color : matplotlib color (default=None) Color for the ECDF plot; uses matplotlib defaults if None ecdf_marker : matplotlib marker (default='o') Marker style for the ECDF plot percentile_color : matplotlib color (default='black') Color for the percentile threshold if percentile is not None percentile_linestyle : matplotlib linestyle (default='--') Line style for the percentile threshold if percentile is not None Returns ax : matplotlib.axes.Axes object percentile_threshold : float Feature threshold at the percentile or None if percentile=None percentile_count : Number of if percentile is not None Number of samples that have a feature less or equal than the feature threshold at a percentile threshold or None if percentile=None Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/ecdf/","title":"Ecdf"},{"location":"api_modules/mlxtend.plotting/ecdf/#ecdf","text":"ecdf(x, y_label='ECDF', x_label=None, ax=None, percentile=None, ecdf_color=None, ecdf_marker='o', percentile_color='black', percentile_linestyle='--') Plots an Empirical Cumulative Distribution Function Parameters x : array or list, shape=[n_samples,] Array-like object containing the feature values y_label : str (default='ECDF') Text label for the y-axis x_label : str (default=None) Text label for the x-axis ax : matplotlib.axes.Axes (default: None) An existing matplotlib Axes. Creates one if ax=None percentile : float (default=None) Float between 0 and 1 for plotting a percentile threshold line ecdf_color : matplotlib color (default=None) Color for the ECDF plot; uses matplotlib defaults if None ecdf_marker : matplotlib marker (default='o') Marker style for the ECDF plot percentile_color : matplotlib color (default='black') Color for the percentile threshold if percentile is not None percentile_linestyle : matplotlib linestyle (default='--') Line style for the percentile threshold if percentile is not None Returns ax : matplotlib.axes.Axes object percentile_threshold : float Feature threshold at the percentile or None if percentile=None percentile_count : Number of if percentile is not None Number of samples that have a feature less or equal than the feature threshold at a percentile threshold or None if percentile=None Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/ecdf/","title":"ecdf"},{"location":"api_modules/mlxtend.plotting/enrichment_plot/","text":"enrichment_plot enrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2, where='post', grid=True, count_label='Count', xlim='auto', ylim='auto', invert_axes=False, legend_loc='best', ax=None) Plot stacked barplots Parameters df : pandas.DataFrame A pandas DataFrame where columns represent the different categories. colors: str (default: 'bgrcky') The colors of the bars. markers : str (default: ' ') Matplotlib markerstyles, e.g, 'sov' for square,circle, and triangle markers. linestyles : str (default: '-') Matplotlib linestyles, e.g., '-,--' to cycle normal and dashed lines. Note that the different linestyles need to be separated by commas. alpha : float (default: 0.5) Transparency level from 0.0 to 1.0. lw : int or float (default: 2) Linewidth parameter. where : {'post', 'pre', 'mid'} (default: 'post') Starting location of the steps. grid : bool (default: True ) Plots a grid if True. count_label : str (default: 'Count') Label for the \"Count\"-axis. xlim : 'auto' or array-like [min, max] (default: 'auto') Min and maximum position of the x-axis range. ylim : 'auto' or array-like [min, max] (default: 'auto') Min and maximum position of the y-axis range. invert_axes : bool (default: False) Plots count on the x-axis if True. legend_loc : str (default: 'best') Location of the plot legend {best, upper left, upper right, lower left, lower right} No legend if legend_loc=False ax : matplotlib axis, optional (default: None) Use this axis for plotting or make a new one otherwise Returns ax : matplotlib axis Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/enrichment_plot/","title":"Enrichment plot"},{"location":"api_modules/mlxtend.plotting/enrichment_plot/#enrichment_plot","text":"enrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2, where='post', grid=True, count_label='Count', xlim='auto', ylim='auto', invert_axes=False, legend_loc='best', ax=None) Plot stacked barplots Parameters df : pandas.DataFrame A pandas DataFrame where columns represent the different categories. colors: str (default: 'bgrcky') The colors of the bars. markers : str (default: ' ') Matplotlib markerstyles, e.g, 'sov' for square,circle, and triangle markers. linestyles : str (default: '-') Matplotlib linestyles, e.g., '-,--' to cycle normal and dashed lines. Note that the different linestyles need to be separated by commas. alpha : float (default: 0.5) Transparency level from 0.0 to 1.0. lw : int or float (default: 2) Linewidth parameter. where : {'post', 'pre', 'mid'} (default: 'post') Starting location of the steps. grid : bool (default: True ) Plots a grid if True. count_label : str (default: 'Count') Label for the \"Count\"-axis. xlim : 'auto' or array-like [min, max] (default: 'auto') Min and maximum position of the x-axis range. ylim : 'auto' or array-like [min, max] (default: 'auto') Min and maximum position of the y-axis range. invert_axes : bool (default: False) Plots count on the x-axis if True. legend_loc : str (default: 'best') Location of the plot legend {best, upper left, upper right, lower left, lower right} No legend if legend_loc=False ax : matplotlib axis, optional (default: None) Use this axis for plotting or make a new one otherwise Returns ax : matplotlib axis Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/enrichment_plot/","title":"enrichment_plot"},{"location":"api_modules/mlxtend.plotting/heatmap/","text":"heatmap heatmap(matrix, hide_spines=False, hide_ticks=False, figsize=None, cmap=None, colorbar=True, row_names=None, column_names=None, column_name_rotation=45, cell_values=True, cell_fmt='.2f', cell_font_size=None, text_color_threshold=None) Plot a heatmap via matplotlib. Parameters conf_mat : array-like, shape = [n_rows, n_columns] And arbitrary 2D array. hide_spines : bool (default: False) Hides axis spines if True. hide_ticks : bool (default: False) Hides axis ticks if True figsize : tuple (default: (2.5, 2.5)) Height and width of the figure cmap : matplotlib colormap (default: None ) Uses matplotlib.pyplot.cm.viridis if None colorbar : bool (default: True) Shows a colorbar if True row_names : array-like, shape = [n_rows] (default: None) List of row names to be used as y-axis tick labels. column_names : array-like, shape = [n_columns] (default: None) List of column names to be used as x-axis tick labels. column_name_rotation : int (default: 45) Number of degrees for rotating column x-tick labels. cell_values : bool (default: True) Plots cell values if True. cell_fmt : string (default: '.2f') Format specification for cell values (if cell_values=True ) cell_font_size : int (default: None) Font size for cell values (if cell_values=True ) text_color_threshold : float (default: None) Threshold for the black/white text threshold of the text annotation. Default (None) tried to infer a good threshold automatically using np.max(normed_matrix) / 2 . Returns fig, ax : matplotlib.pyplot subplot objects Figure and axis elements of the subplot. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/heatmap/","title":"Heatmap"},{"location":"api_modules/mlxtend.plotting/heatmap/#heatmap","text":"heatmap(matrix, hide_spines=False, hide_ticks=False, figsize=None, cmap=None, colorbar=True, row_names=None, column_names=None, column_name_rotation=45, cell_values=True, cell_fmt='.2f', cell_font_size=None, text_color_threshold=None) Plot a heatmap via matplotlib. Parameters conf_mat : array-like, shape = [n_rows, n_columns] And arbitrary 2D array. hide_spines : bool (default: False) Hides axis spines if True. hide_ticks : bool (default: False) Hides axis ticks if True figsize : tuple (default: (2.5, 2.5)) Height and width of the figure cmap : matplotlib colormap (default: None ) Uses matplotlib.pyplot.cm.viridis if None colorbar : bool (default: True) Shows a colorbar if True row_names : array-like, shape = [n_rows] (default: None) List of row names to be used as y-axis tick labels. column_names : array-like, shape = [n_columns] (default: None) List of column names to be used as x-axis tick labels. column_name_rotation : int (default: 45) Number of degrees for rotating column x-tick labels. cell_values : bool (default: True) Plots cell values if True. cell_fmt : string (default: '.2f') Format specification for cell values (if cell_values=True ) cell_font_size : int (default: None) Font size for cell values (if cell_values=True ) text_color_threshold : float (default: None) Threshold for the black/white text threshold of the text annotation. Default (None) tried to infer a good threshold automatically using np.max(normed_matrix) / 2 . Returns fig, ax : matplotlib.pyplot subplot objects Figure and axis elements of the subplot. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/heatmap/","title":"heatmap"},{"location":"api_modules/mlxtend.plotting/plot_confusion_matrix/","text":"plot_confusion_matrix plot_confusion_matrix(conf_mat, hide_spines=False, hide_ticks=False, figsize=None, cmap=None, colorbar=False, show_absolute=True, show_normed=False, norm_colormap=None, class_names=None, figure=None, axis=None, fontcolor_threshold=0.5) Plot a confusion matrix via matplotlib. Parameters conf_mat : array-like, shape = [n_classes, n_classes] Confusion matrix from evaluate.confusion matrix. hide_spines : bool (default: False) Hides axis spines if True. hide_ticks : bool (default: False) Hides axis ticks if True figsize : tuple (default: (2.5, 2.5)) Height and width of the figure cmap : matplotlib colormap (default: None ) Uses matplotlib.pyplot.cm.Blues if None colorbar : bool (default: False) Shows a colorbar if True show_absolute : bool (default: True) Shows absolute confusion matrix coefficients if True. At least one of show_absolute or show_normed must be True. show_normed : bool (default: False) Shows normed confusion matrix coefficients if True. The normed confusion matrix coefficients give the proportion of training examples per class that are assigned the correct label. At least one of show_absolute or show_normed must be True. norm_colormap : bool (default: False) Matplotlib color normalization object to normalize the color scale, e.g., matplotlib.colors.LogNorm() . class_names : array-like, shape = [n_classes] (default: None) List of class names. If not None , ticks will be set to these values. figure : None or Matplotlib figure (default: None) If None will create a new figure. axis : None or Matplotlib figure axis (default: None) If None will create a new axis. fontcolor_threshold : Float (default: 0.5) Sets a threshold for choosing black and white font colors for the cells. By default all values larger than 0.5 times the maximum cell value are converted to white, and everything equal or smaller than 0.5 times the maximum cell value are converted to black. Returns fig, ax : matplotlib.pyplot subplot objects Figure and axis elements of the subplot. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/","title":"Plot confusion matrix"},{"location":"api_modules/mlxtend.plotting/plot_confusion_matrix/#plot_confusion_matrix","text":"plot_confusion_matrix(conf_mat, hide_spines=False, hide_ticks=False, figsize=None, cmap=None, colorbar=False, show_absolute=True, show_normed=False, norm_colormap=None, class_names=None, figure=None, axis=None, fontcolor_threshold=0.5) Plot a confusion matrix via matplotlib. Parameters conf_mat : array-like, shape = [n_classes, n_classes] Confusion matrix from evaluate.confusion matrix. hide_spines : bool (default: False) Hides axis spines if True. hide_ticks : bool (default: False) Hides axis ticks if True figsize : tuple (default: (2.5, 2.5)) Height and width of the figure cmap : matplotlib colormap (default: None ) Uses matplotlib.pyplot.cm.Blues if None colorbar : bool (default: False) Shows a colorbar if True show_absolute : bool (default: True) Shows absolute confusion matrix coefficients if True. At least one of show_absolute or show_normed must be True. show_normed : bool (default: False) Shows normed confusion matrix coefficients if True. The normed confusion matrix coefficients give the proportion of training examples per class that are assigned the correct label. At least one of show_absolute or show_normed must be True. norm_colormap : bool (default: False) Matplotlib color normalization object to normalize the color scale, e.g., matplotlib.colors.LogNorm() . class_names : array-like, shape = [n_classes] (default: None) List of class names. If not None , ticks will be set to these values. figure : None or Matplotlib figure (default: None) If None will create a new figure. axis : None or Matplotlib figure axis (default: None) If None will create a new axis. fontcolor_threshold : Float (default: 0.5) Sets a threshold for choosing black and white font colors for the cells. By default all values larger than 0.5 times the maximum cell value are converted to white, and everything equal or smaller than 0.5 times the maximum cell value are converted to black. Returns fig, ax : matplotlib.pyplot subplot objects Figure and axis elements of the subplot. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/","title":"plot_confusion_matrix"},{"location":"api_modules/mlxtend.plotting/plot_decision_regions/","text":"plot_decision_regions plot_decision_regions(X, y, clf, feature_index=None, filler_feature_values=None, filler_feature_ranges=None, ax=None, X_highlight=None, zoom_factor=1.0, legend=1, hide_spines=True, markers='s^oxv<>', colors='#1f77b4,#ff7f0e,#3ca02c,#d62728,#9467bd,#8c564b,#e377c2,#7f7f7f,#bcbd22,#17becf', scatter_kwargs=None, contourf_kwargs=None, contour_kwargs=None, scatter_highlight_kwargs=None, n_jobs=None) Plot decision regions of a classifier. Please note that this functions assumes that class labels are labeled consecutively, e.g,. 0, 1, 2, 3, 4, and 5. If you have class labels with integer labels > 4, you may want to provide additional colors and/or markers as `colors` and `markers` arguments. See https://matplotlib.org/examples/color/named_colors.html for more information. Parameters X : array-like, shape = [n_samples, n_features] Feature Matrix. y : array-like, shape = [n_samples] True class labels. clf : Classifier object. Must have a .predict method. feature_index : array-like (default: (0,) for 1D, (0, 1) otherwise) Feature indices to use for plotting. The first index in feature_index will be on the x-axis, the second index will be on the y-axis. filler_feature_values : dict (default: None) Only needed for number features > 2. Dictionary of feature index-value pairs for the features not being plotted. filler_feature_ranges : dict (default: None) Only needed for number features > 2. Dictionary of feature index-value pairs for the features not being plotted. Will use the ranges provided to select training samples for plotting. ax : matplotlib.axes.Axes (default: None) An existing matplotlib Axes. Creates one if ax=None. X_highlight : array-like, shape = [n_samples, n_features] (default: None) An array with data points that are used to highlight samples in X . zoom_factor : float (default: 1.0) Controls the scale of the x- and y-axis of the decision plot. hide_spines : bool (default: True) Hide axis spines if True. legend : int (default: 1) Integer to specify the legend location. No legend if legend is 0. markers : str (default: 's^oxv<>') Scatterplot markers. colors : str (default: 'red,blue,limegreen,gray,cyan') Comma separated list of colors. scatter_kwargs : dict (default: None) Keyword arguments for underlying matplotlib scatter function. contourf_kwargs : dict (default: None) Keyword arguments for underlying matplotlib contourf function. contour_kwargs : dict (default: None) Keyword arguments for underlying matplotlib contour function (which draws the lines between decision regions). scatter_highlight_kwargs : dict (default: None) Keyword arguments for underlying matplotlib scatter function. n_jobs : int or None, optional (default=None) The number of CPUs to use to do the computation using Python's multiprocessing library. None means 1. -1 means using all processors. New in v0.22.0. Returns ax : matplotlib.axes.Axes object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/","title":"Plot decision regions"},{"location":"api_modules/mlxtend.plotting/plot_decision_regions/#plot_decision_regions","text":"plot_decision_regions(X, y, clf, feature_index=None, filler_feature_values=None, filler_feature_ranges=None, ax=None, X_highlight=None, zoom_factor=1.0, legend=1, hide_spines=True, markers='s^oxv<>', colors='#1f77b4,#ff7f0e,#3ca02c,#d62728,#9467bd,#8c564b,#e377c2,#7f7f7f,#bcbd22,#17becf', scatter_kwargs=None, contourf_kwargs=None, contour_kwargs=None, scatter_highlight_kwargs=None, n_jobs=None) Plot decision regions of a classifier. Please note that this functions assumes that class labels are labeled consecutively, e.g,. 0, 1, 2, 3, 4, and 5. If you have class labels with integer labels > 4, you may want to provide additional colors and/or markers as `colors` and `markers` arguments. See https://matplotlib.org/examples/color/named_colors.html for more information. Parameters X : array-like, shape = [n_samples, n_features] Feature Matrix. y : array-like, shape = [n_samples] True class labels. clf : Classifier object. Must have a .predict method. feature_index : array-like (default: (0,) for 1D, (0, 1) otherwise) Feature indices to use for plotting. The first index in feature_index will be on the x-axis, the second index will be on the y-axis. filler_feature_values : dict (default: None) Only needed for number features > 2. Dictionary of feature index-value pairs for the features not being plotted. filler_feature_ranges : dict (default: None) Only needed for number features > 2. Dictionary of feature index-value pairs for the features not being plotted. Will use the ranges provided to select training samples for plotting. ax : matplotlib.axes.Axes (default: None) An existing matplotlib Axes. Creates one if ax=None. X_highlight : array-like, shape = [n_samples, n_features] (default: None) An array with data points that are used to highlight samples in X . zoom_factor : float (default: 1.0) Controls the scale of the x- and y-axis of the decision plot. hide_spines : bool (default: True) Hide axis spines if True. legend : int (default: 1) Integer to specify the legend location. No legend if legend is 0. markers : str (default: 's^oxv<>') Scatterplot markers. colors : str (default: 'red,blue,limegreen,gray,cyan') Comma separated list of colors. scatter_kwargs : dict (default: None) Keyword arguments for underlying matplotlib scatter function. contourf_kwargs : dict (default: None) Keyword arguments for underlying matplotlib contourf function. contour_kwargs : dict (default: None) Keyword arguments for underlying matplotlib contour function (which draws the lines between decision regions). scatter_highlight_kwargs : dict (default: None) Keyword arguments for underlying matplotlib scatter function. n_jobs : int or None, optional (default=None) The number of CPUs to use to do the computation using Python's multiprocessing library. None means 1. -1 means using all processors. New in v0.22.0. Returns ax : matplotlib.axes.Axes object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/","title":"plot_decision_regions"},{"location":"api_modules/mlxtend.plotting/plot_learning_curves/","text":"plot_learning_curves plot_learning_curves(X_train, y_train, X_test, y_test, clf, train_marker='o', test_marker='^', scoring='misclassification error', suppress_plot=False, print_model=True, title_fontsize=12, style='default', legend_loc='best') Plots learning curves of a classifier. Parameters X_train : array-like, shape = [n_samples, n_features] Feature matrix of the training dataset. y_train : array-like, shape = [n_samples] True class labels of the training dataset. X_test : array-like, shape = [n_samples, n_features] Feature matrix of the test dataset. y_test : array-like, shape = [n_samples] True class labels of the test dataset. clf : Classifier object. Must have a .predict .fit method. train_marker : str (default: 'o') Marker for the training set line plot. test_marker : str (default: '^') Marker for the test set line plot. scoring : str (default: 'misclassification error') If not 'misclassification error', accepts the following metrics (from scikit-learn): {'accuracy', 'average_precision', 'f1_micro', 'f1_macro', 'f1_weighted', 'f1_samples', 'log_loss', 'precision', 'recall', 'roc_auc', 'adjusted_rand_score', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'r2'} suppress_plot=False : bool (default: False) Suppress matplotlib plots if True. Recommended for testing purposes. print_model : bool (default: True) Print model parameters in plot title if True. title_fontsize : int (default: 12) Determines the size of the plot title font. style : str (default: 'default') Matplotlib style. For more styles, please see https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html legend_loc : str (default: 'best') Where to place the plot legend: {'best', 'upper left', 'upper right', 'lower left', 'lower right'} Returns errors : (training_error, test_error): tuple of lists Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_learning_curves/","title":"Plot learning curves"},{"location":"api_modules/mlxtend.plotting/plot_learning_curves/#plot_learning_curves","text":"plot_learning_curves(X_train, y_train, X_test, y_test, clf, train_marker='o', test_marker='^', scoring='misclassification error', suppress_plot=False, print_model=True, title_fontsize=12, style='default', legend_loc='best') Plots learning curves of a classifier. Parameters X_train : array-like, shape = [n_samples, n_features] Feature matrix of the training dataset. y_train : array-like, shape = [n_samples] True class labels of the training dataset. X_test : array-like, shape = [n_samples, n_features] Feature matrix of the test dataset. y_test : array-like, shape = [n_samples] True class labels of the test dataset. clf : Classifier object. Must have a .predict .fit method. train_marker : str (default: 'o') Marker for the training set line plot. test_marker : str (default: '^') Marker for the test set line plot. scoring : str (default: 'misclassification error') If not 'misclassification error', accepts the following metrics (from scikit-learn): {'accuracy', 'average_precision', 'f1_micro', 'f1_macro', 'f1_weighted', 'f1_samples', 'log_loss', 'precision', 'recall', 'roc_auc', 'adjusted_rand_score', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'r2'} suppress_plot=False : bool (default: False) Suppress matplotlib plots if True. Recommended for testing purposes. print_model : bool (default: True) Print model parameters in plot title if True. title_fontsize : int (default: 12) Determines the size of the plot title font. style : str (default: 'default') Matplotlib style. For more styles, please see https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html legend_loc : str (default: 'best') Where to place the plot legend: {'best', 'upper left', 'upper right', 'lower left', 'lower right'} Returns errors : (training_error, test_error): tuple of lists Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_learning_curves/","title":"plot_learning_curves"},{"location":"api_modules/mlxtend.plotting/plot_linear_regression/","text":"plot_linear_regression plot_linear_regression(X, y, model=LinearRegression(), corr_func='pearsonr', scattercolor='blue', fit_style='k--', legend=True, xlim='auto') Plot a linear regression line fit. Parameters X : numpy array, shape = [n_samples,] Samples. y : numpy array, shape (n_samples,) Target values model: object (default: sklearn.linear_model.LinearRegression) Estimator object for regression. Must implement a .fit() and .predict() method. corr_func: str or function (default: 'pearsonr') Uses pearsonr from scipy.stats if corr_func='pearsonr'. to compute the regression slope. If not 'pearsonr', the corr_func , the corr_func parameter expects a function of the form func( , ) as inputs, which is expected to return a tuple (<correlation_coefficient>, <some_unused_value>) . scattercolor: string (default: blue) Color of scatter plot points. fit_style: string (default: k--) Style for the line fit. legend: bool (default: True) Plots legend with corr_coeff coef., fit coef., and intercept values. xlim: array-like (x_min, x_max) or 'auto' (default: 'auto') X-axis limits for the linear line fit. Returns regression_fit : tuple intercept, slope, corr_coeff (float, float, float) Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_linear_regression/","title":"Plot linear regression"},{"location":"api_modules/mlxtend.plotting/plot_linear_regression/#plot_linear_regression","text":"plot_linear_regression(X, y, model=LinearRegression(), corr_func='pearsonr', scattercolor='blue', fit_style='k--', legend=True, xlim='auto') Plot a linear regression line fit. Parameters X : numpy array, shape = [n_samples,] Samples. y : numpy array, shape (n_samples,) Target values model: object (default: sklearn.linear_model.LinearRegression) Estimator object for regression. Must implement a .fit() and .predict() method. corr_func: str or function (default: 'pearsonr') Uses pearsonr from scipy.stats if corr_func='pearsonr'. to compute the regression slope. If not 'pearsonr', the corr_func , the corr_func parameter expects a function of the form func( , ) as inputs, which is expected to return a tuple (<correlation_coefficient>, <some_unused_value>) . scattercolor: string (default: blue) Color of scatter plot points. fit_style: string (default: k--) Style for the line fit. legend: bool (default: True) Plots legend with corr_coeff coef., fit coef., and intercept values. xlim: array-like (x_min, x_max) or 'auto' (default: 'auto') X-axis limits for the linear line fit. Returns regression_fit : tuple intercept, slope, corr_coeff (float, float, float) Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_linear_regression/","title":"plot_linear_regression"},{"location":"api_modules/mlxtend.plotting/plot_pca_correlation_graph/","text":"plot_pca_correlation_graph plot_pca_correlation_graph(X, variables_names, dimensions=(1, 2), figure_axis_size=6, X_pca=None, explained_variance=None) Compute the PCA for X and plots the Correlation graph Parameters X : 2d array like. The columns represent the different variables and the rows are the samples of thos variables variables_names : array like Name of the columns (the variables) of X dimensions: tuple with two elements. dimensions to be plotted (x,y) figure_axis_size : size of the final frame. The figure created is a square with length and width equal to figure_axis_size. X_pca : np.ndarray, shape = [n_samples, n_components]. Optional. X_pca is the matrix of the transformed components from X. If not provided, the function computes PCA automatically using mlxtend.feature_extraction.PrincipalComponentAnalysis Expected n_componentes >= max(dimensions) explained_variance : 1 dimension np.ndarray, length = n_components Optional. explained_variance are the eigenvalues from the diagonalized covariance matrix on the PCA transformatiopn. If not provided, the function computes PCA independently Expected n_componentes == X.shape[1] Returns matplotlib_figure, correlation_matrix Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_pca_correlation_graph/","title":"Plot pca correlation graph"},{"location":"api_modules/mlxtend.plotting/plot_pca_correlation_graph/#plot_pca_correlation_graph","text":"plot_pca_correlation_graph(X, variables_names, dimensions=(1, 2), figure_axis_size=6, X_pca=None, explained_variance=None) Compute the PCA for X and plots the Correlation graph Parameters X : 2d array like. The columns represent the different variables and the rows are the samples of thos variables variables_names : array like Name of the columns (the variables) of X dimensions: tuple with two elements. dimensions to be plotted (x,y) figure_axis_size : size of the final frame. The figure created is a square with length and width equal to figure_axis_size. X_pca : np.ndarray, shape = [n_samples, n_components]. Optional. X_pca is the matrix of the transformed components from X. If not provided, the function computes PCA automatically using mlxtend.feature_extraction.PrincipalComponentAnalysis Expected n_componentes >= max(dimensions) explained_variance : 1 dimension np.ndarray, length = n_components Optional. explained_variance are the eigenvalues from the diagonalized covariance matrix on the PCA transformatiopn. If not provided, the function computes PCA independently Expected n_componentes == X.shape[1] Returns matplotlib_figure, correlation_matrix Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_pca_correlation_graph/","title":"plot_pca_correlation_graph"},{"location":"api_modules/mlxtend.plotting/plot_sequential_feature_selection/","text":"plot_sequential_feature_selection plot_sequential_feature_selection(metric_dict, figsize=None, kind='std_dev', color='blue', bcolor='steelblue', marker='o', alpha=0.2, ylabel='Performance', confidence_interval=0.95) Plot feature selection results. Parameters metric_dict : mlxtend.SequentialFeatureSelector.get_metric_dict() object figsize : tuple (default: None) Height and width of the figure kind : str (default: \"std_dev\") The kind of error bar or confidence interval in {'std_dev', 'std_err', 'ci', None}. color : str (default: \"blue\") Color of the lineplot (accepts any matplotlib color name) bcolor : str (default: \"steelblue\"). Color of the error bars / confidence intervals (accepts any matplotlib color name). marker : str (default: \"o\") Marker of the line plot (accepts any matplotlib marker name). alpha : float in [0, 1] (default: 0.2) Transparency of the error bars / confidence intervals. ylabel : str (default: \"Performance\") Y-axis label. confidence_interval : float (default: 0.95) Confidence level if kind='ci' . Returns fig : matplotlib.pyplot.figure() object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_sequential_feature_selection/","title":"Plot sequential feature selection"},{"location":"api_modules/mlxtend.plotting/plot_sequential_feature_selection/#plot_sequential_feature_selection","text":"plot_sequential_feature_selection(metric_dict, figsize=None, kind='std_dev', color='blue', bcolor='steelblue', marker='o', alpha=0.2, ylabel='Performance', confidence_interval=0.95) Plot feature selection results. Parameters metric_dict : mlxtend.SequentialFeatureSelector.get_metric_dict() object figsize : tuple (default: None) Height and width of the figure kind : str (default: \"std_dev\") The kind of error bar or confidence interval in {'std_dev', 'std_err', 'ci', None}. color : str (default: \"blue\") Color of the lineplot (accepts any matplotlib color name) bcolor : str (default: \"steelblue\"). Color of the error bars / confidence intervals (accepts any matplotlib color name). marker : str (default: \"o\") Marker of the line plot (accepts any matplotlib marker name). alpha : float in [0, 1] (default: 0.2) Transparency of the error bars / confidence intervals. ylabel : str (default: \"Performance\") Y-axis label. confidence_interval : float (default: 0.95) Confidence level if kind='ci' . Returns fig : matplotlib.pyplot.figure() object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_sequential_feature_selection/","title":"plot_sequential_feature_selection"},{"location":"api_modules/mlxtend.plotting/remove_borders/","text":"remove_borders remove_borders(axes, left=False, bottom=False, right=True, top=True) Remove chart junk from matplotlib plots. Parameters axes : iterable An iterable containing plt.gca() or plt.subplot() objects, e.g. [plt.gca()]. left : bool (default: False ) Hide left axis spine if True. bottom : bool (default: False ) Hide bottom axis spine if True. right : bool (default: True ) Hide right axis spine if True. top : bool (default: True ) Hide top axis spine if True. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/remove_chartjunk/","title":"Remove borders"},{"location":"api_modules/mlxtend.plotting/remove_borders/#remove_borders","text":"remove_borders(axes, left=False, bottom=False, right=True, top=True) Remove chart junk from matplotlib plots. Parameters axes : iterable An iterable containing plt.gca() or plt.subplot() objects, e.g. [plt.gca()]. left : bool (default: False ) Hide left axis spine if True. bottom : bool (default: False ) Hide bottom axis spine if True. right : bool (default: True ) Hide right axis spine if True. top : bool (default: True ) Hide top axis spine if True. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/remove_chartjunk/","title":"remove_borders"},{"location":"api_modules/mlxtend.plotting/scatter_hist/","text":"scatter_hist scatter_hist(x, y, xlabel=None, ylabel=None, figsize=(5, 5)) Scatter plot and individual feature histograms along axes. Parameters x : 1D array-like or Pandas Series X-axis values. y : 1D array-like or Pandas Series Y-axis values. xlabel : str (default: None ) Label for the X-axis values. If x is a pandas Series, and xlabel is None , the label is inferred automatically. ylabel : str (default: None ) Label for the X-axis values. If y is a pandas Series, and ylabel is None , the label is inferred automatically. figsize : tuple (default: (5, 5) ) Matplotlib figure size. Returns plot : Matplotlib Figure object","title":"Scatter hist"},{"location":"api_modules/mlxtend.plotting/scatter_hist/#scatter_hist","text":"scatter_hist(x, y, xlabel=None, ylabel=None, figsize=(5, 5)) Scatter plot and individual feature histograms along axes. Parameters x : 1D array-like or Pandas Series X-axis values. y : 1D array-like or Pandas Series Y-axis values. xlabel : str (default: None ) Label for the X-axis values. If x is a pandas Series, and xlabel is None , the label is inferred automatically. ylabel : str (default: None ) Label for the X-axis values. If y is a pandas Series, and ylabel is None , the label is inferred automatically. figsize : tuple (default: (5, 5) ) Matplotlib figure size. Returns plot : Matplotlib Figure object","title":"scatter_hist"},{"location":"api_modules/mlxtend.plotting/scatterplotmatrix/","text":"scatterplotmatrix scatterplotmatrix(X, fig_axes=None, names=None, figsize=(8, 8), alpha=1.0, kwargs) Lower triangular of a scatterplot matrix Parameters X : array-like, shape={num_examples, num_features} Design matrix containing data instances (examples) with multiple exploratory variables (features). fix_axes : tuple (default: None) A (fig, axes) tuple, where fig is an figure object and axes is an axes object created via matplotlib, for example, by calling the pyplot subplot function fig, axes = plt.subplots(...) names : list (default: None) A list of string names, which should have the same number of elements as there are features (columns) in X . figsize : tuple (default: (8, 8)) Height and width of the subplot grid. Ignored if fig_axes is not None . alpha : float (default: 1.0) Transparency for both the scatter plots and the histograms along the diagonal. **kwargs : kwargs Keyword arguments for the scatterplots. Returns fix_axes : tuple A (fig, axes) tuple, where fig is an figure object and axes is an axes object created via matplotlib, for example, by calling the pyplot subplot function fig, axes = plt.subplots(...) Examples For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/scatterplotmatrix/","title":"Scatterplotmatrix"},{"location":"api_modules/mlxtend.plotting/scatterplotmatrix/#scatterplotmatrix","text":"scatterplotmatrix(X, fig_axes=None, names=None, figsize=(8, 8), alpha=1.0, kwargs) Lower triangular of a scatterplot matrix Parameters X : array-like, shape={num_examples, num_features} Design matrix containing data instances (examples) with multiple exploratory variables (features). fix_axes : tuple (default: None) A (fig, axes) tuple, where fig is an figure object and axes is an axes object created via matplotlib, for example, by calling the pyplot subplot function fig, axes = plt.subplots(...) names : list (default: None) A list of string names, which should have the same number of elements as there are features (columns) in X . figsize : tuple (default: (8, 8)) Height and width of the subplot grid. Ignored if fig_axes is not None . alpha : float (default: 1.0) Transparency for both the scatter plots and the histograms along the diagonal. **kwargs : kwargs Keyword arguments for the scatterplots. Returns fix_axes : tuple A (fig, axes) tuple, where fig is an figure object and axes is an axes object created via matplotlib, for example, by calling the pyplot subplot function fig, axes = plt.subplots(...) Examples For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/scatterplotmatrix/","title":"scatterplotmatrix"},{"location":"api_modules/mlxtend.plotting/stacked_barplot/","text":"stacked_barplot stacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend_loc='best') Function to plot stacked barplots Parameters df : pandas.DataFrame A pandas DataFrame where the index denotes the x-axis labels, and the columns contain the different measurements for each row. bar_width: 'auto' or float (default: 'auto') Parameter to set the widths of the bars. if 'auto', the width is automatically determined by the number of columns in the dataset. colors: str (default: 'bgrcky') The colors of the bars. labels: 'index' or iterable (default: 'index') If 'index', the DataFrame index will be used as x-tick labels. rotation: int (default: 90) Parameter to rotate the x-axis labels. legend_loc : str (default: 'best') Location of the plot legend {best, upper left, upper right, lower left, lower right} No legend if legend_loc=False Returns fig : matplotlib.pyplot figure object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/stacked_barplot/","title":"Stacked barplot"},{"location":"api_modules/mlxtend.plotting/stacked_barplot/#stacked_barplot","text":"stacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend_loc='best') Function to plot stacked barplots Parameters df : pandas.DataFrame A pandas DataFrame where the index denotes the x-axis labels, and the columns contain the different measurements for each row. bar_width: 'auto' or float (default: 'auto') Parameter to set the widths of the bars. if 'auto', the width is automatically determined by the number of columns in the dataset. colors: str (default: 'bgrcky') The colors of the bars. labels: 'index' or iterable (default: 'index') If 'index', the DataFrame index will be used as x-tick labels. rotation: int (default: 90) Parameter to rotate the x-axis labels. legend_loc : str (default: 'best') Location of the plot legend {best, upper left, upper right, lower left, lower right} No legend if legend_loc=False Returns fig : matplotlib.pyplot figure object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/stacked_barplot/","title":"stacked_barplot"},{"location":"api_modules/mlxtend.preprocessing/CopyTransformer/","text":"CopyTransformer CopyTransformer() Transformer that returns a copy of the input array For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/CopyTransformer/ Methods fit(X, y=None) Mock method. Does nothing. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns self fit_transform(X, y=None) Return a copy of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_copy : copy of the input X array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X, y=None) Return a copy of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_copy : copy of the input X array.","title":"CopyTransformer"},{"location":"api_modules/mlxtend.preprocessing/CopyTransformer/#copytransformer","text":"CopyTransformer() Transformer that returns a copy of the input array For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/CopyTransformer/","title":"CopyTransformer"},{"location":"api_modules/mlxtend.preprocessing/CopyTransformer/#methods","text":"fit(X, y=None) Mock method. Does nothing. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns self fit_transform(X, y=None) Return a copy of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_copy : copy of the input X array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X, y=None) Return a copy of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_copy : copy of the input X array.","title":"Methods"},{"location":"api_modules/mlxtend.preprocessing/DenseTransformer/","text":"DenseTransformer DenseTransformer(return_copy=True) Convert a sparse array into a dense array. For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/DenseTransformer/ Methods fit(X, y=None) Mock method. Does nothing. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns self fit_transform(X, y=None) Return a dense version of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_dense : dense version of the input X array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X, y=None) Return a dense version of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_dense : dense version of the input X array.","title":"DenseTransformer"},{"location":"api_modules/mlxtend.preprocessing/DenseTransformer/#densetransformer","text":"DenseTransformer(return_copy=True) Convert a sparse array into a dense array. For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/DenseTransformer/","title":"DenseTransformer"},{"location":"api_modules/mlxtend.preprocessing/DenseTransformer/#methods","text":"fit(X, y=None) Mock method. Does nothing. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns self fit_transform(X, y=None) Return a dense version of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_dense : dense version of the input X array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X, y=None) Return a dense version of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_dense : dense version of the input X array.","title":"Methods"},{"location":"api_modules/mlxtend.preprocessing/MeanCenterer/","text":"MeanCenterer MeanCenterer() Column centering of vectors and matrices. Attributes col_means : numpy.ndarray [n_columns] NumPy array storing the mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/MeanCenterer/ Methods fit(X) Gets the column means for mean centering. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Array of data vectors, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X) Fits and transforms an arry. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Array of data vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_tr : {array-like, sparse matrix}, shape = [n_samples, n_features] A copy of the input array with the columns centered. transform(X) Centers a NumPy array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Array of data vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_tr : {array-like, sparse matrix}, shape = [n_samples, n_features] A copy of the input array with the columns centered.","title":"MeanCenterer"},{"location":"api_modules/mlxtend.preprocessing/MeanCenterer/#meancenterer","text":"MeanCenterer() Column centering of vectors and matrices. Attributes col_means : numpy.ndarray [n_columns] NumPy array storing the mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/MeanCenterer/","title":"MeanCenterer"},{"location":"api_modules/mlxtend.preprocessing/MeanCenterer/#methods","text":"fit(X) Gets the column means for mean centering. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Array of data vectors, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X) Fits and transforms an arry. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Array of data vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_tr : {array-like, sparse matrix}, shape = [n_samples, n_features] A copy of the input array with the columns centered. transform(X) Centers a NumPy array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Array of data vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_tr : {array-like, sparse matrix}, shape = [n_samples, n_features] A copy of the input array with the columns centered.","title":"Methods"},{"location":"api_modules/mlxtend.preprocessing/TransactionEncoder/","text":"TransactionEncoder TransactionEncoder() Encoder class for transaction data in Python lists Parameters None Attributes columns_: list List of unique names in the X input list of lists Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/TransactionEncoder/ Methods fit(X) Learn unique column names from transaction DataFrame Parameters X : list of lists A python list of lists, where the outer list stores the n transactions and the inner list stores the items in each transaction. For example, [['Apple', 'Beer', 'Rice', 'Chicken'], ['Apple', 'Beer', 'Rice'], ['Apple', 'Beer'], ['Apple', 'Bananas'], ['Milk', 'Beer', 'Rice', 'Chicken'], ['Milk', 'Beer', 'Rice'], ['Milk', 'Beer'], ['Apple', 'Bananas']] fit_transform(X, sparse=False) Fit a TransactionEncoder encoder and transform a dataset. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. inverse_transform(array) Transforms an encoded NumPy array back into transactions. Parameters array : NumPy array [n_transactions, n_unique_items] The NumPy one-hot encoded boolean array of the input transactions, where the columns represent the unique items found in the input array in alphabetic order For example, array([[True , False, True , True , False, True ], [True , False, True , False, False, True ], [True , False, True , False, False, False], [True , True , False, False, False, False], [False, False, True , True , True , True ], [False, False, True , False, True , True ], [False, False, True , False, True , False], [True , True , False, False, False, False]]) The corresponding column labels are available as self.columns_, e.g., ['Apple', 'Bananas', 'Beer', 'Chicken', 'Milk', 'Rice'] Returns X : list of lists A python list of lists, where the outer list stores the n transactions and the inner list stores the items in each transaction. For example, [['Apple', 'Beer', 'Rice', 'Chicken'], ['Apple', 'Beer', 'Rice'], ['Apple', 'Beer'], ['Apple', 'Bananas'], ['Milk', 'Beer', 'Rice', 'Chicken'], ['Milk', 'Beer', 'Rice'], ['Milk', 'Beer'], ['Apple', 'Bananas']] set_inverse_transform_request(self: mlxtend.preprocessing.transactionencoder.TransactionEncoder, , array: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.preprocessing.transactionencoder.TransactionEncoder* Request metadata passed to the inverse_transform method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``inverse_transform`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``inverse_transform``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters array : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for array parameter in inverse_transform . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. set_transform_request(self: mlxtend.preprocessing.transactionencoder.TransactionEncoder, , sparse: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.preprocessing.transactionencoder.TransactionEncoder* Request metadata passed to the transform method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sparse : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sparse parameter in transform . Returns self : object The updated object. transform(X, sparse=False) Transform transactions into a one-hot encoded NumPy array. Parameters X : list of lists A python list of lists, where the outer list stores the n transactions and the inner list stores the items in each transaction. For example, [['Apple', 'Beer', 'Rice', 'Chicken'], ['Apple', 'Beer', 'Rice'], ['Apple', 'Beer'], ['Apple', 'Bananas'], ['Milk', 'Beer', 'Rice', 'Chicken'], ['Milk', 'Beer', 'Rice'], ['Milk', 'Beer'], ['Apple', 'Bananas']] sparse: bool (default=False) If True, transform will return Compressed Sparse Row matrix instead of the regular one. Returns array : NumPy array [n_transactions, n_unique_items] if sparse=False (default). Compressed Sparse Row matrix otherwise The one-hot encoded boolean array of the input transactions, where the columns represent the unique items found in the input array in alphabetic order. Exact representation depends on the sparse argument For example, array([[True , False, True , True , False, True ], [True , False, True , False, False, True ], [True , False, True , False, False, False], [True , True , False, False, False, False], [False, False, True , True , True , True ], [False, False, True , False, True , True ], [False, False, True , False, True , False], [True , True , False, False, False, False]]) The corresponding column labels are available as self.columns_, e.g., ['Apple', 'Bananas', 'Beer', 'Chicken', 'Milk', 'Rice']","title":"TransactionEncoder"},{"location":"api_modules/mlxtend.preprocessing/TransactionEncoder/#transactionencoder","text":"TransactionEncoder() Encoder class for transaction data in Python lists Parameters None Attributes columns_: list List of unique names in the X input list of lists Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/TransactionEncoder/","title":"TransactionEncoder"},{"location":"api_modules/mlxtend.preprocessing/TransactionEncoder/#methods","text":"fit(X) Learn unique column names from transaction DataFrame Parameters X : list of lists A python list of lists, where the outer list stores the n transactions and the inner list stores the items in each transaction. For example, [['Apple', 'Beer', 'Rice', 'Chicken'], ['Apple', 'Beer', 'Rice'], ['Apple', 'Beer'], ['Apple', 'Bananas'], ['Milk', 'Beer', 'Rice', 'Chicken'], ['Milk', 'Beer', 'Rice'], ['Milk', 'Beer'], ['Apple', 'Bananas']] fit_transform(X, sparse=False) Fit a TransactionEncoder encoder and transform a dataset. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. inverse_transform(array) Transforms an encoded NumPy array back into transactions. Parameters array : NumPy array [n_transactions, n_unique_items] The NumPy one-hot encoded boolean array of the input transactions, where the columns represent the unique items found in the input array in alphabetic order For example, array([[True , False, True , True , False, True ], [True , False, True , False, False, True ], [True , False, True , False, False, False], [True , True , False, False, False, False], [False, False, True , True , True , True ], [False, False, True , False, True , True ], [False, False, True , False, True , False], [True , True , False, False, False, False]]) The corresponding column labels are available as self.columns_, e.g., ['Apple', 'Bananas', 'Beer', 'Chicken', 'Milk', 'Rice'] Returns X : list of lists A python list of lists, where the outer list stores the n transactions and the inner list stores the items in each transaction. For example, [['Apple', 'Beer', 'Rice', 'Chicken'], ['Apple', 'Beer', 'Rice'], ['Apple', 'Beer'], ['Apple', 'Bananas'], ['Milk', 'Beer', 'Rice', 'Chicken'], ['Milk', 'Beer', 'Rice'], ['Milk', 'Beer'], ['Apple', 'Bananas']] set_inverse_transform_request(self: mlxtend.preprocessing.transactionencoder.TransactionEncoder, , array: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.preprocessing.transactionencoder.TransactionEncoder* Request metadata passed to the inverse_transform method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``inverse_transform`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``inverse_transform``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters array : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for array parameter in inverse_transform . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. set_transform_request(self: mlxtend.preprocessing.transactionencoder.TransactionEncoder, , sparse: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.preprocessing.transactionencoder.TransactionEncoder* Request metadata passed to the transform method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sparse : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sparse parameter in transform . Returns self : object The updated object. transform(X, sparse=False) Transform transactions into a one-hot encoded NumPy array. Parameters X : list of lists A python list of lists, where the outer list stores the n transactions and the inner list stores the items in each transaction. For example, [['Apple', 'Beer', 'Rice', 'Chicken'], ['Apple', 'Beer', 'Rice'], ['Apple', 'Beer'], ['Apple', 'Bananas'], ['Milk', 'Beer', 'Rice', 'Chicken'], ['Milk', 'Beer', 'Rice'], ['Milk', 'Beer'], ['Apple', 'Bananas']] sparse: bool (default=False) If True, transform will return Compressed Sparse Row matrix instead of the regular one. Returns array : NumPy array [n_transactions, n_unique_items] if sparse=False (default). Compressed Sparse Row matrix otherwise The one-hot encoded boolean array of the input transactions, where the columns represent the unique items found in the input array in alphabetic order. Exact representation depends on the sparse argument For example, array([[True , False, True , True , False, True ], [True , False, True , False, False, True ], [True , False, True , False, False, False], [True , True , False, False, False, False], [False, False, True , True , True , True ], [False, False, True , False, True , True ], [False, False, True , False, True , False], [True , True , False, False, False, False]]) The corresponding column labels are available as self.columns_, e.g., ['Apple', 'Bananas', 'Beer', 'Chicken', 'Milk', 'Rice']","title":"Methods"},{"location":"api_modules/mlxtend.preprocessing/minmax_scaling/","text":"minmax_scaling minmax_scaling(array, columns, min_val=0, max_val=1) Min max scaling of pandas' DataFrames. Parameters array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/minmax_scaling/","title":"Minmax scaling"},{"location":"api_modules/mlxtend.preprocessing/minmax_scaling/#minmax_scaling","text":"minmax_scaling(array, columns, min_val=0, max_val=1) Min max scaling of pandas' DataFrames. Parameters array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/minmax_scaling/","title":"minmax_scaling"},{"location":"api_modules/mlxtend.preprocessing/one_hot/","text":"one_hot one_hot(y, num_labels='auto', dtype='float') One-hot encoding of class labels Parameters y : array-like, shape = [n_classlabels] Python list or numpy array consisting of class labels. num_labels : int or 'auto' Number of unique labels in the class label array. Infers the number of unique labels from the input array if set to 'auto'. dtype : str NumPy array type (float, float32, float64) of the output array. Returns ary : numpy.ndarray, shape = [n_classlabels] One-hot encoded array, where each sample is represented as a row vector in the returned array. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/one_hot/","title":"One hot"},{"location":"api_modules/mlxtend.preprocessing/one_hot/#one_hot","text":"one_hot(y, num_labels='auto', dtype='float') One-hot encoding of class labels Parameters y : array-like, shape = [n_classlabels] Python list or numpy array consisting of class labels. num_labels : int or 'auto' Number of unique labels in the class label array. Infers the number of unique labels from the input array if set to 'auto'. dtype : str NumPy array type (float, float32, float64) of the output array. Returns ary : numpy.ndarray, shape = [n_classlabels] One-hot encoded array, where each sample is represented as a row vector in the returned array. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/one_hot/","title":"one_hot"},{"location":"api_modules/mlxtend.preprocessing/shuffle_arrays_unison/","text":"shuffle_arrays_unison shuffle_arrays_unison(arrays, random_seed=None) Shuffle NumPy arrays in unison. Parameters arrays : array-like, shape = [n_arrays] A list of NumPy arrays. random_seed : int (default: None) Sets the random state. Returns shuffled_arrays : A list of NumPy arrays after shuffling. Examples ``` >>> import numpy as np >>> from mlxtend.preprocessing import shuffle_arrays_unison >>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> y1 = np.array([1, 2, 3]) >>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_seed=3) >>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all()) >>> assert(y2.all() == np.array([2, 1, 3]).all()) >>> For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/shuffle_arrays_unison/ ```","title":"Shuffle arrays unison"},{"location":"api_modules/mlxtend.preprocessing/shuffle_arrays_unison/#shuffle_arrays_unison","text":"shuffle_arrays_unison(arrays, random_seed=None) Shuffle NumPy arrays in unison. Parameters arrays : array-like, shape = [n_arrays] A list of NumPy arrays. random_seed : int (default: None) Sets the random state. Returns shuffled_arrays : A list of NumPy arrays after shuffling. Examples ``` >>> import numpy as np >>> from mlxtend.preprocessing import shuffle_arrays_unison >>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> y1 = np.array([1, 2, 3]) >>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_seed=3) >>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all()) >>> assert(y2.all() == np.array([2, 1, 3]).all()) >>> For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/shuffle_arrays_unison/ ```","title":"shuffle_arrays_unison"},{"location":"api_modules/mlxtend.preprocessing/standardize/","text":"standardize standardize(array, columns=None, ddof=0, return_params=False, params=None) Standardize columns in pandas DataFrames. Parameters array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] (default: None) Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] If None, standardizes all columns. ddof : int (default: 0) Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. return_params : dict (default: False) If set to True, a dictionary is returned in addition to the standardized array. The parameter dictionary contains the column means ('avgs') and standard deviations ('stds') of the individual columns. params : dict (default: None) A dictionary with column means and standard deviations as returned by the standardize function if return_params was set to True. If a params dictionary is provided, the standardize function will use these instead of computing them from the current array. Notes If all values in a given column are the same, these values are all set to 0.0 . The standard deviation in the parameters dictionary is consequently set to 1.0 to avoid dividing by zero. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with standardized columns. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/standardize/","title":"Standardize"},{"location":"api_modules/mlxtend.preprocessing/standardize/#standardize","text":"standardize(array, columns=None, ddof=0, return_params=False, params=None) Standardize columns in pandas DataFrames. Parameters array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] (default: None) Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] If None, standardizes all columns. ddof : int (default: 0) Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. return_params : dict (default: False) If set to True, a dictionary is returned in addition to the standardized array. The parameter dictionary contains the column means ('avgs') and standard deviations ('stds') of the individual columns. params : dict (default: None) A dictionary with column means and standard deviations as returned by the standardize function if return_params was set to True. If a params dictionary is provided, the standardize function will use these instead of computing them from the current array. Notes If all values in a given column are the same, these values are all set to 0.0 . The standard deviation in the parameters dictionary is consequently set to 1.0 to avoid dividing by zero. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with standardized columns. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/standardize/","title":"standardize"},{"location":"api_modules/mlxtend.regressor/LinearRegression/","text":"LinearRegression LinearRegression(method='direct', eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0) Ordinary least squares linear regression. Parameters method : string (default: 'direct') For gradient descent-based optimization, use sgd (see minibatch parameter for further options). Otherwise, if direct (default), the analytical method is used. For alternative, numerically more stable solutions, use either qr (QR decomopisition) or svd (Singular Value Decomposition). eta : float (default: 0.01) solver learning rate (between 0.0 and 1.0). Used with method = 'sgd' . (See methods parameter for details) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. Used with method = 'sgd' . (See methods parameter for details) minibatches : int (default: None) The number of minibatches for gradient-based optimization. If None: Direct method, QR, or SVD method (see method parameter for details) If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent learning If 1 < minibatches < len(y): Minibatch learning random_seed : int (default: None) Set random state for shuffling and initializing the weights. Used in method = 'sgd' . (See methods parameter for details) print_progress : int (default: 0) Prints progress in fitting to stderr if method = 'sgd' . 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list Sum of squared errors after each epoch; ignored if solver='normal equation' Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/ Methods fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"LinearRegression"},{"location":"api_modules/mlxtend.regressor/LinearRegression/#linearregression","text":"LinearRegression(method='direct', eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0) Ordinary least squares linear regression. Parameters method : string (default: 'direct') For gradient descent-based optimization, use sgd (see minibatch parameter for further options). Otherwise, if direct (default), the analytical method is used. For alternative, numerically more stable solutions, use either qr (QR decomopisition) or svd (Singular Value Decomposition). eta : float (default: 0.01) solver learning rate (between 0.0 and 1.0). Used with method = 'sgd' . (See methods parameter for details) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. Used with method = 'sgd' . (See methods parameter for details) minibatches : int (default: None) The number of minibatches for gradient-based optimization. If None: Direct method, QR, or SVD method (see method parameter for details) If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent learning If 1 < minibatches < len(y): Minibatch learning random_seed : int (default: None) Set random state for shuffling and initializing the weights. Used in method = 'sgd' . (See methods parameter for details) print_progress : int (default: 0) Prints progress in fitting to stderr if method = 'sgd' . 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list Sum of squared errors after each epoch; ignored if solver='normal equation' Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/","title":"LinearRegression"},{"location":"api_modules/mlxtend.regressor/LinearRegression/#methods","text":"fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Methods"},{"location":"api_modules/mlxtend.regressor/StackingCVRegressor/","text":"StackingCVRegressor StackingCVRegressor(regressors, meta_regressor, cv=5, shuffle=True, random_state=None, verbose=0, refit=True, use_features_in_secondary=False, store_train_meta_features=False, n_jobs=None, pre_dispatch='2 n_jobs', multi_output=False)* A 'Stacking Cross-Validation' regressor for scikit-learn estimators. Parameters regressors : array-like, shape = [n_regressors] A list of regressors. Invoking the fit method on the StackingCVRegressor will fit clones of these original regressors that will be stored in the class attribute self.regr_ . meta_regressor : object The meta-regressor to be fitted on the ensemble of regressor cv : int, cross-validation generator or iterable, optional (default: 5) Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross validation, - integer, to specify the number of folds in a KFold , - An object to be used as a cross-validation generator. - An iterable yielding train, test splits. For integer/None inputs, it will use KFold cross-validation shuffle : bool (default: True) If True, and the cv argument is integer, the training data will be shuffled at fitting stage prior to cross-validation. If the cv argument is a specific cross validation technique, this argument is omitted. random_state : int, RandomState instance or None, optional (default: None) Constrols the randomness of the cv splitter. Used when cv is integer and shuffle=True . New in v0.16.0. verbose : int, optional (default=0) Controls the verbosity of the building process. New in v0.16.0 refit : bool (default: True) Clones the regressors for stacking regression if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Setting refit=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. use_features_in_secondary : bool (default: False) If True, the meta-regressor will be trained both on the predictions of the original regressors and the original dataset. If False, the meta-regressor will be trained only on the predictions of the original regressors. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-regressor stored in the self.train_meta_features_ array, which can be accessed after calling fit . n_jobs : int or None, optional (default=None) The number of CPUs to use to do the computation. None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details. New in v0.16.0. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs - An int, giving the exact number of total jobs that are spawned - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' multi_output : bool (default: False) If True, allow multi-output targets, but forbid nan or inf values. If False, y will be checked to be a vector. (New in v0.19.0.) Attributes train_meta_features : numpy array, shape = [n_samples, n_regressors] meta-features for training data, where n_samples is the number of samples in training data and len(self.regressors) is the number of regressors. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor/ Methods fit(X, y, groups=None, sample_weight=None) Fit ensemble regressors and the meta-regressor. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : numpy array, shape = [n_samples] or [n_samples, n_targets] Target values. Multiple targets are supported only if self.multi_output is True. groups : numpy array/None, shape = [n_samples] The group that each sample belongs to. This is used by specific folding strategies such as GroupKFold() sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. predict(X) Predict target values for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns y_target : array-like, shape = [n_samples] or [n_samples, n_targets] Predicted target values. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, len(self.regressors)] meta-features for test data, where n_samples is the number of samples in test data and len(self.regressors) is the number of regressors. If self.multi_output is True, then the number of columns is len(self.regressors) * n_targets. score(X, y, sample_weight=None) Return the coefficient of determination of the prediction. The coefficient of determination :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual sum of squares ((y_true - y_pred)** 2).sum() and :math: v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum() . The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y , disregarding the input features, would get a :math: R^2 score of 0.0. Parameters X : array-like of shape (n_samples, n_features) Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted) , where n_samples_fitted is the number of samples used in the fitting for the estimator. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True values for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float :math: R^2 of self.predict(X) w.r.t. y . Notes The :math: R^2 score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of :func: ~sklearn.metrics.r2_score . This influences the score method of all the multioutput regressors (except for :class: ~sklearn.multioutput.MultiOutputRegressor ). set_fit_request(self: mlxtend.regressor.stacking_cv_regression.StackingCVRegressor, , groups: Union[bool, NoneType, str] = ' UNCHANGED ', sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_cv_regression.StackingCVRegressor* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.regressor.stacking_cv_regression.StackingCVRegressor, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_cv_regression.StackingCVRegressor* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object. Properties named_regressors Returns List of named estimator tuples, like [('svc', SVC(...))]","title":"StackingCVRegressor"},{"location":"api_modules/mlxtend.regressor/StackingCVRegressor/#stackingcvregressor","text":"StackingCVRegressor(regressors, meta_regressor, cv=5, shuffle=True, random_state=None, verbose=0, refit=True, use_features_in_secondary=False, store_train_meta_features=False, n_jobs=None, pre_dispatch='2 n_jobs', multi_output=False)* A 'Stacking Cross-Validation' regressor for scikit-learn estimators. Parameters regressors : array-like, shape = [n_regressors] A list of regressors. Invoking the fit method on the StackingCVRegressor will fit clones of these original regressors that will be stored in the class attribute self.regr_ . meta_regressor : object The meta-regressor to be fitted on the ensemble of regressor cv : int, cross-validation generator or iterable, optional (default: 5) Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross validation, - integer, to specify the number of folds in a KFold , - An object to be used as a cross-validation generator. - An iterable yielding train, test splits. For integer/None inputs, it will use KFold cross-validation shuffle : bool (default: True) If True, and the cv argument is integer, the training data will be shuffled at fitting stage prior to cross-validation. If the cv argument is a specific cross validation technique, this argument is omitted. random_state : int, RandomState instance or None, optional (default: None) Constrols the randomness of the cv splitter. Used when cv is integer and shuffle=True . New in v0.16.0. verbose : int, optional (default=0) Controls the verbosity of the building process. New in v0.16.0 refit : bool (default: True) Clones the regressors for stacking regression if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Setting refit=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. use_features_in_secondary : bool (default: False) If True, the meta-regressor will be trained both on the predictions of the original regressors and the original dataset. If False, the meta-regressor will be trained only on the predictions of the original regressors. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-regressor stored in the self.train_meta_features_ array, which can be accessed after calling fit . n_jobs : int or None, optional (default=None) The number of CPUs to use to do the computation. None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details. New in v0.16.0. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs - An int, giving the exact number of total jobs that are spawned - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' multi_output : bool (default: False) If True, allow multi-output targets, but forbid nan or inf values. If False, y will be checked to be a vector. (New in v0.19.0.) Attributes train_meta_features : numpy array, shape = [n_samples, n_regressors] meta-features for training data, where n_samples is the number of samples in training data and len(self.regressors) is the number of regressors. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor/","title":"StackingCVRegressor"},{"location":"api_modules/mlxtend.regressor/StackingCVRegressor/#methods","text":"fit(X, y, groups=None, sample_weight=None) Fit ensemble regressors and the meta-regressor. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : numpy array, shape = [n_samples] or [n_samples, n_targets] Target values. Multiple targets are supported only if self.multi_output is True. groups : numpy array/None, shape = [n_samples] The group that each sample belongs to. This is used by specific folding strategies such as GroupKFold() sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. predict(X) Predict target values for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns y_target : array-like, shape = [n_samples] or [n_samples, n_targets] Predicted target values. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, len(self.regressors)] meta-features for test data, where n_samples is the number of samples in test data and len(self.regressors) is the number of regressors. If self.multi_output is True, then the number of columns is len(self.regressors) * n_targets. score(X, y, sample_weight=None) Return the coefficient of determination of the prediction. The coefficient of determination :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual sum of squares ((y_true - y_pred)** 2).sum() and :math: v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum() . The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y , disregarding the input features, would get a :math: R^2 score of 0.0. Parameters X : array-like of shape (n_samples, n_features) Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted) , where n_samples_fitted is the number of samples used in the fitting for the estimator. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True values for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float :math: R^2 of self.predict(X) w.r.t. y . Notes The :math: R^2 score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of :func: ~sklearn.metrics.r2_score . This influences the score method of all the multioutput regressors (except for :class: ~sklearn.multioutput.MultiOutputRegressor ). set_fit_request(self: mlxtend.regressor.stacking_cv_regression.StackingCVRegressor, , groups: Union[bool, NoneType, str] = ' UNCHANGED ', sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_cv_regression.StackingCVRegressor* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.regressor.stacking_cv_regression.StackingCVRegressor, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_cv_regression.StackingCVRegressor* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object.","title":"Methods"},{"location":"api_modules/mlxtend.regressor/StackingCVRegressor/#properties","text":"named_regressors Returns List of named estimator tuples, like [('svc', SVC(...))]","title":"Properties"},{"location":"api_modules/mlxtend.regressor/StackingRegressor/","text":"StackingRegressor StackingRegressor(regressors, meta_regressor, verbose=0, use_features_in_secondary=False, store_train_meta_features=False, refit=True, multi_output=False) A Stacking regressor for scikit-learn estimators for regression. Parameters regressors : array-like, shape = [n_regressors] A list of regressors. Invoking the fit method on the StackingRegressor will fit clones of those original regressors that will be stored in the class attribute self.regr_ . meta_regressor : object The meta-regressor to be fitted on the ensemble of regressors verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the regressor being fitted - verbose=2 : Prints info about the parameters of the regressor being fitted - verbose>2 : Changes verbose param of the underlying regressor to self.verbose - 2 use_features_in_secondary : bool (default: False) If True, the meta-regressor will be trained both on the predictions of the original regressors and the original dataset. If False, the meta-regressor will be trained only on the predictions of the original regressors. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-regressor stored in the self.train_meta_features_ array, which can be accessed after calling fit . Attributes regr_ : list, shape=[n_regressors] Fitted regressors (clones of the original regressors) meta_regr_ : estimator Fitted meta-regressor (clone of the original meta-estimator) coef_ : array-like, shape = [n_features] Model coefficients of the fitted meta-estimator intercept_ : float Intercept of the fitted meta-estimator train_meta_features : numpy array, shape = [n_samples, len(self.regressors)] meta-features for training data, where n_samples is the number of samples in training data and len(self.regressors) is the number of regressors. refit : bool (default: True) Clones the regressors for stacking regression if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Setting refit=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor/ Methods fit(X, y, sample_weight=None) Learn weight coefficients from training data for each regressor. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : numpy array, shape = [n_samples] or [n_samples, n_targets] Target values. Multiple targets are supported only if self.multi_output is True. sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict target values for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns y_target : array-like, shape = [n_samples] or [n_samples, n_targets] Predicted target values. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, len(self.regressors)] meta-features for test data, where n_samples is the number of samples in test data and len(self.regressors) is the number of regressors. If self.multi_output is True, then the number of columns is len(self.regressors) * n_targets score(X, y, sample_weight=None) Return the coefficient of determination of the prediction. The coefficient of determination :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual sum of squares ((y_true - y_pred)** 2).sum() and :math: v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum() . The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y , disregarding the input features, would get a :math: R^2 score of 0.0. Parameters X : array-like of shape (n_samples, n_features) Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted) , where n_samples_fitted is the number of samples used in the fitting for the estimator. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True values for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float :math: R^2 of self.predict(X) w.r.t. y . Notes The :math: R^2 score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of :func: ~sklearn.metrics.r2_score . This influences the score method of all the multioutput regressors (except for :class: ~sklearn.multioutput.MultiOutputRegressor ). set_fit_request(self: mlxtend.regressor.stacking_regression.StackingRegressor, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_regression.StackingRegressor* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.regressor.stacking_regression.StackingRegressor, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_regression.StackingRegressor* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object. Properties coef_ None intercept_ None named_regressors None","title":"StackingRegressor"},{"location":"api_modules/mlxtend.regressor/StackingRegressor/#stackingregressor","text":"StackingRegressor(regressors, meta_regressor, verbose=0, use_features_in_secondary=False, store_train_meta_features=False, refit=True, multi_output=False) A Stacking regressor for scikit-learn estimators for regression. Parameters regressors : array-like, shape = [n_regressors] A list of regressors. Invoking the fit method on the StackingRegressor will fit clones of those original regressors that will be stored in the class attribute self.regr_ . meta_regressor : object The meta-regressor to be fitted on the ensemble of regressors verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the regressor being fitted - verbose=2 : Prints info about the parameters of the regressor being fitted - verbose>2 : Changes verbose param of the underlying regressor to self.verbose - 2 use_features_in_secondary : bool (default: False) If True, the meta-regressor will be trained both on the predictions of the original regressors and the original dataset. If False, the meta-regressor will be trained only on the predictions of the original regressors. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-regressor stored in the self.train_meta_features_ array, which can be accessed after calling fit . Attributes regr_ : list, shape=[n_regressors] Fitted regressors (clones of the original regressors) meta_regr_ : estimator Fitted meta-regressor (clone of the original meta-estimator) coef_ : array-like, shape = [n_features] Model coefficients of the fitted meta-estimator intercept_ : float Intercept of the fitted meta-estimator train_meta_features : numpy array, shape = [n_samples, len(self.regressors)] meta-features for training data, where n_samples is the number of samples in training data and len(self.regressors) is the number of regressors. refit : bool (default: True) Clones the regressors for stacking regression if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Setting refit=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor/","title":"StackingRegressor"},{"location":"api_modules/mlxtend.regressor/StackingRegressor/#methods","text":"fit(X, y, sample_weight=None) Learn weight coefficients from training data for each regressor. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : numpy array, shape = [n_samples] or [n_samples, n_targets] Target values. Multiple targets are supported only if self.multi_output is True. sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict target values for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns y_target : array-like, shape = [n_samples] or [n_samples, n_targets] Predicted target values. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, len(self.regressors)] meta-features for test data, where n_samples is the number of samples in test data and len(self.regressors) is the number of regressors. If self.multi_output is True, then the number of columns is len(self.regressors) * n_targets score(X, y, sample_weight=None) Return the coefficient of determination of the prediction. The coefficient of determination :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual sum of squares ((y_true - y_pred)** 2).sum() and :math: v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum() . The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y , disregarding the input features, would get a :math: R^2 score of 0.0. Parameters X : array-like of shape (n_samples, n_features) Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted) , where n_samples_fitted is the number of samples used in the fitting for the estimator. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True values for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float :math: R^2 of self.predict(X) w.r.t. y . Notes The :math: R^2 score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of :func: ~sklearn.metrics.r2_score . This influences the score method of all the multioutput regressors (except for :class: ~sklearn.multioutput.MultiOutputRegressor ). set_fit_request(self: mlxtend.regressor.stacking_regression.StackingRegressor, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_regression.StackingRegressor* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.regressor.stacking_regression.StackingRegressor, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_regression.StackingRegressor* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object.","title":"Methods"},{"location":"api_modules/mlxtend.regressor/StackingRegressor/#properties","text":"coef_ None intercept_ None named_regressors None","title":"Properties"},{"location":"api_modules/mlxtend.text/generalize_names/","text":"generalize_names generalize_names(name, output_sep=' ', firstname_output_letters=1) Generalize a person's first and last name. Returns a person's name in the format <last_name><separator><firstname letter(s)> (all lowercase) Parameters name : str Name of the player output_sep : str (default: ' ') String for separating last name and first name in the output. firstname_output_letters : int Number of letters in the abbreviated first name. Returns gen_name : str The generalized name. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/generalize_names/","title":"Generalize names"},{"location":"api_modules/mlxtend.text/generalize_names/#generalize_names","text":"generalize_names(name, output_sep=' ', firstname_output_letters=1) Generalize a person's first and last name. Returns a person's name in the format <last_name><separator><firstname letter(s)> (all lowercase) Parameters name : str Name of the player output_sep : str (default: ' ') String for separating last name and first name in the output. firstname_output_letters : int Number of letters in the abbreviated first name. Returns gen_name : str The generalized name. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/generalize_names/","title":"generalize_names"},{"location":"api_modules/mlxtend.text/generalize_names_duplcheck/","text":"generalize_names_duplcheck generalize_names_duplcheck(df, col_name) Generalizes names and removes duplicates. Applies mlxtend.text.generalize_names to a DataFrame with 1 first name letter by default and uses more first name letters if duplicates are detected. Parameters df : pandas.DataFrame DataFrame that contains a column where generalize_names should be applied. col_name : str Name of the DataFrame column where generalize_names function should be applied to. Returns df_new : str New DataFrame object where generalize_names function has been applied without duplicates. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/generalize_names_duplcheck/","title":"Generalize names duplcheck"},{"location":"api_modules/mlxtend.text/generalize_names_duplcheck/#generalize_names_duplcheck","text":"generalize_names_duplcheck(df, col_name) Generalizes names and removes duplicates. Applies mlxtend.text.generalize_names to a DataFrame with 1 first name letter by default and uses more first name letters if duplicates are detected. Parameters df : pandas.DataFrame DataFrame that contains a column where generalize_names should be applied. col_name : str Name of the DataFrame column where generalize_names function should be applied to. Returns df_new : str New DataFrame object where generalize_names function has been applied without duplicates. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/generalize_names_duplcheck/","title":"generalize_names_duplcheck"},{"location":"api_modules/mlxtend.text/tokenizer_emoticons/","text":"tokenizer_emoticons tokenizer_emoticons(text) Return emoticons from text Examples >>> tokenizer_emoticons('</a>This :) is :( a test :-)!') [':)', ':(', ':-)'] For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/tokenizer_emoticons/","title":"Tokenizer emoticons"},{"location":"api_modules/mlxtend.text/tokenizer_emoticons/#tokenizer_emoticons","text":"tokenizer_emoticons(text) Return emoticons from text Examples >>> tokenizer_emoticons('</a>This :) is :( a test :-)!') [':)', ':(', ':-)'] For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/tokenizer_emoticons/","title":"tokenizer_emoticons"},{"location":"api_modules/mlxtend.text/tokenizer_words_and_emoticons/","text":"tokenizer_words_and_emoticons tokenizer_words_and_emoticons(text) Convert text to lowercase words and emoticons. Examples >>> tokenizer_words_and_emoticons('</a>This :) is :( a test :-)!') ['this', 'is', 'a', 'test', ':)', ':(', ':-)'] For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/tokenizer_words_and_emoticons/","title":"Tokenizer words and emoticons"},{"location":"api_modules/mlxtend.text/tokenizer_words_and_emoticons/#tokenizer_words_and_emoticons","text":"tokenizer_words_and_emoticons(text) Convert text to lowercase words and emoticons. Examples >>> tokenizer_words_and_emoticons('</a>This :) is :( a test :-)!') ['this', 'is', 'a', 'test', ':)', ':(', ':-)'] For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/tokenizer_words_and_emoticons/","title":"tokenizer_words_and_emoticons"},{"location":"api_modules/mlxtend.utils/Counter/","text":"Counter Counter(stderr=False, start_newline=True, precision=0, name=None) Class to display the progress of for-loop iterators. Parameters stderr : bool (default: True) Prints output to sys.stderr if True; uses sys.stdout otherwise. start_newline : bool (default: True) Prepends a new line to the counter, which prevents overwriting counters if multiple counters are printed in succession. precision: int (default: 0) Sets the number of decimal places when displaying the time elapsed in seconds. name : string (default: None) Prepends the specified name before the counter to allow distinguishing between multiple counters. Attributes curr_iter : int The current iteration. start_time : float The system's time in seconds when the Counter was initialized. end_time : float The system's time in seconds when the Counter was last updated. Examples >>> cnt = Counter() >>> for i in range(20): ... # do some computation ... time.sleep(0.1) ... cnt.update() 20 iter | 2 sec >>> print('The counter was initialized.' ' %d seconds ago.' % (time.time() - cnt.start_time)) The counter was initialized 2 seconds ago >>> print('The counter was last updated' ' %d seconds ago.' % (time.time() - cnt.end_time)) The counter was last updated 0 seconds ago. For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/utils/Counter/ Methods update() Print current iteration and time elapsed.","title":"Counter"},{"location":"api_modules/mlxtend.utils/Counter/#counter","text":"Counter(stderr=False, start_newline=True, precision=0, name=None) Class to display the progress of for-loop iterators. Parameters stderr : bool (default: True) Prints output to sys.stderr if True; uses sys.stdout otherwise. start_newline : bool (default: True) Prepends a new line to the counter, which prevents overwriting counters if multiple counters are printed in succession. precision: int (default: 0) Sets the number of decimal places when displaying the time elapsed in seconds. name : string (default: None) Prepends the specified name before the counter to allow distinguishing between multiple counters. Attributes curr_iter : int The current iteration. start_time : float The system's time in seconds when the Counter was initialized. end_time : float The system's time in seconds when the Counter was last updated. Examples >>> cnt = Counter() >>> for i in range(20): ... # do some computation ... time.sleep(0.1) ... cnt.update() 20 iter | 2 sec >>> print('The counter was initialized.' ' %d seconds ago.' % (time.time() - cnt.start_time)) The counter was initialized 2 seconds ago >>> print('The counter was last updated' ' %d seconds ago.' % (time.time() - cnt.end_time)) The counter was last updated 0 seconds ago. For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/utils/Counter/","title":"Counter"},{"location":"api_modules/mlxtend.utils/Counter/#methods","text":"update() Print current iteration and time elapsed.","title":"Methods"},{"location":"api_modules/mlxtend.utils/assert_raises/","text":"assert_raises assert_raises(exception_type, message, func, args, * kwargs) Check that an exception is raised with a specific message Parameters exception_type : exception The exception that should be raised message : str (default: None) The error message that should be raised. Ignored if False or None. func : callable The function that raises the exception *args : positional arguments to func . **kwargs : keyword arguments to func","title":"Assert raises"},{"location":"api_modules/mlxtend.utils/assert_raises/#assert_raises","text":"assert_raises(exception_type, message, func, args, * kwargs) Check that an exception is raised with a specific message Parameters exception_type : exception The exception that should be raised message : str (default: None) The error message that should be raised. Ignored if False or None. func : callable The function that raises the exception *args : positional arguments to func . **kwargs : keyword arguments to func","title":"assert_raises"},{"location":"api_modules/mlxtend.utils/check_Xy/","text":"check_Xy check_Xy(X, y, y_int=True) None","title":"check Xy"},{"location":"api_modules/mlxtend.utils/check_Xy/#check_xy","text":"check_Xy(X, y, y_int=True) None","title":"check_Xy"},{"location":"api_modules/mlxtend.utils/format_kwarg_dictionaries/","text":"format_kwarg_dictionaries format_kwarg_dictionaries(default_kwargs=None, user_kwargs=None, protected_keys=None) Function to combine default and user specified kwargs dictionaries Parameters default_kwargs : dict, optional Default kwargs (default is None). user_kwargs : dict, optional User specified kwargs (default is None). protected_keys : array_like, optional Sequence of keys to be removed from the returned dictionary (default is None). Returns formatted_kwargs : dict Formatted kwargs dictionary.","title":"Format kwarg dictionaries"},{"location":"api_modules/mlxtend.utils/format_kwarg_dictionaries/#format_kwarg_dictionaries","text":"format_kwarg_dictionaries(default_kwargs=None, user_kwargs=None, protected_keys=None) Function to combine default and user specified kwargs dictionaries Parameters default_kwargs : dict, optional Default kwargs (default is None). user_kwargs : dict, optional User specified kwargs (default is None). protected_keys : array_like, optional Sequence of keys to be removed from the returned dictionary (default is None). Returns formatted_kwargs : dict Formatted kwargs dictionary.","title":"format_kwarg_dictionaries"},{"location":"api_subpackages/mlxtend._base/","text":"mlxtend version: 0.23.1","title":"Mlxtend. base"},{"location":"api_subpackages/mlxtend.classifier/","text":"mlxtend version: 0.23.1 Adaline Adaline(eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0) ADAptive LInear NEuron classifier. Note that this implementation of Adaline expects binary class labels in {0, 1}. Parameters eta : float (default: 0.01) solver rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. minibatches : int (default: None) The number of minibatches for gradient-based optimization. If None: Normal Equations (closed-form solution) If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent (SGD) online learning If 1 < minibatches < len(y): SGD Minibatch learning random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr if not solver='normal equation' 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list Sum of squared errors after each epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/Adaline/ Methods fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause EnsembleVoteClassifier EnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0, use_clones=True, fit_base_estimators=True) Soft Voting/Majority Rule classifier for scikit-learn estimators. Parameters clfs : array-like, shape = [n_classifiers] A list of classifiers. Invoking the fit method on the VotingClassifier will fit clones of those original classifiers be stored in the class attribute if use_clones=True (default) and fit_base_estimators=True (default). voting : str, {'hard', 'soft'} (default='hard') If 'hard', uses predicted class labels for majority rule voting. Else if 'soft', predicts the class label based on the argmax of the sums of the predicted probalities, which is recommended for an ensemble of well-calibrated classifiers. weights : array-like, shape = [n_classifiers], optional (default= None ) Sequence of weights ( float or int ) to weight the occurances of predicted class labels ( hard voting) or class probabilities before averaging ( soft voting). Uses uniform weights if None . verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the clf being fitted - verbose=2 : Prints info about the parameters of the clf being fitted - verbose>2 : Changes verbose param of the underlying clf to self.verbose - 2 use_clones : bool (default: True) Clones the classifiers for stacking classification if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Hence, if use_clones=True, the original input classifiers will remain unmodified upon using the StackingClassifier's fit method. Setting use_clones=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. fit_base_estimators : bool (default: True) Refits classifiers in clfs if True; uses references to the clfs , otherwise (assumes that the classifiers were already fit). Note: fit_base_estimators=False will enforce use_clones to be False, and is incompatible to most scikit-learn wrappers! For instance, if any form of cross-validation is performed this would require the re-fitting classifiers to training folds, which would raise a NotFitterError if fit_base_estimators=False. (New in mlxtend v0.6.) Attributes classes_ : array-like, shape = [n_predictions] clf : array-like, shape = [n_predictions] The input classifiers; may be overwritten if use_clones=False clf_ : array-like, shape = [n_predictions] Fitted input classifiers; clones if use_clones=True Examples ``` >>> import numpy as np >>> from sklearn.linear_model import LogisticRegression >>> from sklearn.naive_bayes import GaussianNB >>> from sklearn.ensemble import RandomForestClassifier >>> from mlxtend.sklearn import EnsembleVoteClassifier >>> clf1 = LogisticRegression(random_seed=1) >>> clf2 = RandomForestClassifier(random_seed=1) >>> clf3 = GaussianNB() >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> y = np.array([1, 1, 1, 2, 2, 2]) >>> eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], ... voting='hard', verbose=1) >>> eclf1 = eclf1.fit(X, y) >>> print(eclf1.predict(X)) [1 1 1 2 2 2] >>> eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft') >>> eclf2 = eclf2.fit(X, y) >>> print(eclf2.predict(X)) [1 1 1 2 2 2] >>> eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], ... voting='soft', weights=[2,1,1]) >>> eclf3 = eclf3.fit(X, y) >>> print(eclf3.predict(X)) [1 1 1 2 2 2] >>> For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/ ``` Methods fit(X, y, sample_weight=None) Learn weight coefficients from training data for each classifier. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict class labels for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns maj : array-like, shape = [n_samples] Predicted class labels. predict_proba(X) Predict class probabilities for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns avg : array-like, shape = [n_samples, n_classes] Weighted average probability for each class per sample. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_fit_request(self: mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. set_score_request(self: mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object. transform(X) Return class labels or probabilities for X for each estimator. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns If voting='soft'`` : array-like = [n_classifiers, n_samples, n_classes] Class probabilties calculated by each classifier. If voting='hard'`` : array-like = [n_classifiers, n_samples] Class labels predicted by each classifier. LogisticRegression LogisticRegression(eta=0.01, epochs=50, l2_lambda=0.0, minibatches=1, random_seed=None, print_progress=0) Logistic regression classifier. Note that this implementation of Logistic Regression expects binary class labels in {0, 1}. Parameters eta : float (default: 0.01) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. l2_lambda : float Regularization parameter for L2 regularization. No regularization if l2_lambda=0.0. minibatches : int (default: 1) The number of minibatches for gradient-based optimization. If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent (SGD) online learning If 1 < minibatches < len(y): SGD Minibatch learning random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list List of floats with cross_entropy cost (sgd or gd) for every epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/LogisticRegression/ Methods fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. predict_proba(X) Predict class probabilities of X from the net input. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns Class 1 probability : float score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause MultiLayerPerceptron MultiLayerPerceptron(eta=0.5, epochs=50, hidden_layers=[50], n_classes=None, momentum=0.0, l1=0.0, l2=0.0, dropout=1.0, decrease_const=0.0, minibatches=1, random_seed=None, print_progress=0) Multi-layer perceptron classifier with logistic sigmoid activations Parameters eta : float (default: 0.5) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. hidden_layers : list (default: [50]) Number of units per hidden layer. By default 50 units in the first hidden layer. At the moment only 1 hidden layer is supported n_classes : int (default: None) A positive integer to declare the number of class labels if not all class labels are present in a partial training set. Gets the number of class labels automatically if None. l1 : float (default: 0.0) L1 regularization strength l2 : float (default: 0.0) L2 regularization strength momentum : float (default: 0.0) Momentum constant. Factor multiplied with the gradient of the previous epoch t-1 to improve learning speed w(t) := w(t) - (grad(t) + momentum * grad(t-1)) decrease_const : float (default: 0.0) Decrease constant. Shrinks the learning rate after each epoch via eta / (1 + epoch*decrease_const) minibatches : int (default: 1) Divide the training data into k minibatches for accelerated stochastic gradient descent learning. Gradient Descent Learning if minibatches = 1 Stochastic Gradient Descent learning if minibatches = len(y) Minibatch learning if minibatches > 1 random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape=[n_features, n_classes] Weights after fitting. b_ : 1D-array, shape=[n_classes] Bias units after fitting. cost_ : list List of floats; the mean categorical cross entropy cost after each epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/MultiLayerPerceptron/ Methods fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. predict_proba(X) Predict class probabilities of X from the net input. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns Class probabilties : array-like, shape= [n_samples, n_classes] score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause OneRClassifier OneRClassifier(resolve_ties='first') OneR (One Rule) Classifier. Parameters resolve_ties : str (default: 'first') Option for how to resolve ties if two or more features have the same error. Options are - 'first' (default): chooses first feature in the list, i.e., feature with the lower column index. - 'chi-squared': performs a chi-squared test for each feature against the target and selects the feature with the lowest p-value. Attributes self.classes_labels_ : array-like, shape = [n_labels] Array containing the unique class labels found in the training set. self.feature_idx_ : int The index of the rules' feature based on the column in the training set. self.p_value_ : float The p value for a given feature. Only available after calling fit when the OneR attribute resolve_ties = 'chi-squared' is set. self.prediction_dict_ : dict Dictionary containing information about the feature's (self.feature_idx_) rules and total error. E.g., {'total error': 37, 'rules (value: class)': {0: 0, 1: 2}} means the total error is 37, and the rules are \"if feature value == 0 classify as 0\" and \"if feature value == 1 classify as 2\". (And classify as class 1 otherwise.) For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/OneRClassifier/ Methods fit(X, y) Learn rule from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns self : object get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. predict(X) Predict class labels for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns maj : array-like, shape = [n_samples] Predicted class labels. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. set_score_request(self: mlxtend.classifier.oner.OneRClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.oner.OneRClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object. Perceptron Perceptron(eta=0.1, epochs=50, random_seed=None, print_progress=0) Perceptron classifier. Note that this implementation of the Perceptron expects binary class labels in {0, 1}. Parameters eta : float (default: 0.1) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Number of passes over the training dataset. Prior to each epoch, the dataset is shuffled to prevent cycles. random_seed : int Random state for initializing random weights and shuffling. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list Number of misclassifications in every epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/Perceptron/ Methods fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause SoftmaxRegression SoftmaxRegression(eta=0.01, epochs=50, l2=0.0, minibatches=1, n_classes=None, random_seed=None, print_progress=0) Softmax regression classifier. Parameters eta : float (default: 0.01) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. l2 : float Regularization parameter for L2 regularization. No regularization if l2=0.0. minibatches : int (default: 1) The number of minibatches for gradient-based optimization. If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent (SGD) online learning If 1 < minibatches < len(y): SGD Minibatch learning n_classes : int (default: None) A positive integer to declare the number of class labels if not all class labels are present in a partial training set. Gets the number of class labels automatically if None. random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list List of floats, the average cross_entropy for each epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/ Methods fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. predict_proba(X) Predict class probabilities of X from the net input. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns Class probabilties : array-like, shape= [n_samples, n_classes] score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause StackingCVClassifier StackingCVClassifier(classifiers, meta_classifier, use_probas=False, drop_proba_col=None, cv=2, shuffle=True, random_state=None, stratify=True, verbose=0, use_features_in_secondary=False, store_train_meta_features=False, use_clones=True, n_jobs=None, pre_dispatch='2 n_jobs')* A 'Stacking Cross-Validation' classifier for scikit-learn estimators. New in mlxtend v0.4.3 Parameters classifiers : array-like, shape = [n_classifiers] A list of classifiers. Invoking the fit method on the StackingCVClassifer will fit clones of these original classifiers that will be stored in the class attribute self.clfs_ if use_clones=True . meta_classifier : object The meta-classifier to be fitted on the ensemble of classifiers use_probas : bool (default: False) If True, trains meta-classifier based on predicted probabilities instead of class labels. drop_proba_col : string (default: None) Drops extra \"probability\" column in the feature set, because it is redundant: p(y_c) = 1 - p(y_1) + p(y_2) + ... + p(y_{c-1}). This can be useful for meta-classifiers that are sensitive to perfectly collinear features. If 'last', drops last probability column. If 'first', drops first probability column. Only relevant if use_probas=True . cv : int, cross-validation generator or an iterable, optional (default: 2) Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 2-fold cross validation, - integer, to specify the number of folds in a (Stratified)KFold , - An object to be used as a cross-validation generator. - An iterable yielding train, test splits. For integer/None inputs, it will use either a KFold or StratifiedKFold cross validation depending the value of stratify argument. shuffle : bool (default: True) If True, and the cv argument is integer, the training data will be shuffled at fitting stage prior to cross-validation. If the cv argument is a specific cross validation technique, this argument is omitted. random_state : int, RandomState instance or None, optional (default: None) Constrols the randomness of the cv splitter. Used when cv is integer and shuffle=True . New in v0.16.0. stratify : bool (default: True) If True, and the cv argument is integer it will follow a stratified K-Fold cross validation technique. If the cv argument is a specific cross validation technique, this argument is omitted. verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the regressor being fitted and which fold is currently being used for fitting - verbose=2 : Prints info about the parameters of the regressor being fitted - verbose>2 : Changes verbose param of the underlying regressor to self.verbose - 2 use_features_in_secondary : bool (default: False) If True, the meta-classifier will be trained both on the predictions of the original classifiers and the original dataset. If False, the meta-classifier will be trained only on the predictions of the original classifiers. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-classifier stored in the self.train_meta_features_ array, which can be accessed after calling fit . use_clones : bool (default: True) Clones the classifiers for stacking classification if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Hence, if use_clones=True, the original input classifiers will remain unmodified upon using the StackingCVClassifier's fit method. Setting use_clones=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. n_jobs : int or None, optional (default=None) The number of CPUs to use to do the computation. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. for more details. New in v0.16.0. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs - An int, giving the exact number of total jobs that are spawned - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' New in v0.16.0. Attributes clfs_ : list, shape=[n_classifiers] Fitted classifiers (clones of the original classifiers) meta_clf_ : estimator Fitted meta-classifier (clone of the original meta-estimator) train_meta_features : numpy array, shape = [n_samples, n_classifiers] meta-features for training data, where n_samples is the number of samples in training data and n_classifiers is the number of classfiers. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/ Methods decision_function(X) Predict class confidence scores for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns scores : shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes). Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. fit(X, y, groups=None, sample_weight=None) Fit ensemble classifers and the meta-classifier. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : numpy array, shape = [n_samples] Target values. groups : numpy array/None, shape = [n_samples] The group that each sample belongs to. This is used by specific folding strategies such as GroupKFold() sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict target values for X. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns labels : array-like, shape = [n_samples] Predicted class labels. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, n_classifiers] Returns the meta-features for test data. predict_proba(X) Predict class probabilities for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns proba : array-like, shape = [n_samples, n_classes] or a list of n_outputs of such arrays if n_outputs > 1. Probability for each class per sample. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_fit_request(self: mlxtend.classifier.stacking_cv_classification.StackingCVClassifier, , groups: Union[bool, NoneType, str] = ' UNCHANGED ', sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_cv_classification.StackingCVClassifier* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.classifier.stacking_cv_classification.StackingCVClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_cv_classification.StackingCVClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object. Properties named_classifiers None StackingClassifier StackingClassifier(classifiers, meta_classifier, use_probas=False, drop_proba_col=None, average_probas=False, verbose=0, use_features_in_secondary=False, store_train_meta_features=False, use_clones=True, fit_base_estimators=True) A Stacking classifier for scikit-learn estimators for classification. Parameters classifiers : array-like, shape = [n_classifiers] A list of classifiers. Invoking the fit method on the StackingClassifer will fit clones of these original classifiers that will be stored in the class attribute self.clfs_ if use_clones=True (default) and fit_base_estimators=True (default). meta_classifier : object The meta-classifier to be fitted on the ensemble of classifiers use_probas : bool (default: False) If True, trains meta-classifier based on predicted probabilities instead of class labels. drop_proba_col : string (default: None) Drops extra \"probability\" column in the feature set, because it is redundant: p(y_c) = 1 - p(y_1) + p(y_2) + ... + p(y_{c-1}). This can be useful for meta-classifiers that are sensitive to perfectly collinear features. If 'last', drops last probability column. If 'first', drops first probability column. Only relevant if use_probas=True . average_probas : bool (default: False) Averages the probabilities as meta features if True . Only relevant if use_probas=True . verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the regressor being fitted - verbose=2 : Prints info about the parameters of the regressor being fitted - verbose>2 : Changes verbose param of the underlying regressor to self.verbose - 2 use_features_in_secondary : bool (default: False) If True, the meta-classifier will be trained both on the predictions of the original classifiers and the original dataset. If False, the meta-classifier will be trained only on the predictions of the original classifiers. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-classifier stored in the self.train_meta_features_ array, which can be accessed after calling fit . use_clones : bool (default: True) Clones the classifiers for stacking classification if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Hence, if use_clones=True, the original input classifiers will remain unmodified upon using the StackingClassifier's fit method. Setting use_clones=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. fit_base_estimators: bool (default: True) Refits classifiers in classifiers if True; uses references to the classifiers , otherwise (assumes that the classifiers were already fit). Note: fit_base_estimators=False will enforce use_clones to be False, and is incompatible to most scikit-learn wrappers! For instance, if any form of cross-validation is performed this would require the re-fitting classifiers to training folds, which would raise a NotFitterError if fit_base_estimators=False. (New in mlxtend v0.6.) Attributes clfs_ : list, shape=[n_classifiers] Fitted classifiers (clones of the original classifiers) meta_clf_ : estimator Fitted meta-classifier (clone of the original meta-estimator) train_meta_features : numpy array, shape = [n_samples, n_classifiers] meta-features for training data, where n_samples is the number of samples in training data and n_classifiers is the number of classfiers. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/ Methods decision_function(X) Predict class confidence scores for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns scores : shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes). Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. fit(X, y, sample_weight=None) Fit ensemble classifers and the meta-classifier. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] or [n_samples, n_outputs] Target values. sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict target values for X. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns labels : array-like, shape = [n_samples] Predicted class labels. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, n_classifiers] Returns the meta-features for test data. predict_proba(X) Predict class probabilities for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns proba : array-like, shape = [n_samples, n_classes] or a list of n_outputs of such arrays if n_outputs > 1. Probability for each class per sample. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_fit_request(self: mlxtend.classifier.stacking_classification.StackingClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_classification.StackingClassifier* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.classifier.stacking_classification.StackingClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_classification.StackingClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object. Properties named_classifiers None","title":"Mlxtend.classifier"},{"location":"api_subpackages/mlxtend.classifier/#adaline","text":"Adaline(eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0) ADAptive LInear NEuron classifier. Note that this implementation of Adaline expects binary class labels in {0, 1}. Parameters eta : float (default: 0.01) solver rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. minibatches : int (default: None) The number of minibatches for gradient-based optimization. If None: Normal Equations (closed-form solution) If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent (SGD) online learning If 1 < minibatches < len(y): SGD Minibatch learning random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr if not solver='normal equation' 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list Sum of squared errors after each epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/Adaline/","title":"Adaline"},{"location":"api_subpackages/mlxtend.classifier/#methods","text":"fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Methods"},{"location":"api_subpackages/mlxtend.classifier/#ensemblevoteclassifier","text":"EnsembleVoteClassifier(clfs, voting='hard', weights=None, verbose=0, use_clones=True, fit_base_estimators=True) Soft Voting/Majority Rule classifier for scikit-learn estimators. Parameters clfs : array-like, shape = [n_classifiers] A list of classifiers. Invoking the fit method on the VotingClassifier will fit clones of those original classifiers be stored in the class attribute if use_clones=True (default) and fit_base_estimators=True (default). voting : str, {'hard', 'soft'} (default='hard') If 'hard', uses predicted class labels for majority rule voting. Else if 'soft', predicts the class label based on the argmax of the sums of the predicted probalities, which is recommended for an ensemble of well-calibrated classifiers. weights : array-like, shape = [n_classifiers], optional (default= None ) Sequence of weights ( float or int ) to weight the occurances of predicted class labels ( hard voting) or class probabilities before averaging ( soft voting). Uses uniform weights if None . verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the clf being fitted - verbose=2 : Prints info about the parameters of the clf being fitted - verbose>2 : Changes verbose param of the underlying clf to self.verbose - 2 use_clones : bool (default: True) Clones the classifiers for stacking classification if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Hence, if use_clones=True, the original input classifiers will remain unmodified upon using the StackingClassifier's fit method. Setting use_clones=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. fit_base_estimators : bool (default: True) Refits classifiers in clfs if True; uses references to the clfs , otherwise (assumes that the classifiers were already fit). Note: fit_base_estimators=False will enforce use_clones to be False, and is incompatible to most scikit-learn wrappers! For instance, if any form of cross-validation is performed this would require the re-fitting classifiers to training folds, which would raise a NotFitterError if fit_base_estimators=False. (New in mlxtend v0.6.) Attributes classes_ : array-like, shape = [n_predictions] clf : array-like, shape = [n_predictions] The input classifiers; may be overwritten if use_clones=False clf_ : array-like, shape = [n_predictions] Fitted input classifiers; clones if use_clones=True Examples ``` >>> import numpy as np >>> from sklearn.linear_model import LogisticRegression >>> from sklearn.naive_bayes import GaussianNB >>> from sklearn.ensemble import RandomForestClassifier >>> from mlxtend.sklearn import EnsembleVoteClassifier >>> clf1 = LogisticRegression(random_seed=1) >>> clf2 = RandomForestClassifier(random_seed=1) >>> clf3 = GaussianNB() >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> y = np.array([1, 1, 1, 2, 2, 2]) >>> eclf1 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], ... voting='hard', verbose=1) >>> eclf1 = eclf1.fit(X, y) >>> print(eclf1.predict(X)) [1 1 1 2 2 2] >>> eclf2 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], voting='soft') >>> eclf2 = eclf2.fit(X, y) >>> print(eclf2.predict(X)) [1 1 1 2 2 2] >>> eclf3 = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], ... voting='soft', weights=[2,1,1]) >>> eclf3 = eclf3.fit(X, y) >>> print(eclf3.predict(X)) [1 1 1 2 2 2] >>> For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/ ```","title":"EnsembleVoteClassifier"},{"location":"api_subpackages/mlxtend.classifier/#methods_1","text":"fit(X, y, sample_weight=None) Learn weight coefficients from training data for each classifier. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict class labels for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns maj : array-like, shape = [n_samples] Predicted class labels. predict_proba(X) Predict class probabilities for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns avg : array-like, shape = [n_samples, n_classes] Weighted average probability for each class per sample. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_fit_request(self: mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. set_score_request(self: mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.ensemble_vote.EnsembleVoteClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object. transform(X) Return class labels or probabilities for X for each estimator. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns If voting='soft'`` : array-like = [n_classifiers, n_samples, n_classes] Class probabilties calculated by each classifier. If voting='hard'`` : array-like = [n_classifiers, n_samples] Class labels predicted by each classifier.","title":"Methods"},{"location":"api_subpackages/mlxtend.classifier/#logisticregression","text":"LogisticRegression(eta=0.01, epochs=50, l2_lambda=0.0, minibatches=1, random_seed=None, print_progress=0) Logistic regression classifier. Note that this implementation of Logistic Regression expects binary class labels in {0, 1}. Parameters eta : float (default: 0.01) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. l2_lambda : float Regularization parameter for L2 regularization. No regularization if l2_lambda=0.0. minibatches : int (default: 1) The number of minibatches for gradient-based optimization. If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent (SGD) online learning If 1 < minibatches < len(y): SGD Minibatch learning random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list List of floats with cross_entropy cost (sgd or gd) for every epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/LogisticRegression/","title":"LogisticRegression"},{"location":"api_subpackages/mlxtend.classifier/#methods_2","text":"fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. predict_proba(X) Predict class probabilities of X from the net input. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns Class 1 probability : float score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Methods"},{"location":"api_subpackages/mlxtend.classifier/#multilayerperceptron","text":"MultiLayerPerceptron(eta=0.5, epochs=50, hidden_layers=[50], n_classes=None, momentum=0.0, l1=0.0, l2=0.0, dropout=1.0, decrease_const=0.0, minibatches=1, random_seed=None, print_progress=0) Multi-layer perceptron classifier with logistic sigmoid activations Parameters eta : float (default: 0.5) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. hidden_layers : list (default: [50]) Number of units per hidden layer. By default 50 units in the first hidden layer. At the moment only 1 hidden layer is supported n_classes : int (default: None) A positive integer to declare the number of class labels if not all class labels are present in a partial training set. Gets the number of class labels automatically if None. l1 : float (default: 0.0) L1 regularization strength l2 : float (default: 0.0) L2 regularization strength momentum : float (default: 0.0) Momentum constant. Factor multiplied with the gradient of the previous epoch t-1 to improve learning speed w(t) := w(t) - (grad(t) + momentum * grad(t-1)) decrease_const : float (default: 0.0) Decrease constant. Shrinks the learning rate after each epoch via eta / (1 + epoch*decrease_const) minibatches : int (default: 1) Divide the training data into k minibatches for accelerated stochastic gradient descent learning. Gradient Descent Learning if minibatches = 1 Stochastic Gradient Descent learning if minibatches = len(y) Minibatch learning if minibatches > 1 random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape=[n_features, n_classes] Weights after fitting. b_ : 1D-array, shape=[n_classes] Bias units after fitting. cost_ : list List of floats; the mean categorical cross entropy cost after each epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/MultiLayerPerceptron/","title":"MultiLayerPerceptron"},{"location":"api_subpackages/mlxtend.classifier/#methods_3","text":"fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. predict_proba(X) Predict class probabilities of X from the net input. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns Class probabilties : array-like, shape= [n_samples, n_classes] score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Methods"},{"location":"api_subpackages/mlxtend.classifier/#onerclassifier","text":"OneRClassifier(resolve_ties='first') OneR (One Rule) Classifier. Parameters resolve_ties : str (default: 'first') Option for how to resolve ties if two or more features have the same error. Options are - 'first' (default): chooses first feature in the list, i.e., feature with the lower column index. - 'chi-squared': performs a chi-squared test for each feature against the target and selects the feature with the lowest p-value. Attributes self.classes_labels_ : array-like, shape = [n_labels] Array containing the unique class labels found in the training set. self.feature_idx_ : int The index of the rules' feature based on the column in the training set. self.p_value_ : float The p value for a given feature. Only available after calling fit when the OneR attribute resolve_ties = 'chi-squared' is set. self.prediction_dict_ : dict Dictionary containing information about the feature's (self.feature_idx_) rules and total error. E.g., {'total error': 37, 'rules (value: class)': {0: 0, 1: 2}} means the total error is 37, and the rules are \"if feature value == 0 classify as 0\" and \"if feature value == 1 classify as 2\". (And classify as class 1 otherwise.) For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/OneRClassifier/","title":"OneRClassifier"},{"location":"api_subpackages/mlxtend.classifier/#methods_4","text":"fit(X, y) Learn rule from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. Returns self : object get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. predict(X) Predict class labels for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns maj : array-like, shape = [n_samples] Predicted class labels. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. set_score_request(self: mlxtend.classifier.oner.OneRClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.oner.OneRClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object.","title":"Methods"},{"location":"api_subpackages/mlxtend.classifier/#perceptron","text":"Perceptron(eta=0.1, epochs=50, random_seed=None, print_progress=0) Perceptron classifier. Note that this implementation of the Perceptron expects binary class labels in {0, 1}. Parameters eta : float (default: 0.1) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Number of passes over the training dataset. Prior to each epoch, the dataset is shuffled to prevent cycles. random_seed : int Random state for initializing random weights and shuffling. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list Number of misclassifications in every epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/Perceptron/","title":"Perceptron"},{"location":"api_subpackages/mlxtend.classifier/#methods_5","text":"fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Methods"},{"location":"api_subpackages/mlxtend.classifier/#softmaxregression","text":"SoftmaxRegression(eta=0.01, epochs=50, l2=0.0, minibatches=1, n_classes=None, random_seed=None, print_progress=0) Softmax regression classifier. Parameters eta : float (default: 0.01) Learning rate (between 0.0 and 1.0) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. l2 : float Regularization parameter for L2 regularization. No regularization if l2=0.0. minibatches : int (default: 1) The number of minibatches for gradient-based optimization. If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent (SGD) online learning If 1 < minibatches < len(y): SGD Minibatch learning n_classes : int (default: None) A positive integer to declare the number of class labels if not all class labels are present in a partial training set. Gets the number of class labels automatically if None. random_seed : int (default: None) Set random state for shuffling and initializing the weights. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list List of floats, the average cross_entropy for each epoch. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/SoftmaxRegression/","title":"SoftmaxRegression"},{"location":"api_subpackages/mlxtend.classifier/#methods_6","text":"fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. predict_proba(X) Predict class probabilities of X from the net input. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns Class probabilties : array-like, shape= [n_samples, n_classes] score(X, y) Compute the prediction accuracy Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values (true class labels). Returns acc : float The prediction accuracy as a float between 0.0 and 1.0 (perfect score). set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Methods"},{"location":"api_subpackages/mlxtend.classifier/#stackingcvclassifier","text":"StackingCVClassifier(classifiers, meta_classifier, use_probas=False, drop_proba_col=None, cv=2, shuffle=True, random_state=None, stratify=True, verbose=0, use_features_in_secondary=False, store_train_meta_features=False, use_clones=True, n_jobs=None, pre_dispatch='2 n_jobs')* A 'Stacking Cross-Validation' classifier for scikit-learn estimators. New in mlxtend v0.4.3 Parameters classifiers : array-like, shape = [n_classifiers] A list of classifiers. Invoking the fit method on the StackingCVClassifer will fit clones of these original classifiers that will be stored in the class attribute self.clfs_ if use_clones=True . meta_classifier : object The meta-classifier to be fitted on the ensemble of classifiers use_probas : bool (default: False) If True, trains meta-classifier based on predicted probabilities instead of class labels. drop_proba_col : string (default: None) Drops extra \"probability\" column in the feature set, because it is redundant: p(y_c) = 1 - p(y_1) + p(y_2) + ... + p(y_{c-1}). This can be useful for meta-classifiers that are sensitive to perfectly collinear features. If 'last', drops last probability column. If 'first', drops first probability column. Only relevant if use_probas=True . cv : int, cross-validation generator or an iterable, optional (default: 2) Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 2-fold cross validation, - integer, to specify the number of folds in a (Stratified)KFold , - An object to be used as a cross-validation generator. - An iterable yielding train, test splits. For integer/None inputs, it will use either a KFold or StratifiedKFold cross validation depending the value of stratify argument. shuffle : bool (default: True) If True, and the cv argument is integer, the training data will be shuffled at fitting stage prior to cross-validation. If the cv argument is a specific cross validation technique, this argument is omitted. random_state : int, RandomState instance or None, optional (default: None) Constrols the randomness of the cv splitter. Used when cv is integer and shuffle=True . New in v0.16.0. stratify : bool (default: True) If True, and the cv argument is integer it will follow a stratified K-Fold cross validation technique. If the cv argument is a specific cross validation technique, this argument is omitted. verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the regressor being fitted and which fold is currently being used for fitting - verbose=2 : Prints info about the parameters of the regressor being fitted - verbose>2 : Changes verbose param of the underlying regressor to self.verbose - 2 use_features_in_secondary : bool (default: False) If True, the meta-classifier will be trained both on the predictions of the original classifiers and the original dataset. If False, the meta-classifier will be trained only on the predictions of the original classifiers. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-classifier stored in the self.train_meta_features_ array, which can be accessed after calling fit . use_clones : bool (default: True) Clones the classifiers for stacking classification if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Hence, if use_clones=True, the original input classifiers will remain unmodified upon using the StackingCVClassifier's fit method. Setting use_clones=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. n_jobs : int or None, optional (default=None) The number of CPUs to use to do the computation. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. for more details. New in v0.16.0. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs - An int, giving the exact number of total jobs that are spawned - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' New in v0.16.0. Attributes clfs_ : list, shape=[n_classifiers] Fitted classifiers (clones of the original classifiers) meta_clf_ : estimator Fitted meta-classifier (clone of the original meta-estimator) train_meta_features : numpy array, shape = [n_samples, n_classifiers] meta-features for training data, where n_samples is the number of samples in training data and n_classifiers is the number of classfiers. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/","title":"StackingCVClassifier"},{"location":"api_subpackages/mlxtend.classifier/#methods_7","text":"decision_function(X) Predict class confidence scores for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns scores : shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes). Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. fit(X, y, groups=None, sample_weight=None) Fit ensemble classifers and the meta-classifier. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : numpy array, shape = [n_samples] Target values. groups : numpy array/None, shape = [n_samples] The group that each sample belongs to. This is used by specific folding strategies such as GroupKFold() sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict target values for X. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns labels : array-like, shape = [n_samples] Predicted class labels. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, n_classifiers] Returns the meta-features for test data. predict_proba(X) Predict class probabilities for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns proba : array-like, shape = [n_samples, n_classes] or a list of n_outputs of such arrays if n_outputs > 1. Probability for each class per sample. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_fit_request(self: mlxtend.classifier.stacking_cv_classification.StackingCVClassifier, , groups: Union[bool, NoneType, str] = ' UNCHANGED ', sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_cv_classification.StackingCVClassifier* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.classifier.stacking_cv_classification.StackingCVClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_cv_classification.StackingCVClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object.","title":"Methods"},{"location":"api_subpackages/mlxtend.classifier/#properties","text":"named_classifiers None","title":"Properties"},{"location":"api_subpackages/mlxtend.classifier/#stackingclassifier","text":"StackingClassifier(classifiers, meta_classifier, use_probas=False, drop_proba_col=None, average_probas=False, verbose=0, use_features_in_secondary=False, store_train_meta_features=False, use_clones=True, fit_base_estimators=True) A Stacking classifier for scikit-learn estimators for classification. Parameters classifiers : array-like, shape = [n_classifiers] A list of classifiers. Invoking the fit method on the StackingClassifer will fit clones of these original classifiers that will be stored in the class attribute self.clfs_ if use_clones=True (default) and fit_base_estimators=True (default). meta_classifier : object The meta-classifier to be fitted on the ensemble of classifiers use_probas : bool (default: False) If True, trains meta-classifier based on predicted probabilities instead of class labels. drop_proba_col : string (default: None) Drops extra \"probability\" column in the feature set, because it is redundant: p(y_c) = 1 - p(y_1) + p(y_2) + ... + p(y_{c-1}). This can be useful for meta-classifiers that are sensitive to perfectly collinear features. If 'last', drops last probability column. If 'first', drops first probability column. Only relevant if use_probas=True . average_probas : bool (default: False) Averages the probabilities as meta features if True . Only relevant if use_probas=True . verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the regressor being fitted - verbose=2 : Prints info about the parameters of the regressor being fitted - verbose>2 : Changes verbose param of the underlying regressor to self.verbose - 2 use_features_in_secondary : bool (default: False) If True, the meta-classifier will be trained both on the predictions of the original classifiers and the original dataset. If False, the meta-classifier will be trained only on the predictions of the original classifiers. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-classifier stored in the self.train_meta_features_ array, which can be accessed after calling fit . use_clones : bool (default: True) Clones the classifiers for stacking classification if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Hence, if use_clones=True, the original input classifiers will remain unmodified upon using the StackingClassifier's fit method. Setting use_clones=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. fit_base_estimators: bool (default: True) Refits classifiers in classifiers if True; uses references to the classifiers , otherwise (assumes that the classifiers were already fit). Note: fit_base_estimators=False will enforce use_clones to be False, and is incompatible to most scikit-learn wrappers! For instance, if any form of cross-validation is performed this would require the re-fitting classifiers to training folds, which would raise a NotFitterError if fit_base_estimators=False. (New in mlxtend v0.6.) Attributes clfs_ : list, shape=[n_classifiers] Fitted classifiers (clones of the original classifiers) meta_clf_ : estimator Fitted meta-classifier (clone of the original meta-estimator) train_meta_features : numpy array, shape = [n_samples, n_classifiers] meta-features for training data, where n_samples is the number of samples in training data and n_classifiers is the number of classfiers. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/","title":"StackingClassifier"},{"location":"api_subpackages/mlxtend.classifier/#methods_8","text":"decision_function(X) Predict class confidence scores for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns scores : shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes). Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. fit(X, y, sample_weight=None) Fit ensemble classifers and the meta-classifier. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] or [n_samples, n_outputs] Target values. sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict target values for X. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns labels : array-like, shape = [n_samples] Predicted class labels. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, n_classifiers] Returns the meta-features for test data. predict_proba(X) Predict class probabilities for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns proba : array-like, shape = [n_samples, n_classes] or a list of n_outputs of such arrays if n_outputs > 1. Probability for each class per sample. score(X, y, sample_weight=None) Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float Mean accuracy of self.predict(X) w.r.t. y . set_fit_request(self: mlxtend.classifier.stacking_classification.StackingClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_classification.StackingClassifier* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.classifier.stacking_classification.StackingClassifier, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.classifier.stacking_classification.StackingClassifier* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object.","title":"Methods"},{"location":"api_subpackages/mlxtend.classifier/#properties_1","text":"named_classifiers None","title":"Properties"},{"location":"api_subpackages/mlxtend.cluster/","text":"mlxtend version: 0.23.1 Kmeans Kmeans(k, max_iter=10, convergence_tolerance=1e-05, random_seed=None, print_progress=0) K-means clustering class. Added in 0.4.1dev Parameters k : int Number of clusters max_iter : int (default: 10) Number of iterations during cluster assignment. Cluster re-assignment stops automatically when the algorithm converged. convergence_tolerance : float (default: 1e-05) Compares current centroids with centroids of the previous iteration using the given tolerance (a small positive float)to determine if the algorithm converged early. random_seed : int (default: None) Set random state for the initial centroid assignment. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Iterations elapsed 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes centroids_ : 2d-array, shape={k, n_features} Feature values of the k cluster centroids. custers_ : dictionary The cluster assignments stored as a Python dictionary; the dictionary keys denote the cluster indeces and the items are Python lists of the sample indices that were assigned to each cluster. iterations_ : int Number of iterations until convergence. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/Kmeans/ Methods fit(X, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Mlxtend.cluster"},{"location":"api_subpackages/mlxtend.cluster/#kmeans","text":"Kmeans(k, max_iter=10, convergence_tolerance=1e-05, random_seed=None, print_progress=0) K-means clustering class. Added in 0.4.1dev Parameters k : int Number of clusters max_iter : int (default: 10) Number of iterations during cluster assignment. Cluster re-assignment stops automatically when the algorithm converged. convergence_tolerance : float (default: 1e-05) Compares current centroids with centroids of the previous iteration using the given tolerance (a small positive float)to determine if the algorithm converged early. random_seed : int (default: None) Set random state for the initial centroid assignment. print_progress : int (default: 0) Prints progress in fitting to stderr. 0: No output 1: Iterations elapsed 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes centroids_ : 2d-array, shape={k, n_features} Feature values of the k cluster centroids. custers_ : dictionary The cluster assignments stored as a Python dictionary; the dictionary keys denote the cluster indeces and the items are Python lists of the sample indices that were assigned to each cluster. iterations_ : int Number of iterations until convergence. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/classifier/Kmeans/","title":"Kmeans"},{"location":"api_subpackages/mlxtend.cluster/#methods","text":"fit(X, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Methods"},{"location":"api_subpackages/mlxtend.data/","text":"mlxtend version: 0.23.1 autompg_data autompg_data() Auto MPG dataset. Source : https://archive.ics.uci.edu/ml/datasets/Auto+MPG Number of samples : 392 Continuous target variable : mpg Dataset Attributes: 1) cylinders: multi-valued discrete 2) displacement: continuous 3) horsepower: continuous 4) weight: continuous 5) acceleration: continuous 6) model year: multi-valued discrete 7) origin: multi-valued discrete 8) car name: string (unique for each instance) Returns X, y : [n_samples, n_features], [n_targets] X is the feature matrix with 392 auto samples as rows and 8 feature columns (6 rows with NaNs removed). y is a 1-dimensional array of the target MPG values. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/autompg_data/ boston_housing_data boston_housing_data() Boston Housing dataset. Source : https://archive.ics.uci.edu/ml/datasets/Housing Number of samples : 506 Continuous target variable : MEDV MEDV = Median value of owner-occupied homes in $1000's Dataset Attributes: 1) CRIM per capita crime rate by town 2) ZN proportion of residential land zoned for lots over 25,000 sq.ft. 3) INDUS proportion of non-retail business acres per town 4) CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) 5) NOX nitric oxides concentration (parts per 10 million) 6) RM average number of rooms per dwelling 7) AGE proportion of owner-occupied units built prior to 1940 8) DIS weighted distances to five Boston employment centres 9) RAD index of accessibility to radial highways 10) TAX full-value property-tax rate per $10,000 11) PTRATIO pupil-teacher ratio by town 12) B 1000(Bk - 0.63)^2 where Bk is the prop. of b. by town 13) LSTAT % lower status of the population Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 506 housing samples as rows and 13 feature columns. y is a 1-dimensional array of the continuous target variable MEDV Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/boston_housing_data/ iris_data iris_data(version='uci') Iris flower dataset. Source : https://archive.ics.uci.edu/ml/datasets/Iris Number of samples : 150 Class labels : {0, 1, 2}, distribution: [50, 50, 50] 0 = setosa, 1 = versicolor, 2 = virginica. Dataset Attributes: 1) sepal length [cm] 2) sepal width [cm] 3) petal length [cm] 4) petal width [cm] Parameters version : string, optional (default: 'uci'). Version to use {'uci', 'corrected'}. 'uci' loads the dataset as deposited on the UCI machine learning repository, and 'corrected' provides the version that is consistent with Fisher's original paper. See Note for details. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 150 flower samples as rows, and 4 feature columns sepal length, sepal width, petal length, and petal width. y is a 1-dimensional array of the class labels {0, 1, 2} Note The Iris dataset (originally collected by Edgar Anderson) and available in UCI's machine learning repository is different from the Iris dataset described in the original paper by R.A. Fisher [1]). Precisely, there are two data points (row number 34 and 37) in UCI's Machine Learning repository are different from the origianlly published Iris dataset. Also, the original version of the Iris Dataset, which can be loaded via version='corrected' is the same as the one in R. [1] . A. Fisher (1936). \"The use of multiple measurements in taxonomic problems\". Annals of Eugenics. 7 (2): 179\u2013188 Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/iris_data/ loadlocal_mnist loadlocal_mnist(images_path, labels_path) Read MNIST from ubyte files. Parameters images_path : str path to the test or train MNIST ubyte file labels_path : str path to the test or train MNIST class labels file Returns images : [n_samples, n_pixels] numpy.array Pixel values of the images. labels : [n_samples] numpy array Target class labels Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/loadlocal_mnist/ make_multiplexer_dataset make_multiplexer_dataset(address_bits=2, sample_size=100, positive_class_ratio=0.5, shuffle=False, random_seed=None) Function to create a binary n-bit multiplexer dataset. New in mlxtend v0.9 Parameters address_bits : int (default: 2) A positive integer that determines the number of address bits in the multiplexer, which in turn determine the n-bit capacity of the multiplexer and therefore the number of features. The number of features is determined by the number of address bits. For example, 2 address bits will result in a 6 bit multiplexer and consequently 6 features (2 + 2^2 = 6). If address_bits=3 , then this results in an 11-bit multiplexer as (2 + 2^3 = 11) with 11 features. sample_size : int (default: 100) The total number of samples generated. positive_class_ratio : float (default: 0.5) The fraction (a float between 0 and 1) of samples in the sample_size d dataset that have class label 1. If positive_class_ratio=0.5 (default), then the ratio of class 0 and class 1 samples is perfectly balanced. shuffle : Bool (default: False) Whether or not to shuffle the features and labels. If False (default), the samples are returned in sorted order starting with sample_size /2 samples with class label 0 and followed by sample_size /2 samples with class label 1. random_seed : int (default: None) Random seed used for generating the multiplexer samples and shuffling. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with the number of samples equal to sample_size . The number of features is determined by the number of address bits. For instance, 2 address bits will result in a 6 bit multiplexer and consequently 6 features (2 + 2^2 = 6). All features are binary (values in {0, 1}). y is a 1-dimensional array of class labels in {0, 1}. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/make_multiplexer_dataset mnist_data mnist_data() 5000 samples from the MNIST handwritten digits dataset. Data Source : https://yann.lecun.com/exdb/mnist/ Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 5000 image samples as rows, each row consists of 28x28 pixels that were unrolled into 784 pixel feature vectors. y contains the 10 unique class labels 0-9. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/mnist_data/ three_blobs_data three_blobs_data() A random dataset of 3 2D blobs for clustering. Number of samples : 150 Suggested labels : {0, 1, 2}, distribution: [50, 50, 50] Returns X, y : [n_samples, n_features], [n_cluster_labels] X is the feature matrix with 159 samples as rows and 2 feature columns. y is a 1-dimensional array of the 3 suggested cluster labels 0, 1, 2 Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/three_blobs_data wine_data wine_data() Wine dataset. Source : https://archive.ics.uci.edu/ml/datasets/Wine Number of samples : 178 Class labels : {0, 1, 2}, distribution: [59, 71, 48] Dataset Attributes: 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted wines 13) Proline Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 178 wine samples as rows and 13 feature columns. y is a 1-dimensional array of the 3 class labels 0, 1, 2 Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/wine_data","title":"Mlxtend.data"},{"location":"api_subpackages/mlxtend.data/#autompg_data","text":"autompg_data() Auto MPG dataset. Source : https://archive.ics.uci.edu/ml/datasets/Auto+MPG Number of samples : 392 Continuous target variable : mpg Dataset Attributes: 1) cylinders: multi-valued discrete 2) displacement: continuous 3) horsepower: continuous 4) weight: continuous 5) acceleration: continuous 6) model year: multi-valued discrete 7) origin: multi-valued discrete 8) car name: string (unique for each instance) Returns X, y : [n_samples, n_features], [n_targets] X is the feature matrix with 392 auto samples as rows and 8 feature columns (6 rows with NaNs removed). y is a 1-dimensional array of the target MPG values. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/autompg_data/","title":"autompg_data"},{"location":"api_subpackages/mlxtend.data/#boston_housing_data","text":"boston_housing_data() Boston Housing dataset. Source : https://archive.ics.uci.edu/ml/datasets/Housing Number of samples : 506 Continuous target variable : MEDV MEDV = Median value of owner-occupied homes in $1000's Dataset Attributes: 1) CRIM per capita crime rate by town 2) ZN proportion of residential land zoned for lots over 25,000 sq.ft. 3) INDUS proportion of non-retail business acres per town 4) CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) 5) NOX nitric oxides concentration (parts per 10 million) 6) RM average number of rooms per dwelling 7) AGE proportion of owner-occupied units built prior to 1940 8) DIS weighted distances to five Boston employment centres 9) RAD index of accessibility to radial highways 10) TAX full-value property-tax rate per $10,000 11) PTRATIO pupil-teacher ratio by town 12) B 1000(Bk - 0.63)^2 where Bk is the prop. of b. by town 13) LSTAT % lower status of the population Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 506 housing samples as rows and 13 feature columns. y is a 1-dimensional array of the continuous target variable MEDV Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/boston_housing_data/","title":"boston_housing_data"},{"location":"api_subpackages/mlxtend.data/#iris_data","text":"iris_data(version='uci') Iris flower dataset. Source : https://archive.ics.uci.edu/ml/datasets/Iris Number of samples : 150 Class labels : {0, 1, 2}, distribution: [50, 50, 50] 0 = setosa, 1 = versicolor, 2 = virginica. Dataset Attributes: 1) sepal length [cm] 2) sepal width [cm] 3) petal length [cm] 4) petal width [cm] Parameters version : string, optional (default: 'uci'). Version to use {'uci', 'corrected'}. 'uci' loads the dataset as deposited on the UCI machine learning repository, and 'corrected' provides the version that is consistent with Fisher's original paper. See Note for details. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 150 flower samples as rows, and 4 feature columns sepal length, sepal width, petal length, and petal width. y is a 1-dimensional array of the class labels {0, 1, 2} Note The Iris dataset (originally collected by Edgar Anderson) and available in UCI's machine learning repository is different from the Iris dataset described in the original paper by R.A. Fisher [1]). Precisely, there are two data points (row number 34 and 37) in UCI's Machine Learning repository are different from the origianlly published Iris dataset. Also, the original version of the Iris Dataset, which can be loaded via version='corrected' is the same as the one in R. [1] . A. Fisher (1936). \"The use of multiple measurements in taxonomic problems\". Annals of Eugenics. 7 (2): 179\u2013188 Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/iris_data/","title":"iris_data"},{"location":"api_subpackages/mlxtend.data/#loadlocal_mnist","text":"loadlocal_mnist(images_path, labels_path) Read MNIST from ubyte files. Parameters images_path : str path to the test or train MNIST ubyte file labels_path : str path to the test or train MNIST class labels file Returns images : [n_samples, n_pixels] numpy.array Pixel values of the images. labels : [n_samples] numpy array Target class labels Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/loadlocal_mnist/","title":"loadlocal_mnist"},{"location":"api_subpackages/mlxtend.data/#make_multiplexer_dataset","text":"make_multiplexer_dataset(address_bits=2, sample_size=100, positive_class_ratio=0.5, shuffle=False, random_seed=None) Function to create a binary n-bit multiplexer dataset. New in mlxtend v0.9 Parameters address_bits : int (default: 2) A positive integer that determines the number of address bits in the multiplexer, which in turn determine the n-bit capacity of the multiplexer and therefore the number of features. The number of features is determined by the number of address bits. For example, 2 address bits will result in a 6 bit multiplexer and consequently 6 features (2 + 2^2 = 6). If address_bits=3 , then this results in an 11-bit multiplexer as (2 + 2^3 = 11) with 11 features. sample_size : int (default: 100) The total number of samples generated. positive_class_ratio : float (default: 0.5) The fraction (a float between 0 and 1) of samples in the sample_size d dataset that have class label 1. If positive_class_ratio=0.5 (default), then the ratio of class 0 and class 1 samples is perfectly balanced. shuffle : Bool (default: False) Whether or not to shuffle the features and labels. If False (default), the samples are returned in sorted order starting with sample_size /2 samples with class label 0 and followed by sample_size /2 samples with class label 1. random_seed : int (default: None) Random seed used for generating the multiplexer samples and shuffling. Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with the number of samples equal to sample_size . The number of features is determined by the number of address bits. For instance, 2 address bits will result in a 6 bit multiplexer and consequently 6 features (2 + 2^2 = 6). All features are binary (values in {0, 1}). y is a 1-dimensional array of class labels in {0, 1}. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/make_multiplexer_dataset","title":"make_multiplexer_dataset"},{"location":"api_subpackages/mlxtend.data/#mnist_data","text":"mnist_data() 5000 samples from the MNIST handwritten digits dataset. Data Source : https://yann.lecun.com/exdb/mnist/ Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 5000 image samples as rows, each row consists of 28x28 pixels that were unrolled into 784 pixel feature vectors. y contains the 10 unique class labels 0-9. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/mnist_data/","title":"mnist_data"},{"location":"api_subpackages/mlxtend.data/#three_blobs_data","text":"three_blobs_data() A random dataset of 3 2D blobs for clustering. Number of samples : 150 Suggested labels : {0, 1, 2}, distribution: [50, 50, 50] Returns X, y : [n_samples, n_features], [n_cluster_labels] X is the feature matrix with 159 samples as rows and 2 feature columns. y is a 1-dimensional array of the 3 suggested cluster labels 0, 1, 2 Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/three_blobs_data","title":"three_blobs_data"},{"location":"api_subpackages/mlxtend.data/#wine_data","text":"wine_data() Wine dataset. Source : https://archive.ics.uci.edu/ml/datasets/Wine Number of samples : 178 Class labels : {0, 1, 2}, distribution: [59, 71, 48] Dataset Attributes: 1) Alcohol 2) Malic acid 3) Ash 4) Alcalinity of ash 5) Magnesium 6) Total phenols 7) Flavanoids 8) Nonflavanoid phenols 9) Proanthocyanins 10) Color intensity 11) Hue 12) OD280/OD315 of diluted wines 13) Proline Returns X, y : [n_samples, n_features], [n_class_labels] X is the feature matrix with 178 wine samples as rows and 13 feature columns. y is a 1-dimensional array of the 3 class labels 0, 1, 2 Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/data/wine_data","title":"wine_data"},{"location":"api_subpackages/mlxtend.evaluate/","text":"mlxtend version: 0.23.1 BootstrapOutOfBag BootstrapOutOfBag(n_splits=200, random_seed=None) Parameters n_splits : int (default=200) Number of bootstrap iterations. Must be larger than 1. random_seed : int (default=None) If int, random_seed is the seed used by the random number generator. Returns train_idx : ndarray The training set indices for that split. test_idx : ndarray The testing set indices for that split. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/BootstrapOutOfBag/ Methods get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator Parameters X : object Always ignored, exists for compatibility with scikit-learn. y : object Always ignored, exists for compatibility with scikit-learn. groups : object Always ignored, exists for compatibility with scikit-learn. Returns n_splits : int Returns the number of splitting iterations in the cross-validator. split(X, y=None, groups=None) y : array-like or None (default: None) Argument is not used and only included as parameter for compatibility, similar to KFold in scikit-learn. groups : array-like or None (default: None) Argument is not used and only included as parameter for compatibility, similar to KFold in scikit-learn. GroupTimeSeriesSplit GroupTimeSeriesSplit(test_size, train_size=None, n_splits=None, gap_size=0, shift_size=1, window_type='rolling') Group time series cross-validator. Parameters test_size : int Size of test dataset. train_size : int (default=None) Size of train dataset. n_splits : int (default=None) Number of the splits. gap_size : int (default=0) Gap size between train and test datasets. shift_size : int (default=1) Step to shift for the next fold. window_type : str (default=\"rolling\") Type of the window. Possible values: \"rolling\", \"expanding\". Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/GroupTimeSeriesSplit/ Methods get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator. Parameters X : object Always ignored, exists for compatibility. y : object Always ignored, exists for compatibility. groups : object Always ignored, exists for compatibility. Returns n_splits : int Returns the number of splitting iterations in the cross-validator. split(X, y=None, groups=None) Generate indices to split data into training and test set. Parameters X : array-like Training data. y : array-like (default=None) Always ignored, exists for compatibility. groups : array-like (default=None) Array with group names or sequence numbers. Yields train : ndarray The training set indices for that split. test : ndarray The testing set indices for that split. PredefinedHoldoutSplit PredefinedHoldoutSplit(valid_indices) Train/Validation set splitter for sklearn's GridSearchCV etc. Uses user-specified train/validation set indices to split a dataset into train/validation sets using user-defined or random indices. Parameters valid_indices : array-like, shape (num_examples,) Indices of the training examples in the training set to be used for validation. All other indices in the training set are used to for a training subset for model fitting. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/PredefinedHoldoutSplit/ Methods get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator Parameters X : object Always ignored, exists for compatibility. y : object Always ignored, exists for compatibility. groups : object Always ignored, exists for compatibility. Returns n_splits : 1 Returns the number of splitting iterations in the cross-validator. Always returns 1. split(X, y, groups=None) Generate indices to split data into training and test set. Parameters X : array-like, shape (num_examples, num_features) Training data, where num_examples is the number of examples and num_features is the number of features. y : array-like, shape (num_examples,) The target variable for supervised learning problems. Stratification is done based on the y labels. groups : object Always ignored, exists for compatibility. Yields train_index : ndarray The training set indices for that split. valid_index : ndarray The validation set indices for that split. RandomHoldoutSplit RandomHoldoutSplit(valid_size=0.5, random_seed=None, stratify=False) Train/Validation set splitter for sklearn's GridSearchCV etc. Provides train/validation set indices to split a dataset into train/validation sets using random indices. Parameters valid_size : float (default: 0.5) Proportion of examples that being assigned as validation examples. 1- valid_size will then automatically be assigned as training set examples. random_seed : int (default: None) The random seed for splitting the data into training and validation set partitions. stratify : bool (default: False) True or False, whether to perform a stratified split or not Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/RandomHoldoutSplit/ Methods get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator Parameters X : object Always ignored, exists for compatibility. y : object Always ignored, exists for compatibility. groups : object Always ignored, exists for compatibility. Returns n_splits : 1 Returns the number of splitting iterations in the cross-validator. Always returns 1. split(X, y, groups=None) Generate indices to split data into training and test set. Parameters X : array-like, shape (num_examples, num_features) Training data, where num_examples is the number of training examples and num_features is the number of features. y : array-like, shape (num_examples,) The target variable for supervised learning problems. Stratification is done based on the y labels. groups : object Always ignored, exists for compatibility. Yields train_index : ndarray The training set indices for that split. valid_index : ndarray The validation set indices for that split. accuracy_score accuracy_score(y_target, y_predicted, method='standard', pos_label=1, normalize=True) General accuracy function for supervised learning. Parameters y_target : array-like, shape=[n_values] True class labels or target values. y_predicted : array-like, shape=[n_values] Predicted class labels or target values. method : str, 'standard' by default. The chosen method for accuracy computation. If set to 'standard', computes overall accuracy. If set to 'binary', computes accuracy for class pos_label. If set to 'average', computes average per-class (balanced) accuracy. If set to 'balanced', computes the scikit-learn-style balanced accuracy. pos_label : str or int, 1 by default. The class whose accuracy score is to be reported. Used only when method is set to 'binary' normalize : bool, True by default. If True, returns fraction of correctly classified samples. If False, returns number of correctly classified samples. Returns score: float Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/accuracy_score/ bias_variance_decomp bias_variance_decomp(estimator, X_train, y_train, X_test, y_test, loss='0-1_loss', num_rounds=200, random_seed=None, fit_params) estimator : object A classifier or regressor object or class implementing both a fit and predict method similar to the scikit-learn API. X_train : array-like, shape=(num_examples, num_features) A training dataset for drawing the bootstrap samples to carry out the bias-variance decomposition. y_train : array-like, shape=(num_examples) Targets (class labels, continuous values in case of regression) associated with the X_train examples. X_test : array-like, shape=(num_examples, num_features) The test dataset for computing the average loss, bias, and variance. y_test : array-like, shape=(num_examples) Targets (class labels, continuous values in case of regression) associated with the X_test examples. loss : str (default='0-1_loss') Loss function for performing the bias-variance decomposition. Currently allowed values are '0-1_loss' and 'mse'. num_rounds : int (default=200) Number of bootstrap rounds (sampling from the training set) for performing the bias-variance decomposition. Each bootstrap sample has the same size as the original training set. random_seed : int (default=None) Random seed for the bootstrap sampling used for the bias-variance decomposition. fit_params : additional parameters Additional parameters to be passed to the .fit() function of the estimator when it is fit to the bootstrap samples. Returns avg_expected_loss, avg_bias, avg_var : returns the average expected average bias, and average bias (all floats), where the average is computed over the data points in the test set. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/ bootstrap bootstrap(x, func, num_rounds=1000, ci=0.95, ddof=1, seed=None) Implements the ordinary nonparametric bootstrap Parameters x : NumPy array, shape=(n_samples, [n_columns]) An one or multidimensional array of data records func : A function which computes a statistic that is used to compute the bootstrap replicates (the statistic computed from the bootstrap samples). This function must return a scalar value. For example, np.mean or np.median would be an acceptable argument for func if x is a 1-dimensional array or vector. num_rounds : int (default=1000) The number of bootstrap samples to draw where each bootstrap sample has the same number of records as the original dataset. ci : int (default=0.95) An integer in the range (0, 1) that represents the confidence level for computing the confidence interval. For example, ci=0.95 (default) will compute the 95% confidence interval from the bootstrap replicates. ddof : int The delta degrees of freedom used when computing the standard error. seed : int or None (default=None) Random seed for generating bootstrap samples. Returns original, standard_error, (lower_ci, upper_ci) : tuple Returns the statistic of the original sample ( original ), the standard error of the estimate, and the respective confidence interval bounds. Examples ``` >>> from mlxtend.evaluate import bootstrap >>> rng = np.random.RandomState(123) >>> x = rng.normal(loc=5., size=100) >>> original, std_err, ci_bounds = bootstrap(x, ... num_rounds=1000, ... func=np.mean, ... ci=0.95, ... seed=123) >>> print('Mean: %.2f, SE: +/- %.2f, CI95: [%.2f, %.2f]' % (original, ... std_err, ... ci_bounds[0], ... ci_bounds[1])) Mean: 5.03, SE: +/- 0.11, CI95: [4.80, 5.26] >>> For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap/ ## bootstrap_point632_score *bootstrap_point632_score(estimator, X, y, n_splits=200, method='.632', scoring_func=None, predict_proba=False, random_seed=None, clone_estimator=True, **fit_params)* Implementation of the .632 [1] and .632+ [2] bootstrap for supervised learning References: - [1] Efron, Bradley. 1983. \"Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation.\" Journal of the American Statistical Association 78 (382): 316. doi:10.2307/2288636. - [2] Efron, Bradley, and Robert Tibshirani. 1997. \"Improvements on Cross-Validation: The .632+ Bootstrap Method.\" Journal of the American Statistical Association 92 (438): 548. doi:10.2307/2965703. **Parameters** - `estimator` : object An estimator for classification or regression that follows the scikit-learn API and implements \"fit\" and \"predict\" methods. - `X` : array-like The data to fit. Can be, for example a list, or an array at least 2d. - `y` : array-like, optional, default: None The target variable to try to predict in the case of supervised learning. - `n_splits` : int (default=200) Number of bootstrap iterations. Must be larger than 1. - `method` : str (default='.632') The bootstrap method, which can be either - 1) '.632' bootstrap (default) - 2) '.632+' bootstrap - 3) 'oob' (regular out-of-bag, no weighting) for comparison studies. - `scoring_func` : callable, Score function (or loss function) with signature ``scoring_func(y, y_pred, **kwargs)``. If none, uses classification accuracy if the estimator is a classifier and mean squared error if the estimator is a regressor. - `predict_proba` : bool Whether to use the `predict_proba` function for the `estimator` argument. This is to be used in conjunction with `scoring_func` which takes in probability values instead of actual predictions. For example, if the scoring_func is :meth:`sklearn.metrics.roc_auc_score`, then use `predict_proba=True`. Note that this requires `estimator` to have `predict_proba` method implemented. - `random_seed` : int (default=None) If int, random_seed is the seed used by the random number generator. - `clone_estimator` : bool (default=True) Clones the estimator if true, otherwise fits the original. - `fit_params` : additional parameters Additional parameters to be passed to the .fit() function of the estimator when it is fit to the bootstrap samples. **Returns** - `scores` : array of float, shape=(len(list(n_splits)),) Array of scores of the estimator for each bootstrap replicate. **Examples** >>> from sklearn import datasets, linear_model >>> from mlxtend.evaluate import bootstrap_point632_score >>> iris = datasets.load_iris() >>> X = iris.data >>> y = iris.target >>> lr = linear_model.LogisticRegression() >>> scores = bootstrap_point632_score(lr, X, y) >>> acc = np.mean(scores) >>> print('Accuracy:', acc) 0.953023146884 >>> lower = np.percentile(scores, 2.5) >>> upper = np.percentile(scores, 97.5) >>> print('95%% Confidence interval: [%.2f, %.2f]' % (lower, upper)) 95% Confidence interval: [0.90, 0.98] For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap_point632_score/ ``` cochrans_q cochrans_q(y_target, y_model_predictions)* Cochran's Q test to compare 2 or more models. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. *y_model_predictions : array-likes, shape=[n_samples] Variable number of 2 or more arrays that contain the predicted class labels from models as 1D NumPy array. Returns q, p : float or None, float Returns the Q (chi-squared) value and the p-value Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/cochrans_q/ combined_ftest_5x2cv combined_ftest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None) Implements the 5x2cv combined F test proposed by Alpaydin 1999, to compare the performance of two models. Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. random_seed : int or None (default: None) Random seed for creating the test/train splits. Returns f : float The F-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/combined_ftest_5x2cv/ confusion_matrix confusion_matrix(y_target, y_predicted, binary=False, positive_label=1) Compute a confusion matrix/contingency table. Parameters y_target : array-like, shape=[n_samples] True class labels. y_predicted : array-like, shape=[n_samples] Predicted class labels. binary : bool (default: False) Maps a multi-class problem onto a binary confusion matrix, where the positive class is 1 and all other classes are 0. positive_label : int (default: 1) Class label of the positive class. Returns mat : array-like, shape=[n_classes, n_classes] Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix/ create_counterfactual create_counterfactual(x_reference, y_desired, model, X_dataset, y_desired_proba=None, lammbda=0.1, random_seed=None) Implementation of the counterfactual method by Wachter et al. 2017 References: - Wachter, S., Mittelstadt, B., & Russell, C. (2017). Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harv. JL & Tech., 31, 841., https://arxiv.org/abs/1711.00399 Parameters x_reference : array-like, shape=[m_features] The data instance (training example) to be explained. y_desired : int The desired class label for x_reference . model : estimator A (scikit-learn) estimator implementing .predict() and/or predict_proba() . - If model supports predict_proba() , then this is used by default for the first loss term, (lambda * model.predict[_proba](x_counterfact) - y_desired[_proba])^2 - Otherwise, method will fall back to predict . X_dataset : array-like, shape=[n_examples, m_features] A (training) dataset for picking the initial counterfactual as initial value for starting the optimization procedure. y_desired_proba : float (default: None) A float within the range [0, 1] designating the desired class probability for y_desired . - If y_desired_proba=None (default), the first loss term is (lambda * model(x_counterfact) - y_desired)^2 where y_desired is a class label - If y_desired_proba is not None, the first loss term is (lambda * model(x_counterfact) - y_desired_proba)^2 lammbda : Weighting parameter for the first loss term, (lambda * model(x_counterfact) - y_desired[_proba])^2 random_seed : int (default=None) If int, random_seed is the seed used by the random number generator for selecting the inital counterfactual from X_dataset . feature_importance_permutation feature_importance_permutation(X, y, predict_method, metric, num_rounds=1, feature_groups=None, seed=None) Feature importance imputation via permutation importance Parameters X : NumPy array, shape = [n_samples, n_features] Dataset, where n_samples is the number of samples and n_features is the number of features. y : NumPy array, shape = [n_samples] Target values. predict_method : prediction function A callable function that predicts the target values from X. metric : str, callable The metric for evaluating the feature importance through permutation. By default, the strings 'accuracy' is recommended for classifiers and the string 'r2' is recommended for regressors. Optionally, a custom scoring function (e.g., metric=scoring_func ) that accepts two arguments, y_true and y_pred, which have similar shape to the y array. num_rounds : int (default=1) Number of rounds the feature columns are permuted to compute the permutation importance. feature_groups : list or None (default=None) Optional argument for treating certain features as a group. For example [1, 2, [3, 4, 5]] , which can be useful for interpretability, for example, if features 3, 4, 5 are one-hot encoded features. seed : int or None (default=None) Random seed for permuting the feature columns. Returns mean_importance_vals, all_importance_vals : NumPy arrays. The first array, mean_importance_vals has shape [n_features, ] and contains the importance values for all features. The shape of the second array is [n_features, num_rounds] and contains the feature importance for each repetition. If num_rounds=1, it contains the same values as the first array, mean_importance_vals. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/ ftest ftest(y_target, y_model_predictions)* F-Test test to compare 2 or more models. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. *y_model_predictions : array-likes, shape=[n_samples] Variable number of 2 or more arrays that contain the predicted class labels from models as 1D NumPy array. Returns f, p : float or None, float Returns the F-value and the p-value Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/ftest/ lift_score lift_score(y_target, y_predicted, binary=True, positive_label=1) Lift measures the degree to which the predictions of a classification model are better than randomly-generated predictions. The in terms of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN), the lift score is computed as: [ TP / (TP+FP) ] / [ (TP+FN) / (TP+TN+FP+FN) ] Parameters y_target : array-like, shape=[n_samples] True class labels. y_predicted : array-like, shape=[n_samples] Predicted class labels. binary : bool (default: True) Maps a multi-class problem onto a binary, where the positive class is 1 and all other classes are 0. positive_label : int (default: 0) Class label of the positive class. Returns score : float Lift score in the range [0, infinity] Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/lift_score/ mcnemar mcnemar(ary, corrected=True, exact=False) McNemar test for paired nominal data Parameters ary : array-like, shape=[2, 2] 2 x 2 contigency table (as returned by evaluate.mcnemar_table), where a: ary[0, 0]: # of samples that both models predicted correctly b: ary[0, 1]: # of samples that model 1 got right and model 2 got wrong c: ary[1, 0]: # of samples that model 2 got right and model 1 got wrong d: aryCell [1, 1]: # of samples that both models predicted incorrectly corrected : array-like, shape=[n_samples] (default: True) Uses Edward's continuity correction for chi-squared if True exact : bool, (default: False) If True , uses an exact binomial test comparing b to a binomial distribution with n = b + c and p = 0.5. It is highly recommended to use exact=True for sample sizes < 25 since chi-squared is not well-approximated by the chi-squared distribution! Returns chi2, p : float or None, float Returns the chi-squared value and the p-value; if exact=True (default: False ), chi2 is None Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar/ mcnemar_table mcnemar_table(y_target, y_model1, y_model2) Compute a 2x2 contigency table for McNemar's test. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. y_model1 : array-like, shape=[n_samples] Predicted class labels from model as 1D NumPy array. y_model2 : array-like, shape=[n_samples] Predicted class labels from model 2 as 1D NumPy array. Returns tb : array-like, shape=[2, 2] 2x2 contingency table with the following contents: a: tb[0, 0]: # of samples that both models predicted correctly b: tb[0, 1]: # of samples that model 1 got right and model 2 got wrong c: tb[1, 0]: # of samples that model 2 got right and model 1 got wrong d: tb[1, 1]: # of samples that both models predicted incorrectly Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar_table/ mcnemar_tables mcnemar_tables(y_target, y_model_predictions)* Compute multiple 2x2 contigency tables for McNemar's test or Cochran's Q test. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. y_model_predictions : array-like, shape=[n_samples] Predicted class labels for a model. Returns tables : dict Dictionary of NumPy arrays with shape=[2, 2]. Each dictionary key names the two models to be compared based on the order the models were passed as *y_model_predictions . The number of dictionary entries is equal to the number of pairwise combinations between the m models, i.e., \"m choose 2.\" For example the following target array (containing the true labels) and 3 models y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) y_mod0 = np.array([0, 1, 0, 0, 0, 1, 1, 0, 0, 0]) y_mod1 = np.array([0, 0, 1, 1, 0, 1, 1, 0, 0, 0]) y_mod2 = np.array([0, 1, 1, 1, 0, 1, 0, 0, 0, 0]) would result in the following dictionary: {'model_0 vs model_1': array([[ 4., 1.], [ 2., 3.]]), 'model_0 vs model_2': array([[ 3., 0.], [ 3., 4.]]), 'model_1 vs model_2': array([[ 3., 0.], [ 2., 5.]])} Each array is structured in the following way: tb[0, 0]: # of samples that both models predicted correctly tb[0, 1]: # of samples that model a got right and model b got wrong tb[1, 0]: # of samples that model b got right and model a got wrong tb[1, 1]: # of samples that both models predicted incorrectly Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar_tables/ paired_ttest_5x2cv paired_ttest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None) Implements the 5x2cv paired t test proposed by Dieterrich (1998) to compare the performance of two models. Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. random_seed : int or None (default: None) Random seed for creating the test/train splits. Returns t : float The t-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_5x2cv/ paired_ttest_kfold_cv paired_ttest_kfold_cv(estimator1, estimator2, X, y, cv=10, scoring=None, shuffle=False, random_seed=None) Implements the k-fold paired t test procedure to compare the performance of two models. Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. cv : int (default: 10) Number of splits and iteration for the cross-validation procedure scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. shuffle : bool (default: True) Whether to shuffle the dataset for generating the k-fold splits. random_seed : int or None (default: None) Random seed for shuffling the dataset for generating the k-fold splits. Ignored if shuffle=False. Returns t : float The t-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_kfold_cv/ paired_ttest_resampled paired_ttest_resampled(estimator1, estimator2, X, y, num_rounds=30, test_size=0.3, scoring=None, random_seed=None) Implements the resampled paired t test procedure to compare the performance of two models (also called k-hold-out paired t test). Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. num_rounds : int (default: 30) Number of resampling iterations (i.e., train/test splits) test_size : float or int (default: 0.3) If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to use as a test set. If int, represents the absolute number of test exsamples. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. random_seed : int or None (default: None) Random seed for creating the test/train splits. Returns t : float The t-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_resampled/ permutation_test permutation_test(x, y, func='x_mean != y_mean', method='exact', num_rounds=1000, seed=None, paired=False) Nonparametric permutation test Parameters x : list or numpy array with shape (n_datapoints,) A list or 1D numpy array of the first sample (e.g., the treatment group). y : list or numpy array with shape (n_datapoints,) A list or 1D numpy array of the second sample (e.g., the control group). func : custom function or str (default: 'x_mean != y_mean') function to compute the statistic for the permutation test. - If 'x_mean != y_mean', uses func=lambda x, y: np.abs(np.mean(x) - np.mean(y))) for a two-sided test. - If 'x_mean > y_mean', uses func=lambda x, y: np.mean(x) - np.mean(y)) for a one-sided test. - If 'x_mean < y_mean', uses func=lambda x, y: np.mean(y) - np.mean(x)) for a one-sided test. method : 'approximate' or 'exact' (default: 'exact') If 'exact' (default), all possible permutations are considered. If 'approximate' the number of drawn samples is given by num_rounds . Note that 'exact' is typically not feasible unless the dataset size is relatively small. paired : bool If True, a paired test is performed by only exchanging each datapoint with its associate. num_rounds : int (default: 1000) The number of permutation samples if method='approximate' . seed : int or None (default: None) The random seed for generating permutation samples if method='approximate' . Returns p-value under the null hypothesis Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/permutation_test/ proportion_difference proportion_difference(proportion_1, proportion_2, n_1, n_2=None) Computes the test statistic and p-value for a difference of proportions test. Parameters proportion_1 : float The first proportion proportion_2 : float The second proportion n_1 : int The sample size of the first test sample n_2 : int or None (default=None) The sample size of the second test sample. If None , n_1 = n_2 . Returns z, p : float or None, float Returns the z-score and the p-value Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/proportion_difference/ scoring scoring(y_target, y_predicted, metric='error', positive_label=1, unique_labels='auto') Compute a scoring metric for supervised learning. Parameters y_target : array-like, shape=[n_values] True class labels or target values. y_predicted : array-like, shape=[n_values] Predicted class labels or target values. metric : str (default: 'error') Performance metric: 'accuracy': (TP + TN)/(FP + FN + TP + TN) = 1-ERR 'average per-class accuracy': Average per-class accuracy 'average per-class error': Average per-class error 'balanced per-class accuracy': Average per-class accuracy 'balanced per-class error': Average per-class error 'error': (TP + TN)/(FP+ FN + TP + TN) = 1-ACC 'false_positive_rate': FP/N = FP/(FP + TN) 'true_positive_rate': TP/P = TP/(FN + TP) 'true_negative_rate': TN/N = TN/(FP + TN) 'precision': TP/(TP + FP) 'recall': equal to 'true_positive_rate' 'sensitivity': equal to 'true_positive_rate' or 'recall' 'specificity': equal to 'true_negative_rate' 'f1': 2 * (PRE * REC)/(PRE + REC) 'matthews_corr_coef': (TP TN - FP FN) / (sqrt{(TP + FP)( TP + FN )( TN + FP )( TN + FN )}) Where: [TP: True positives, TN = True negatives, TN: True negatives, FN = False negatives] positive_label : int (default: 1) Label of the positive class for binary classification metrics. unique_labels : str or array-like (default: 'auto') If 'auto', deduces the unique class labels from y_target Returns score : float Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/scoring/","title":"Mlxtend.evaluate"},{"location":"api_subpackages/mlxtend.evaluate/#bootstrapoutofbag","text":"BootstrapOutOfBag(n_splits=200, random_seed=None) Parameters n_splits : int (default=200) Number of bootstrap iterations. Must be larger than 1. random_seed : int (default=None) If int, random_seed is the seed used by the random number generator. Returns train_idx : ndarray The training set indices for that split. test_idx : ndarray The testing set indices for that split. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/BootstrapOutOfBag/","title":"BootstrapOutOfBag"},{"location":"api_subpackages/mlxtend.evaluate/#methods","text":"get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator Parameters X : object Always ignored, exists for compatibility with scikit-learn. y : object Always ignored, exists for compatibility with scikit-learn. groups : object Always ignored, exists for compatibility with scikit-learn. Returns n_splits : int Returns the number of splitting iterations in the cross-validator. split(X, y=None, groups=None) y : array-like or None (default: None) Argument is not used and only included as parameter for compatibility, similar to KFold in scikit-learn. groups : array-like or None (default: None) Argument is not used and only included as parameter for compatibility, similar to KFold in scikit-learn.","title":"Methods"},{"location":"api_subpackages/mlxtend.evaluate/#grouptimeseriessplit","text":"GroupTimeSeriesSplit(test_size, train_size=None, n_splits=None, gap_size=0, shift_size=1, window_type='rolling') Group time series cross-validator. Parameters test_size : int Size of test dataset. train_size : int (default=None) Size of train dataset. n_splits : int (default=None) Number of the splits. gap_size : int (default=0) Gap size between train and test datasets. shift_size : int (default=1) Step to shift for the next fold. window_type : str (default=\"rolling\") Type of the window. Possible values: \"rolling\", \"expanding\". Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/GroupTimeSeriesSplit/","title":"GroupTimeSeriesSplit"},{"location":"api_subpackages/mlxtend.evaluate/#methods_1","text":"get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator. Parameters X : object Always ignored, exists for compatibility. y : object Always ignored, exists for compatibility. groups : object Always ignored, exists for compatibility. Returns n_splits : int Returns the number of splitting iterations in the cross-validator. split(X, y=None, groups=None) Generate indices to split data into training and test set. Parameters X : array-like Training data. y : array-like (default=None) Always ignored, exists for compatibility. groups : array-like (default=None) Array with group names or sequence numbers. Yields train : ndarray The training set indices for that split. test : ndarray The testing set indices for that split.","title":"Methods"},{"location":"api_subpackages/mlxtend.evaluate/#predefinedholdoutsplit","text":"PredefinedHoldoutSplit(valid_indices) Train/Validation set splitter for sklearn's GridSearchCV etc. Uses user-specified train/validation set indices to split a dataset into train/validation sets using user-defined or random indices. Parameters valid_indices : array-like, shape (num_examples,) Indices of the training examples in the training set to be used for validation. All other indices in the training set are used to for a training subset for model fitting. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/PredefinedHoldoutSplit/","title":"PredefinedHoldoutSplit"},{"location":"api_subpackages/mlxtend.evaluate/#methods_2","text":"get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator Parameters X : object Always ignored, exists for compatibility. y : object Always ignored, exists for compatibility. groups : object Always ignored, exists for compatibility. Returns n_splits : 1 Returns the number of splitting iterations in the cross-validator. Always returns 1. split(X, y, groups=None) Generate indices to split data into training and test set. Parameters X : array-like, shape (num_examples, num_features) Training data, where num_examples is the number of examples and num_features is the number of features. y : array-like, shape (num_examples,) The target variable for supervised learning problems. Stratification is done based on the y labels. groups : object Always ignored, exists for compatibility. Yields train_index : ndarray The training set indices for that split. valid_index : ndarray The validation set indices for that split.","title":"Methods"},{"location":"api_subpackages/mlxtend.evaluate/#randomholdoutsplit","text":"RandomHoldoutSplit(valid_size=0.5, random_seed=None, stratify=False) Train/Validation set splitter for sklearn's GridSearchCV etc. Provides train/validation set indices to split a dataset into train/validation sets using random indices. Parameters valid_size : float (default: 0.5) Proportion of examples that being assigned as validation examples. 1- valid_size will then automatically be assigned as training set examples. random_seed : int (default: None) The random seed for splitting the data into training and validation set partitions. stratify : bool (default: False) True or False, whether to perform a stratified split or not Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/RandomHoldoutSplit/","title":"RandomHoldoutSplit"},{"location":"api_subpackages/mlxtend.evaluate/#methods_3","text":"get_n_splits(X=None, y=None, groups=None) Returns the number of splitting iterations in the cross-validator Parameters X : object Always ignored, exists for compatibility. y : object Always ignored, exists for compatibility. groups : object Always ignored, exists for compatibility. Returns n_splits : 1 Returns the number of splitting iterations in the cross-validator. Always returns 1. split(X, y, groups=None) Generate indices to split data into training and test set. Parameters X : array-like, shape (num_examples, num_features) Training data, where num_examples is the number of training examples and num_features is the number of features. y : array-like, shape (num_examples,) The target variable for supervised learning problems. Stratification is done based on the y labels. groups : object Always ignored, exists for compatibility. Yields train_index : ndarray The training set indices for that split. valid_index : ndarray The validation set indices for that split.","title":"Methods"},{"location":"api_subpackages/mlxtend.evaluate/#accuracy_score","text":"accuracy_score(y_target, y_predicted, method='standard', pos_label=1, normalize=True) General accuracy function for supervised learning. Parameters y_target : array-like, shape=[n_values] True class labels or target values. y_predicted : array-like, shape=[n_values] Predicted class labels or target values. method : str, 'standard' by default. The chosen method for accuracy computation. If set to 'standard', computes overall accuracy. If set to 'binary', computes accuracy for class pos_label. If set to 'average', computes average per-class (balanced) accuracy. If set to 'balanced', computes the scikit-learn-style balanced accuracy. pos_label : str or int, 1 by default. The class whose accuracy score is to be reported. Used only when method is set to 'binary' normalize : bool, True by default. If True, returns fraction of correctly classified samples. If False, returns number of correctly classified samples. Returns score: float Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/accuracy_score/","title":"accuracy_score"},{"location":"api_subpackages/mlxtend.evaluate/#bias_variance_decomp","text":"bias_variance_decomp(estimator, X_train, y_train, X_test, y_test, loss='0-1_loss', num_rounds=200, random_seed=None, fit_params) estimator : object A classifier or regressor object or class implementing both a fit and predict method similar to the scikit-learn API. X_train : array-like, shape=(num_examples, num_features) A training dataset for drawing the bootstrap samples to carry out the bias-variance decomposition. y_train : array-like, shape=(num_examples) Targets (class labels, continuous values in case of regression) associated with the X_train examples. X_test : array-like, shape=(num_examples, num_features) The test dataset for computing the average loss, bias, and variance. y_test : array-like, shape=(num_examples) Targets (class labels, continuous values in case of regression) associated with the X_test examples. loss : str (default='0-1_loss') Loss function for performing the bias-variance decomposition. Currently allowed values are '0-1_loss' and 'mse'. num_rounds : int (default=200) Number of bootstrap rounds (sampling from the training set) for performing the bias-variance decomposition. Each bootstrap sample has the same size as the original training set. random_seed : int (default=None) Random seed for the bootstrap sampling used for the bias-variance decomposition. fit_params : additional parameters Additional parameters to be passed to the .fit() function of the estimator when it is fit to the bootstrap samples. Returns avg_expected_loss, avg_bias, avg_var : returns the average expected average bias, and average bias (all floats), where the average is computed over the data points in the test set. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/","title":"bias_variance_decomp"},{"location":"api_subpackages/mlxtend.evaluate/#bootstrap","text":"bootstrap(x, func, num_rounds=1000, ci=0.95, ddof=1, seed=None) Implements the ordinary nonparametric bootstrap Parameters x : NumPy array, shape=(n_samples, [n_columns]) An one or multidimensional array of data records func : A function which computes a statistic that is used to compute the bootstrap replicates (the statistic computed from the bootstrap samples). This function must return a scalar value. For example, np.mean or np.median would be an acceptable argument for func if x is a 1-dimensional array or vector. num_rounds : int (default=1000) The number of bootstrap samples to draw where each bootstrap sample has the same number of records as the original dataset. ci : int (default=0.95) An integer in the range (0, 1) that represents the confidence level for computing the confidence interval. For example, ci=0.95 (default) will compute the 95% confidence interval from the bootstrap replicates. ddof : int The delta degrees of freedom used when computing the standard error. seed : int or None (default=None) Random seed for generating bootstrap samples. Returns original, standard_error, (lower_ci, upper_ci) : tuple Returns the statistic of the original sample ( original ), the standard error of the estimate, and the respective confidence interval bounds. Examples ``` >>> from mlxtend.evaluate import bootstrap >>> rng = np.random.RandomState(123) >>> x = rng.normal(loc=5., size=100) >>> original, std_err, ci_bounds = bootstrap(x, ... num_rounds=1000, ... func=np.mean, ... ci=0.95, ... seed=123) >>> print('Mean: %.2f, SE: +/- %.2f, CI95: [%.2f, %.2f]' % (original, ... std_err, ... ci_bounds[0], ... ci_bounds[1])) Mean: 5.03, SE: +/- 0.11, CI95: [4.80, 5.26] >>> For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap/ ## bootstrap_point632_score *bootstrap_point632_score(estimator, X, y, n_splits=200, method='.632', scoring_func=None, predict_proba=False, random_seed=None, clone_estimator=True, **fit_params)* Implementation of the .632 [1] and .632+ [2] bootstrap for supervised learning References: - [1] Efron, Bradley. 1983. \"Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation.\" Journal of the American Statistical Association 78 (382): 316. doi:10.2307/2288636. - [2] Efron, Bradley, and Robert Tibshirani. 1997. \"Improvements on Cross-Validation: The .632+ Bootstrap Method.\" Journal of the American Statistical Association 92 (438): 548. doi:10.2307/2965703. **Parameters** - `estimator` : object An estimator for classification or regression that follows the scikit-learn API and implements \"fit\" and \"predict\" methods. - `X` : array-like The data to fit. Can be, for example a list, or an array at least 2d. - `y` : array-like, optional, default: None The target variable to try to predict in the case of supervised learning. - `n_splits` : int (default=200) Number of bootstrap iterations. Must be larger than 1. - `method` : str (default='.632') The bootstrap method, which can be either - 1) '.632' bootstrap (default) - 2) '.632+' bootstrap - 3) 'oob' (regular out-of-bag, no weighting) for comparison studies. - `scoring_func` : callable, Score function (or loss function) with signature ``scoring_func(y, y_pred, **kwargs)``. If none, uses classification accuracy if the estimator is a classifier and mean squared error if the estimator is a regressor. - `predict_proba` : bool Whether to use the `predict_proba` function for the `estimator` argument. This is to be used in conjunction with `scoring_func` which takes in probability values instead of actual predictions. For example, if the scoring_func is :meth:`sklearn.metrics.roc_auc_score`, then use `predict_proba=True`. Note that this requires `estimator` to have `predict_proba` method implemented. - `random_seed` : int (default=None) If int, random_seed is the seed used by the random number generator. - `clone_estimator` : bool (default=True) Clones the estimator if true, otherwise fits the original. - `fit_params` : additional parameters Additional parameters to be passed to the .fit() function of the estimator when it is fit to the bootstrap samples. **Returns** - `scores` : array of float, shape=(len(list(n_splits)),) Array of scores of the estimator for each bootstrap replicate. **Examples** >>> from sklearn import datasets, linear_model >>> from mlxtend.evaluate import bootstrap_point632_score >>> iris = datasets.load_iris() >>> X = iris.data >>> y = iris.target >>> lr = linear_model.LogisticRegression() >>> scores = bootstrap_point632_score(lr, X, y) >>> acc = np.mean(scores) >>> print('Accuracy:', acc) 0.953023146884 >>> lower = np.percentile(scores, 2.5) >>> upper = np.percentile(scores, 97.5) >>> print('95%% Confidence interval: [%.2f, %.2f]' % (lower, upper)) 95% Confidence interval: [0.90, 0.98] For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/bootstrap_point632_score/ ```","title":"bootstrap"},{"location":"api_subpackages/mlxtend.evaluate/#cochrans_q","text":"cochrans_q(y_target, y_model_predictions)* Cochran's Q test to compare 2 or more models. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. *y_model_predictions : array-likes, shape=[n_samples] Variable number of 2 or more arrays that contain the predicted class labels from models as 1D NumPy array. Returns q, p : float or None, float Returns the Q (chi-squared) value and the p-value Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/cochrans_q/","title":"cochrans_q"},{"location":"api_subpackages/mlxtend.evaluate/#combined_ftest_5x2cv","text":"combined_ftest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None) Implements the 5x2cv combined F test proposed by Alpaydin 1999, to compare the performance of two models. Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. random_seed : int or None (default: None) Random seed for creating the test/train splits. Returns f : float The F-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/combined_ftest_5x2cv/","title":"combined_ftest_5x2cv"},{"location":"api_subpackages/mlxtend.evaluate/#confusion_matrix","text":"confusion_matrix(y_target, y_predicted, binary=False, positive_label=1) Compute a confusion matrix/contingency table. Parameters y_target : array-like, shape=[n_samples] True class labels. y_predicted : array-like, shape=[n_samples] Predicted class labels. binary : bool (default: False) Maps a multi-class problem onto a binary confusion matrix, where the positive class is 1 and all other classes are 0. positive_label : int (default: 1) Class label of the positive class. Returns mat : array-like, shape=[n_classes, n_classes] Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/confusion_matrix/","title":"confusion_matrix"},{"location":"api_subpackages/mlxtend.evaluate/#create_counterfactual","text":"create_counterfactual(x_reference, y_desired, model, X_dataset, y_desired_proba=None, lammbda=0.1, random_seed=None) Implementation of the counterfactual method by Wachter et al. 2017 References: - Wachter, S., Mittelstadt, B., & Russell, C. (2017). Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harv. JL & Tech., 31, 841., https://arxiv.org/abs/1711.00399 Parameters x_reference : array-like, shape=[m_features] The data instance (training example) to be explained. y_desired : int The desired class label for x_reference . model : estimator A (scikit-learn) estimator implementing .predict() and/or predict_proba() . - If model supports predict_proba() , then this is used by default for the first loss term, (lambda * model.predict[_proba](x_counterfact) - y_desired[_proba])^2 - Otherwise, method will fall back to predict . X_dataset : array-like, shape=[n_examples, m_features] A (training) dataset for picking the initial counterfactual as initial value for starting the optimization procedure. y_desired_proba : float (default: None) A float within the range [0, 1] designating the desired class probability for y_desired . - If y_desired_proba=None (default), the first loss term is (lambda * model(x_counterfact) - y_desired)^2 where y_desired is a class label - If y_desired_proba is not None, the first loss term is (lambda * model(x_counterfact) - y_desired_proba)^2 lammbda : Weighting parameter for the first loss term, (lambda * model(x_counterfact) - y_desired[_proba])^2 random_seed : int (default=None) If int, random_seed is the seed used by the random number generator for selecting the inital counterfactual from X_dataset .","title":"create_counterfactual"},{"location":"api_subpackages/mlxtend.evaluate/#feature_importance_permutation","text":"feature_importance_permutation(X, y, predict_method, metric, num_rounds=1, feature_groups=None, seed=None) Feature importance imputation via permutation importance Parameters X : NumPy array, shape = [n_samples, n_features] Dataset, where n_samples is the number of samples and n_features is the number of features. y : NumPy array, shape = [n_samples] Target values. predict_method : prediction function A callable function that predicts the target values from X. metric : str, callable The metric for evaluating the feature importance through permutation. By default, the strings 'accuracy' is recommended for classifiers and the string 'r2' is recommended for regressors. Optionally, a custom scoring function (e.g., metric=scoring_func ) that accepts two arguments, y_true and y_pred, which have similar shape to the y array. num_rounds : int (default=1) Number of rounds the feature columns are permuted to compute the permutation importance. feature_groups : list or None (default=None) Optional argument for treating certain features as a group. For example [1, 2, [3, 4, 5]] , which can be useful for interpretability, for example, if features 3, 4, 5 are one-hot encoded features. seed : int or None (default=None) Random seed for permuting the feature columns. Returns mean_importance_vals, all_importance_vals : NumPy arrays. The first array, mean_importance_vals has shape [n_features, ] and contains the importance values for all features. The shape of the second array is [n_features, num_rounds] and contains the feature importance for each repetition. If num_rounds=1, it contains the same values as the first array, mean_importance_vals. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/","title":"feature_importance_permutation"},{"location":"api_subpackages/mlxtend.evaluate/#ftest","text":"ftest(y_target, y_model_predictions)* F-Test test to compare 2 or more models. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. *y_model_predictions : array-likes, shape=[n_samples] Variable number of 2 or more arrays that contain the predicted class labels from models as 1D NumPy array. Returns f, p : float or None, float Returns the F-value and the p-value Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/ftest/","title":"ftest"},{"location":"api_subpackages/mlxtend.evaluate/#lift_score","text":"lift_score(y_target, y_predicted, binary=True, positive_label=1) Lift measures the degree to which the predictions of a classification model are better than randomly-generated predictions. The in terms of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN), the lift score is computed as: [ TP / (TP+FP) ] / [ (TP+FN) / (TP+TN+FP+FN) ] Parameters y_target : array-like, shape=[n_samples] True class labels. y_predicted : array-like, shape=[n_samples] Predicted class labels. binary : bool (default: True) Maps a multi-class problem onto a binary, where the positive class is 1 and all other classes are 0. positive_label : int (default: 0) Class label of the positive class. Returns score : float Lift score in the range [0, infinity] Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/lift_score/","title":"lift_score"},{"location":"api_subpackages/mlxtend.evaluate/#mcnemar","text":"mcnemar(ary, corrected=True, exact=False) McNemar test for paired nominal data Parameters ary : array-like, shape=[2, 2] 2 x 2 contigency table (as returned by evaluate.mcnemar_table), where a: ary[0, 0]: # of samples that both models predicted correctly b: ary[0, 1]: # of samples that model 1 got right and model 2 got wrong c: ary[1, 0]: # of samples that model 2 got right and model 1 got wrong d: aryCell [1, 1]: # of samples that both models predicted incorrectly corrected : array-like, shape=[n_samples] (default: True) Uses Edward's continuity correction for chi-squared if True exact : bool, (default: False) If True , uses an exact binomial test comparing b to a binomial distribution with n = b + c and p = 0.5. It is highly recommended to use exact=True for sample sizes < 25 since chi-squared is not well-approximated by the chi-squared distribution! Returns chi2, p : float or None, float Returns the chi-squared value and the p-value; if exact=True (default: False ), chi2 is None Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar/","title":"mcnemar"},{"location":"api_subpackages/mlxtend.evaluate/#mcnemar_table","text":"mcnemar_table(y_target, y_model1, y_model2) Compute a 2x2 contigency table for McNemar's test. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. y_model1 : array-like, shape=[n_samples] Predicted class labels from model as 1D NumPy array. y_model2 : array-like, shape=[n_samples] Predicted class labels from model 2 as 1D NumPy array. Returns tb : array-like, shape=[2, 2] 2x2 contingency table with the following contents: a: tb[0, 0]: # of samples that both models predicted correctly b: tb[0, 1]: # of samples that model 1 got right and model 2 got wrong c: tb[1, 0]: # of samples that model 2 got right and model 1 got wrong d: tb[1, 1]: # of samples that both models predicted incorrectly Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar_table/","title":"mcnemar_table"},{"location":"api_subpackages/mlxtend.evaluate/#mcnemar_tables","text":"mcnemar_tables(y_target, y_model_predictions)* Compute multiple 2x2 contigency tables for McNemar's test or Cochran's Q test. Parameters y_target : array-like, shape=[n_samples] True class labels as 1D NumPy array. y_model_predictions : array-like, shape=[n_samples] Predicted class labels for a model. Returns tables : dict Dictionary of NumPy arrays with shape=[2, 2]. Each dictionary key names the two models to be compared based on the order the models were passed as *y_model_predictions . The number of dictionary entries is equal to the number of pairwise combinations between the m models, i.e., \"m choose 2.\" For example the following target array (containing the true labels) and 3 models y_true = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1]) y_mod0 = np.array([0, 1, 0, 0, 0, 1, 1, 0, 0, 0]) y_mod1 = np.array([0, 0, 1, 1, 0, 1, 1, 0, 0, 0]) y_mod2 = np.array([0, 1, 1, 1, 0, 1, 0, 0, 0, 0]) would result in the following dictionary: {'model_0 vs model_1': array([[ 4., 1.], [ 2., 3.]]), 'model_0 vs model_2': array([[ 3., 0.], [ 3., 4.]]), 'model_1 vs model_2': array([[ 3., 0.], [ 2., 5.]])} Each array is structured in the following way: tb[0, 0]: # of samples that both models predicted correctly tb[0, 1]: # of samples that model a got right and model b got wrong tb[1, 0]: # of samples that model b got right and model a got wrong tb[1, 1]: # of samples that both models predicted incorrectly Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar_tables/","title":"mcnemar_tables"},{"location":"api_subpackages/mlxtend.evaluate/#paired_ttest_5x2cv","text":"paired_ttest_5x2cv(estimator1, estimator2, X, y, scoring=None, random_seed=None) Implements the 5x2cv paired t test proposed by Dieterrich (1998) to compare the performance of two models. Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. random_seed : int or None (default: None) Random seed for creating the test/train splits. Returns t : float The t-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_5x2cv/","title":"paired_ttest_5x2cv"},{"location":"api_subpackages/mlxtend.evaluate/#paired_ttest_kfold_cv","text":"paired_ttest_kfold_cv(estimator1, estimator2, X, y, cv=10, scoring=None, shuffle=False, random_seed=None) Implements the k-fold paired t test procedure to compare the performance of two models. Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. cv : int (default: 10) Number of splits and iteration for the cross-validation procedure scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. shuffle : bool (default: True) Whether to shuffle the dataset for generating the k-fold splits. random_seed : int or None (default: None) Random seed for shuffling the dataset for generating the k-fold splits. Ignored if shuffle=False. Returns t : float The t-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_kfold_cv/","title":"paired_ttest_kfold_cv"},{"location":"api_subpackages/mlxtend.evaluate/#paired_ttest_resampled","text":"paired_ttest_resampled(estimator1, estimator2, X, y, num_rounds=30, test_size=0.3, scoring=None, random_seed=None) Implements the resampled paired t test procedure to compare the performance of two models (also called k-hold-out paired t test). Parameters estimator1 : scikit-learn classifier or regressor estimator2 : scikit-learn classifier or regressor X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. num_rounds : int (default: 30) Number of resampling iterations (i.e., train/test splits) test_size : float or int (default: 0.3) If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to use as a test set. If int, represents the absolute number of test exsamples. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. random_seed : int or None (default: None) Random seed for creating the test/train splits. Returns t : float The t-statistic pvalue : float Two-tailed p-value. If the chosen significance level is larger than the p-value, we reject the null hypothesis and accept that there are significant differences in the two compared models. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/paired_ttest_resampled/","title":"paired_ttest_resampled"},{"location":"api_subpackages/mlxtend.evaluate/#permutation_test","text":"permutation_test(x, y, func='x_mean != y_mean', method='exact', num_rounds=1000, seed=None, paired=False) Nonparametric permutation test Parameters x : list or numpy array with shape (n_datapoints,) A list or 1D numpy array of the first sample (e.g., the treatment group). y : list or numpy array with shape (n_datapoints,) A list or 1D numpy array of the second sample (e.g., the control group). func : custom function or str (default: 'x_mean != y_mean') function to compute the statistic for the permutation test. - If 'x_mean != y_mean', uses func=lambda x, y: np.abs(np.mean(x) - np.mean(y))) for a two-sided test. - If 'x_mean > y_mean', uses func=lambda x, y: np.mean(x) - np.mean(y)) for a one-sided test. - If 'x_mean < y_mean', uses func=lambda x, y: np.mean(y) - np.mean(x)) for a one-sided test. method : 'approximate' or 'exact' (default: 'exact') If 'exact' (default), all possible permutations are considered. If 'approximate' the number of drawn samples is given by num_rounds . Note that 'exact' is typically not feasible unless the dataset size is relatively small. paired : bool If True, a paired test is performed by only exchanging each datapoint with its associate. num_rounds : int (default: 1000) The number of permutation samples if method='approximate' . seed : int or None (default: None) The random seed for generating permutation samples if method='approximate' . Returns p-value under the null hypothesis Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/permutation_test/","title":"permutation_test"},{"location":"api_subpackages/mlxtend.evaluate/#proportion_difference","text":"proportion_difference(proportion_1, proportion_2, n_1, n_2=None) Computes the test statistic and p-value for a difference of proportions test. Parameters proportion_1 : float The first proportion proportion_2 : float The second proportion n_1 : int The sample size of the first test sample n_2 : int or None (default=None) The sample size of the second test sample. If None , n_1 = n_2 . Returns z, p : float or None, float Returns the z-score and the p-value Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/proportion_difference/","title":"proportion_difference"},{"location":"api_subpackages/mlxtend.evaluate/#scoring","text":"scoring(y_target, y_predicted, metric='error', positive_label=1, unique_labels='auto') Compute a scoring metric for supervised learning. Parameters y_target : array-like, shape=[n_values] True class labels or target values. y_predicted : array-like, shape=[n_values] Predicted class labels or target values. metric : str (default: 'error') Performance metric: 'accuracy': (TP + TN)/(FP + FN + TP + TN) = 1-ERR 'average per-class accuracy': Average per-class accuracy 'average per-class error': Average per-class error 'balanced per-class accuracy': Average per-class accuracy 'balanced per-class error': Average per-class error 'error': (TP + TN)/(FP+ FN + TP + TN) = 1-ACC 'false_positive_rate': FP/N = FP/(FP + TN) 'true_positive_rate': TP/P = TP/(FN + TP) 'true_negative_rate': TN/N = TN/(FP + TN) 'precision': TP/(TP + FP) 'recall': equal to 'true_positive_rate' 'sensitivity': equal to 'true_positive_rate' or 'recall' 'specificity': equal to 'true_negative_rate' 'f1': 2 * (PRE * REC)/(PRE + REC) 'matthews_corr_coef': (TP TN - FP FN) / (sqrt{(TP + FP)( TP + FN )( TN + FP )( TN + FN )}) Where: [TP: True positives, TN = True negatives, TN: True negatives, FN = False negatives] positive_label : int (default: 1) Label of the positive class for binary classification metrics. unique_labels : str or array-like (default: 'auto') If 'auto', deduces the unique class labels from y_target Returns score : float Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/evaluate/scoring/","title":"scoring"},{"location":"api_subpackages/mlxtend.externals/","text":"mlxtend version: 0.23.1","title":"Mlxtend.externals"},{"location":"api_subpackages/mlxtend.feature_extraction/","text":"mlxtend version: 0.23.1 LinearDiscriminantAnalysis LinearDiscriminantAnalysis(n_discriminants=None) Linear Discriminant Analysis Class Parameters n_discriminants : int (default: None) The number of discrimants for transformation. Keeps the original dimensions of the dataset if None . Attributes w_ : array-like, shape=[n_features, n_discriminants] Projection matrix e_vals_ : array-like, shape=[n_features] Eigenvalues in sorted order. e_vecs_ : array-like, shape=[n_features] Eigenvectors in sorted order. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_extraction/LinearDiscriminantAnalysis/ Methods fit(X, y, n_classes=None) Fit the LDA model with X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. n_classes : int (default: None) A positive integer to declare the number of class labels if not all class labels are present in a partial training set. Gets the number of class labels automatically if None. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause transform(X) Apply the linear transformation on X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_projected : np.ndarray, shape = [n_samples, n_discriminants] Projected training vectors. PrincipalComponentAnalysis PrincipalComponentAnalysis(n_components=None, solver='svd', whitening=False) Principal Component Analysis Class Parameters n_components : int (default: None) The number of principal components for transformation. Keeps the original dimensions of the dataset if None . solver : str (default: 'svd') Method for performing the matrix decomposition. {'eigen', 'svd'} whitening : bool (default: False) Performs whitening such that the covariance matrix of the transformed data will be the identity matrix. Attributes w_ : array-like, shape=[n_features, n_components] Projection matrix e_vals_ : array-like, shape=[n_features] Eigenvalues in sorted order. e_vecs_ : array-like, shape=[n_features] Eigenvectors in sorted order. e_vals_normalized_ : array-like, shape=[n_features] Normalized eigen values such that they sum up to 1. This is equal to what's often referred to as \"explained variance ratios.\" loadings_ : array_like, shape=[n_features, n_features] The factor loadings of the original variables onto the principal components. The columns are the principal components, and the rows are the features loadings. For instance, the first column contains the loadings onto the first principal component. Note that the signs may be flipped depending on whether you use the 'eigen' or 'svd' solver; this does not affect the interpretation of the loadings though. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_extraction/PrincipalComponentAnalysis/ Methods fit(X, y=None) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause transform(X) Apply the linear transformation on X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_projected : np.ndarray, shape = [n_samples, n_components] Projected training vectors. RBFKernelPCA RBFKernelPCA(gamma=15.0, n_components=None, copy_X=True) RBF Kernel Principal Component Analysis for dimensionality reduction. Parameters gamma : float (default: 15.0) Free parameter (coefficient) of the RBF kernel. n_components : int (default: None) The number of principal components for transformation. Keeps the original dimensions of the dataset if None . copy_X : bool (default: True) Copies training data, which is required to compute the projection of new data via the transform method. Uses a reference to X if False. Attributes e_vals_ : array-like, shape=[n_features] Eigenvalues in sorted order. e_vecs_ : array-like, shape=[n_features] Eigenvectors in sorted order. X_projected_ : array-like, shape=[n_samples, n_components] Training samples projected along the component axes. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_extraction/RBFKernelPCA/ Methods fit(X) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause transform(X) Apply the non-linear transformation on X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_projected : np.ndarray, shape = [n_samples, n_components] Projected training vectors.","title":"Mlxtend.feature extraction"},{"location":"api_subpackages/mlxtend.feature_extraction/#lineardiscriminantanalysis","text":"LinearDiscriminantAnalysis(n_discriminants=None) Linear Discriminant Analysis Class Parameters n_discriminants : int (default: None) The number of discrimants for transformation. Keeps the original dimensions of the dataset if None . Attributes w_ : array-like, shape=[n_features, n_discriminants] Projection matrix e_vals_ : array-like, shape=[n_features] Eigenvalues in sorted order. e_vecs_ : array-like, shape=[n_features] Eigenvectors in sorted order. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_extraction/LinearDiscriminantAnalysis/","title":"LinearDiscriminantAnalysis"},{"location":"api_subpackages/mlxtend.feature_extraction/#methods","text":"fit(X, y, n_classes=None) Fit the LDA model with X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. n_classes : int (default: None) A positive integer to declare the number of class labels if not all class labels are present in a partial training set. Gets the number of class labels automatically if None. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause transform(X) Apply the linear transformation on X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_projected : np.ndarray, shape = [n_samples, n_discriminants] Projected training vectors.","title":"Methods"},{"location":"api_subpackages/mlxtend.feature_extraction/#principalcomponentanalysis","text":"PrincipalComponentAnalysis(n_components=None, solver='svd', whitening=False) Principal Component Analysis Class Parameters n_components : int (default: None) The number of principal components for transformation. Keeps the original dimensions of the dataset if None . solver : str (default: 'svd') Method for performing the matrix decomposition. {'eigen', 'svd'} whitening : bool (default: False) Performs whitening such that the covariance matrix of the transformed data will be the identity matrix. Attributes w_ : array-like, shape=[n_features, n_components] Projection matrix e_vals_ : array-like, shape=[n_features] Eigenvalues in sorted order. e_vecs_ : array-like, shape=[n_features] Eigenvectors in sorted order. e_vals_normalized_ : array-like, shape=[n_features] Normalized eigen values such that they sum up to 1. This is equal to what's often referred to as \"explained variance ratios.\" loadings_ : array_like, shape=[n_features, n_features] The factor loadings of the original variables onto the principal components. The columns are the principal components, and the rows are the features loadings. For instance, the first column contains the loadings onto the first principal component. Note that the signs may be flipped depending on whether you use the 'eigen' or 'svd' solver; this does not affect the interpretation of the loadings though. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_extraction/PrincipalComponentAnalysis/","title":"PrincipalComponentAnalysis"},{"location":"api_subpackages/mlxtend.feature_extraction/#methods_1","text":"fit(X, y=None) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause transform(X) Apply the linear transformation on X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_projected : np.ndarray, shape = [n_samples, n_components] Projected training vectors.","title":"Methods"},{"location":"api_subpackages/mlxtend.feature_extraction/#rbfkernelpca","text":"RBFKernelPCA(gamma=15.0, n_components=None, copy_X=True) RBF Kernel Principal Component Analysis for dimensionality reduction. Parameters gamma : float (default: 15.0) Free parameter (coefficient) of the RBF kernel. n_components : int (default: None) The number of principal components for transformation. Keeps the original dimensions of the dataset if None . copy_X : bool (default: True) Copies training data, which is required to compute the projection of new data via the transform method. Uses a reference to X if False. Attributes e_vals_ : array-like, shape=[n_features] Eigenvalues in sorted order. e_vecs_ : array-like, shape=[n_features] Eigenvectors in sorted order. X_projected_ : array-like, shape=[n_samples, n_components] Training samples projected along the component axes. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_extraction/RBFKernelPCA/","title":"RBFKernelPCA"},{"location":"api_subpackages/mlxtend.feature_extraction/#methods_2","text":"fit(X) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause transform(X) Apply the non-linear transformation on X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_projected : np.ndarray, shape = [n_samples, n_components] Projected training vectors.","title":"Methods"},{"location":"api_subpackages/mlxtend.feature_selection/","text":"mlxtend version: 0.23.1 ColumnSelector ColumnSelector(cols=None, drop_axis=False) Object for selecting specific columns from a data set. Parameters cols : array-like (default: None) A list specifying the feature indices to be selected. For example, [1, 4, 5] to select the 2nd, 5th, and 6th feature columns, and ['A','C','D'] to select the name of feature columns A, C and D. If None, returns all columns in the array. drop_axis : bool (default=False) Drops last axis if True and the only one column is selected. This is useful, e.g., when the ColumnSelector is used for selecting only one column and the resulting array should be fed to e.g., a scikit-learn column selector. E.g., instead of returning an array with shape (n_samples, 1), drop_axis=True will return an aray with shape (n_samples,). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_selection/ColumnSelector/ Methods fit(X, y=None) Mock method. Does nothing. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns self fit_transform(X, y=None) Return a slice of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_slice : shape = [n_samples, k_features] Subset of the feature space where k_features <= n_features get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X, y=None) Return a slice of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_slice : shape = [n_samples, k_features] Subset of the feature space where k_features <= n_features ExhaustiveFeatureSelector ExhaustiveFeatureSelector(estimator, min_features=1, max_features=1, print_progress=True, scoring='accuracy', cv=5, n_jobs=1, pre_dispatch='2 n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)* Exhaustive Feature Selection for Classification and Regression. (new in v0.4.3) Parameters estimator : scikit-learn classifier or regressor min_features : int (default: 1) Minumum number of features to select max_features : int (default: 1) Maximum number of features to select. If parameter feature_groups is not None, the number of features is equal to the number of feature groups, i.e. len(feature_groups) . For example, if feature_groups = [[0], [1], [2, 3], [4]] , then the max_features value cannot exceed 4. print_progress : bool (default: True) Prints progress as the number of epochs to stderr. scoring : str, (default='accuracy') Scoring metric in {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'r2'} for regressors, or a callable object or function with signature scorer(estimator, X, y) . cv : int (default: 5) Scikit-learn cross-validation generator or int . If estimator is a classifier (or y consists of integer class labels), stratified k-fold is performed, and regular k-fold cross-validation otherwise. No cross-validation if cv is None, False, or 0. n_jobs : int (default: 1) The number of CPUs to use for evaluating different feature subsets in parallel. -1 means 'all CPUs'. pre_dispatch : int, or string (default: '2*n_jobs') Controls the number of jobs that get dispatched during parallel execution if n_jobs > 1 or n_jobs=-1 . Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs clone_estimator : bool (default: True) Clones estimator if True; works with the original estimator instance if False. Set to False if the estimator doesn't implement scikit-learn's set_params and get_params methods. In addition, it is required to set cv=0, and n_jobs=1. fixed_features : tuple (default: None) If not None , the feature indices provided as a tuple will be regarded as fixed by the feature selector. For example, if fixed_features=(1, 3, 7) , the 2nd, 4th, and 8th feature are guaranteed to be present in the solution. Note that if fixed_features is not None , make sure that the number of features to be selected is greater than len(fixed_features) . In other words, ensure that k_features > len(fixed_features) . feature_groups : list or None (default: None) Optional argument for treating certain features as a group. This means, the features within a group are always selected together, never split. For example, feature_groups=[[1], [2], [3, 4, 5]] specifies 3 feature groups.In this case, possible feature selection results with k_features=2 are [[1], [2] , [[1], [3, 4, 5]] , or [[2], [3, 4, 5]] . Feature groups can be useful for interpretability, for example, if features 3, 4, 5 are one-hot encoded features. (For more details, please read the notes at the bottom of this docstring). New in mlxtend v. 0.21.0. Attributes best_idx_ : array-like, shape = [n_predictions] Feature Indices of the selected feature subsets. best_feature_names_ : array-like, shape = [n_predictions] Feature names of the selected feature subsets. If pandas DataFrames are used in the fit method, the feature names correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. New in v 0.13.0. best_score_ : float Cross validation average score of the selected subset. subsets_ : dict A dictionary of selected feature subsets during the exhaustive selection, where the dictionary keys are the lengths k of these feature subsets. The dictionary values are dictionaries themselves with the following keys: 'feature_idx' (tuple of indices of the feature subset) 'feature_names' (tuple of feature names of the feat. subset) 'cv_scores' (list individual cross-validation scores) 'avg_score' (average cross-validation score) Note that if pandas DataFrames are used in the fit method, the 'feature_names' correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. The 'feature_names' is new in v. 0.13.0. Notes (1) If parameter feature_groups is not None, the number of features is equal to the number of feature groups, i.e. len(feature_groups) . For example, if feature_groups = [[0], [1], [2, 3], [4]] , then the max_features value cannot exceed 4. (2) Although two or more individual features may be considered as one group throughout the feature-selection process, it does not mean the individual features of that group have the same impact on the outcome. For instance, in linear regression, the coefficient of the feature 2 and 3 can be different even if they are considered as one group in feature_groups. (3) If both fixed_features and feature_groups are specified, ensure that each feature group contains the fixed_features selection. E.g., for a 3-feature set fixed_features=[0, 1] and feature_groups=[[0, 1], [2]] is valid; fixed_features=[0, 1] and feature_groups=[[0], [1, 2]] is not valid. (4) In case of KeyboardInterrupt, the dictionary subsets may not be completed. If user is still interested in getting the best score, they can use method `finalize_fit`. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/ Methods finalize_fit() None fit(X, y, groups=None, fit_params) Perform feature selection and learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : dict of string -> object, optional Parameters to pass to to the fit method of classifier. Returns self : object fit_transform(X, y, groups=None, fit_params) Fit to training data and return the best selected features from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : dict of string -> object, optional Parameters to pass to to the fit method of classifier. Returns Feature subset of X, shape={n_samples, k_features} get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_metric_dict(confidence_interval=0.95) Return metric dictionary Parameters confidence_interval : float (default: 0.95) A positive float between 0.0 and 1.0 to compute the confidence interval bounds of the CV score averages. Returns Dictionary with items where each dictionary value is a list with the number of iterations (number of feature subsets) as its length. The dictionary keys corresponding to these lists are as follows: 'feature_idx': tuple of the indices of the feature subset 'cv_scores': list with individual CV scores 'avg_score': of CV average scores 'std_dev': standard deviation of the CV score average 'std_err': standard error of the CV score average 'ci_bound': confidence interval bound of the CV score average get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_fit_request(self: mlxtend.feature_selection.exhaustive_feature_selector.ExhaustiveFeatureSelector, , groups: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.feature_selection.exhaustive_feature_selector.ExhaustiveFeatureSelector* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . Returns self : object The updated object. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X) Return the best selected features from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. Returns Feature subset of X, shape={n_samples, k_features} SequentialFeatureSelector SequentialFeatureSelector(estimator, k_features=1, forward=True, floating=False, verbose=0, scoring=None, cv=5, n_jobs=1, pre_dispatch='2 n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)* Sequential Feature Selection for Classification and Regression. Parameters estimator : scikit-learn classifier or regressor k_features : int or tuple or str (default: 1) Number of features to select, where k_features < the full feature set. New in 0.4.2: A tuple containing a min and max value can be provided, and the SFS will consider return any feature combination between min and max that scored highest in cross-validation. For example, the tuple (1, 4) will return any combination from 1 up to 4 features instead of a fixed number of features k. New in 0.8.0: A string argument \"best\" or \"parsimonious\". If \"best\" is provided, the feature selector will return the feature subset with the best cross-validation performance. If \"parsimonious\" is provided as an argument, the smallest feature subset that is within one standard error of the cross-validation performance will be selected. forward : bool (default: True) Forward selection if True, backward selection otherwise floating : bool (default: False) Adds a conditional exclusion/inclusion if True. verbose : int (default: 0), level of verbosity to use in logging. If 0, no output, if 1 number of features in current set, if 2 detailed logging i ncluding timestamp and cv scores at step. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. cv : int (default: 5) Integer or iterable yielding train, test splits. If cv is an integer and estimator is a classifier (or y consists of integer class labels) stratified k-fold. Otherwise regular k-fold cross-validation is performed. No cross-validation if cv is None, False, or 0. n_jobs : int (default: 1) The number of CPUs to use for evaluating different feature subsets in parallel. -1 means 'all CPUs'. pre_dispatch : int, or string (default: '2*n_jobs') Controls the number of jobs that get dispatched during parallel execution if n_jobs > 1 or n_jobs=-1 . Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs clone_estimator : bool (default: True) Clones estimator if True; works with the original estimator instance if False. Set to False if the estimator doesn't implement scikit-learn's set_params and get_params methods. In addition, it is required to set cv=0, and n_jobs=1. fixed_features : tuple (default: None) If not None , the feature indices provided as a tuple will be regarded as fixed by the feature selector. For example, if fixed_features=(1, 3, 7) , the 2nd, 4th, and 8th feature are guaranteed to be present in the solution. Note that if fixed_features is not None , make sure that the number of features to be selected is greater than len(fixed_features) . In other words, ensure that k_features > len(fixed_features) . New in mlxtend v. 0.18.0. feature_groups : list or None (default: None) Optional argument for treating certain features as a group. This means, the features within a group are always selected together, never split. For example, feature_groups=[[1], [2], [3, 4, 5]] specifies 3 feature groups. In this case, possible feature selection results with k_features=2 are [[1], [2] , [[1], [3, 4, 5]] , or [[2], [3, 4, 5]] . Feature groups can be useful for interpretability, for example, if features 3, 4, 5 are one-hot encoded features. (For more details, please read the notes at the bottom of this docstring). New in mlxtend v. 0.21.0. Attributes k_feature_idx_ : array-like, shape = [n_predictions] Feature Indices of the selected feature subsets. k_feature_names_ : array-like, shape = [n_predictions] Feature names of the selected feature subsets. If pandas DataFrames are used in the fit method, the feature names correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. New in v 0.13.0. k_score_ : float Cross validation average score of the selected subset. subsets_ : dict A dictionary of selected feature subsets during the sequential selection, where the dictionary keys are the lengths k of these feature subsets. If the parameter feature_groups is not None, the value of key indicates the number of groups that are selected together. The dictionary values are dictionaries themselves with the following keys: 'feature_idx' (tuple of indices of the feature subset) 'feature_names' (tuple of feature names of the feat. subset) 'cv_scores' (list individual cross-validation scores) 'avg_score' (average cross-validation score) Note that if pandas DataFrames are used in the fit method, the 'feature_names' correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. The 'feature_names' is new in v 0.13.0. Notes (1) If parameter feature_groups is not None, the number of features is equal to the number of feature groups, i.e. len(feature_groups) . For example, if feature_groups = [[0], [1], [2, 3], [4]] , then the max_features value cannot exceed 4. (2) Although two or more individual features may be considered as one group throughout the feature-selection process, it does not mean the individual features of that group have the same impact on the outcome. For instance, in linear regression, the coefficient of the feature 2 and 3 can be different even if they are considered as one group in feature_groups. (3) If both fixed_features and feature_groups are specified, ensure that each feature group contains the fixed_features selection. E.g., for a 3-feature set fixed_features=[0, 1] and feature_groups=[[0, 1], [2]] is valid; fixed_features=[0, 1] and feature_groups=[[0], [1, 2]] is not valid. (4) In case of KeyboardInterrupt, the dictionary subsets may not be completed. If user is still interested in getting the best score, they can use method `finalize_fit`. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/ Methods finalize_fit() None fit(X, y, groups=None, fit_params) Perform feature selection and learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. New in v 0.13.0: pandas DataFrames are now also accepted as argument for y. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : various, optional Additional parameters that are being passed to the estimator. For example, sample_weights=weights . Returns self : object fit_transform(X, y, groups=None, fit_params) Fit to training data then reduce X to its most important features. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. New in v 0.13.0: a pandas Series are now also accepted as argument for y. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : various, optional Additional parameters that are being passed to the estimator. For example, sample_weights=weights . Returns Reduced feature subset of X, shape={n_samples, k_features} generate_error_message_k_features(name) None get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_metric_dict(confidence_interval=0.95) Return metric dictionary Parameters confidence_interval : float (default: 0.95) A positive float between 0.0 and 1.0 to compute the confidence interval bounds of the CV score averages. Returns Dictionary with items where each dictionary value is a list with the number of iterations (number of feature subsets) as its length. The dictionary keys corresponding to these lists are as follows: 'feature_idx': tuple of the indices of the feature subset 'cv_scores': list with individual CV scores 'avg_score': of CV average scores 'std_dev': standard deviation of the CV score average 'std_err': standard error of the CV score average 'ci_bound': confidence interval bound of the CV score average get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_fit_request(self: mlxtend.feature_selection.sequential_feature_selector.SequentialFeatureSelector, , groups: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.feature_selection.sequential_feature_selector.SequentialFeatureSelector* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . Returns self : object The updated object. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with get_params() . Returns self transform(X) Reduce X to its most important features. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. Returns Reduced feature subset of X, shape={n_samples, k_features} Properties named_estimators Returns List of named estimator tuples, like [('svc', SVC(...))]","title":"Mlxtend.feature selection"},{"location":"api_subpackages/mlxtend.feature_selection/#columnselector","text":"ColumnSelector(cols=None, drop_axis=False) Object for selecting specific columns from a data set. Parameters cols : array-like (default: None) A list specifying the feature indices to be selected. For example, [1, 4, 5] to select the 2nd, 5th, and 6th feature columns, and ['A','C','D'] to select the name of feature columns A, C and D. If None, returns all columns in the array. drop_axis : bool (default=False) Drops last axis if True and the only one column is selected. This is useful, e.g., when the ColumnSelector is used for selecting only one column and the resulting array should be fed to e.g., a scikit-learn column selector. E.g., instead of returning an array with shape (n_samples, 1), drop_axis=True will return an aray with shape (n_samples,). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_selection/ColumnSelector/","title":"ColumnSelector"},{"location":"api_subpackages/mlxtend.feature_selection/#methods","text":"fit(X, y=None) Mock method. Does nothing. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns self fit_transform(X, y=None) Return a slice of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_slice : shape = [n_samples, k_features] Subset of the feature space where k_features <= n_features get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X, y=None) Return a slice of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_slice : shape = [n_samples, k_features] Subset of the feature space where k_features <= n_features","title":"Methods"},{"location":"api_subpackages/mlxtend.feature_selection/#exhaustivefeatureselector","text":"ExhaustiveFeatureSelector(estimator, min_features=1, max_features=1, print_progress=True, scoring='accuracy', cv=5, n_jobs=1, pre_dispatch='2 n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)* Exhaustive Feature Selection for Classification and Regression. (new in v0.4.3) Parameters estimator : scikit-learn classifier or regressor min_features : int (default: 1) Minumum number of features to select max_features : int (default: 1) Maximum number of features to select. If parameter feature_groups is not None, the number of features is equal to the number of feature groups, i.e. len(feature_groups) . For example, if feature_groups = [[0], [1], [2, 3], [4]] , then the max_features value cannot exceed 4. print_progress : bool (default: True) Prints progress as the number of epochs to stderr. scoring : str, (default='accuracy') Scoring metric in {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'r2'} for regressors, or a callable object or function with signature scorer(estimator, X, y) . cv : int (default: 5) Scikit-learn cross-validation generator or int . If estimator is a classifier (or y consists of integer class labels), stratified k-fold is performed, and regular k-fold cross-validation otherwise. No cross-validation if cv is None, False, or 0. n_jobs : int (default: 1) The number of CPUs to use for evaluating different feature subsets in parallel. -1 means 'all CPUs'. pre_dispatch : int, or string (default: '2*n_jobs') Controls the number of jobs that get dispatched during parallel execution if n_jobs > 1 or n_jobs=-1 . Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs clone_estimator : bool (default: True) Clones estimator if True; works with the original estimator instance if False. Set to False if the estimator doesn't implement scikit-learn's set_params and get_params methods. In addition, it is required to set cv=0, and n_jobs=1. fixed_features : tuple (default: None) If not None , the feature indices provided as a tuple will be regarded as fixed by the feature selector. For example, if fixed_features=(1, 3, 7) , the 2nd, 4th, and 8th feature are guaranteed to be present in the solution. Note that if fixed_features is not None , make sure that the number of features to be selected is greater than len(fixed_features) . In other words, ensure that k_features > len(fixed_features) . feature_groups : list or None (default: None) Optional argument for treating certain features as a group. This means, the features within a group are always selected together, never split. For example, feature_groups=[[1], [2], [3, 4, 5]] specifies 3 feature groups.In this case, possible feature selection results with k_features=2 are [[1], [2] , [[1], [3, 4, 5]] , or [[2], [3, 4, 5]] . Feature groups can be useful for interpretability, for example, if features 3, 4, 5 are one-hot encoded features. (For more details, please read the notes at the bottom of this docstring). New in mlxtend v. 0.21.0. Attributes best_idx_ : array-like, shape = [n_predictions] Feature Indices of the selected feature subsets. best_feature_names_ : array-like, shape = [n_predictions] Feature names of the selected feature subsets. If pandas DataFrames are used in the fit method, the feature names correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. New in v 0.13.0. best_score_ : float Cross validation average score of the selected subset. subsets_ : dict A dictionary of selected feature subsets during the exhaustive selection, where the dictionary keys are the lengths k of these feature subsets. The dictionary values are dictionaries themselves with the following keys: 'feature_idx' (tuple of indices of the feature subset) 'feature_names' (tuple of feature names of the feat. subset) 'cv_scores' (list individual cross-validation scores) 'avg_score' (average cross-validation score) Note that if pandas DataFrames are used in the fit method, the 'feature_names' correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. The 'feature_names' is new in v. 0.13.0. Notes (1) If parameter feature_groups is not None, the number of features is equal to the number of feature groups, i.e. len(feature_groups) . For example, if feature_groups = [[0], [1], [2, 3], [4]] , then the max_features value cannot exceed 4. (2) Although two or more individual features may be considered as one group throughout the feature-selection process, it does not mean the individual features of that group have the same impact on the outcome. For instance, in linear regression, the coefficient of the feature 2 and 3 can be different even if they are considered as one group in feature_groups. (3) If both fixed_features and feature_groups are specified, ensure that each feature group contains the fixed_features selection. E.g., for a 3-feature set fixed_features=[0, 1] and feature_groups=[[0, 1], [2]] is valid; fixed_features=[0, 1] and feature_groups=[[0], [1, 2]] is not valid. (4) In case of KeyboardInterrupt, the dictionary subsets may not be completed. If user is still interested in getting the best score, they can use method `finalize_fit`. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/","title":"ExhaustiveFeatureSelector"},{"location":"api_subpackages/mlxtend.feature_selection/#methods_1","text":"finalize_fit() None fit(X, y, groups=None, fit_params) Perform feature selection and learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : dict of string -> object, optional Parameters to pass to to the fit method of classifier. Returns self : object fit_transform(X, y, groups=None, fit_params) Fit to training data and return the best selected features from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : dict of string -> object, optional Parameters to pass to to the fit method of classifier. Returns Feature subset of X, shape={n_samples, k_features} get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_metric_dict(confidence_interval=0.95) Return metric dictionary Parameters confidence_interval : float (default: 0.95) A positive float between 0.0 and 1.0 to compute the confidence interval bounds of the CV score averages. Returns Dictionary with items where each dictionary value is a list with the number of iterations (number of feature subsets) as its length. The dictionary keys corresponding to these lists are as follows: 'feature_idx': tuple of the indices of the feature subset 'cv_scores': list with individual CV scores 'avg_score': of CV average scores 'std_dev': standard deviation of the CV score average 'std_err': standard error of the CV score average 'ci_bound': confidence interval bound of the CV score average get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_fit_request(self: mlxtend.feature_selection.exhaustive_feature_selector.ExhaustiveFeatureSelector, , groups: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.feature_selection.exhaustive_feature_selector.ExhaustiveFeatureSelector* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . Returns self : object The updated object. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X) Return the best selected features from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. Returns Feature subset of X, shape={n_samples, k_features}","title":"Methods"},{"location":"api_subpackages/mlxtend.feature_selection/#sequentialfeatureselector","text":"SequentialFeatureSelector(estimator, k_features=1, forward=True, floating=False, verbose=0, scoring=None, cv=5, n_jobs=1, pre_dispatch='2 n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)* Sequential Feature Selection for Classification and Regression. Parameters estimator : scikit-learn classifier or regressor k_features : int or tuple or str (default: 1) Number of features to select, where k_features < the full feature set. New in 0.4.2: A tuple containing a min and max value can be provided, and the SFS will consider return any feature combination between min and max that scored highest in cross-validation. For example, the tuple (1, 4) will return any combination from 1 up to 4 features instead of a fixed number of features k. New in 0.8.0: A string argument \"best\" or \"parsimonious\". If \"best\" is provided, the feature selector will return the feature subset with the best cross-validation performance. If \"parsimonious\" is provided as an argument, the smallest feature subset that is within one standard error of the cross-validation performance will be selected. forward : bool (default: True) Forward selection if True, backward selection otherwise floating : bool (default: False) Adds a conditional exclusion/inclusion if True. verbose : int (default: 0), level of verbosity to use in logging. If 0, no output, if 1 number of features in current set, if 2 detailed logging i ncluding timestamp and cv scores at step. scoring : str, callable, or None (default: None) If None (default), uses 'accuracy' for sklearn classifiers and 'r2' for sklearn regressors. If str, uses a sklearn scoring metric string identifier, for example {accuracy, f1, precision, recall, roc_auc} for classifiers, {'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error', 'median_absolute_error', 'r2'} for regressors. If a callable object or function is provided, it has to be conform with sklearn's signature scorer(estimator, X, y) ; see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html for more information. cv : int (default: 5) Integer or iterable yielding train, test splits. If cv is an integer and estimator is a classifier (or y consists of integer class labels) stratified k-fold. Otherwise regular k-fold cross-validation is performed. No cross-validation if cv is None, False, or 0. n_jobs : int (default: 1) The number of CPUs to use for evaluating different feature subsets in parallel. -1 means 'all CPUs'. pre_dispatch : int, or string (default: '2*n_jobs') Controls the number of jobs that get dispatched during parallel execution if n_jobs > 1 or n_jobs=-1 . Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs clone_estimator : bool (default: True) Clones estimator if True; works with the original estimator instance if False. Set to False if the estimator doesn't implement scikit-learn's set_params and get_params methods. In addition, it is required to set cv=0, and n_jobs=1. fixed_features : tuple (default: None) If not None , the feature indices provided as a tuple will be regarded as fixed by the feature selector. For example, if fixed_features=(1, 3, 7) , the 2nd, 4th, and 8th feature are guaranteed to be present in the solution. Note that if fixed_features is not None , make sure that the number of features to be selected is greater than len(fixed_features) . In other words, ensure that k_features > len(fixed_features) . New in mlxtend v. 0.18.0. feature_groups : list or None (default: None) Optional argument for treating certain features as a group. This means, the features within a group are always selected together, never split. For example, feature_groups=[[1], [2], [3, 4, 5]] specifies 3 feature groups. In this case, possible feature selection results with k_features=2 are [[1], [2] , [[1], [3, 4, 5]] , or [[2], [3, 4, 5]] . Feature groups can be useful for interpretability, for example, if features 3, 4, 5 are one-hot encoded features. (For more details, please read the notes at the bottom of this docstring). New in mlxtend v. 0.21.0. Attributes k_feature_idx_ : array-like, shape = [n_predictions] Feature Indices of the selected feature subsets. k_feature_names_ : array-like, shape = [n_predictions] Feature names of the selected feature subsets. If pandas DataFrames are used in the fit method, the feature names correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. New in v 0.13.0. k_score_ : float Cross validation average score of the selected subset. subsets_ : dict A dictionary of selected feature subsets during the sequential selection, where the dictionary keys are the lengths k of these feature subsets. If the parameter feature_groups is not None, the value of key indicates the number of groups that are selected together. The dictionary values are dictionaries themselves with the following keys: 'feature_idx' (tuple of indices of the feature subset) 'feature_names' (tuple of feature names of the feat. subset) 'cv_scores' (list individual cross-validation scores) 'avg_score' (average cross-validation score) Note that if pandas DataFrames are used in the fit method, the 'feature_names' correspond to the column names. Otherwise, the feature names are string representation of the feature array indices. The 'feature_names' is new in v 0.13.0. Notes (1) If parameter feature_groups is not None, the number of features is equal to the number of feature groups, i.e. len(feature_groups) . For example, if feature_groups = [[0], [1], [2, 3], [4]] , then the max_features value cannot exceed 4. (2) Although two or more individual features may be considered as one group throughout the feature-selection process, it does not mean the individual features of that group have the same impact on the outcome. For instance, in linear regression, the coefficient of the feature 2 and 3 can be different even if they are considered as one group in feature_groups. (3) If both fixed_features and feature_groups are specified, ensure that each feature group contains the fixed_features selection. E.g., for a 3-feature set fixed_features=[0, 1] and feature_groups=[[0, 1], [2]] is valid; fixed_features=[0, 1] and feature_groups=[[0], [1, 2]] is not valid. (4) In case of KeyboardInterrupt, the dictionary subsets may not be completed. If user is still interested in getting the best score, they can use method `finalize_fit`. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/","title":"SequentialFeatureSelector"},{"location":"api_subpackages/mlxtend.feature_selection/#methods_2","text":"finalize_fit() None fit(X, y, groups=None, fit_params) Perform feature selection and learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. New in v 0.13.0: pandas DataFrames are now also accepted as argument for y. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : various, optional Additional parameters that are being passed to the estimator. For example, sample_weights=weights . Returns self : object fit_transform(X, y, groups=None, fit_params) Fit to training data then reduce X to its most important features. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. y : array-like, shape = [n_samples] Target values. New in v 0.13.0: a pandas Series are now also accepted as argument for y. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. Passed to the fit method of the cross-validator. fit_params : various, optional Additional parameters that are being passed to the estimator. For example, sample_weights=weights . Returns Reduced feature subset of X, shape={n_samples, k_features} generate_error_message_k_features(name) None get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_metric_dict(confidence_interval=0.95) Return metric dictionary Parameters confidence_interval : float (default: 0.95) A positive float between 0.0 and 1.0 to compute the confidence interval bounds of the CV score averages. Returns Dictionary with items where each dictionary value is a list with the number of iterations (number of feature subsets) as its length. The dictionary keys corresponding to these lists are as follows: 'feature_idx': tuple of the indices of the feature subset 'cv_scores': list with individual CV scores 'avg_score': of CV average scores 'std_dev': standard deviation of the CV score average 'std_err': standard error of the CV score average 'ci_bound': confidence interval bound of the CV score average get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_fit_request(self: mlxtend.feature_selection.sequential_feature_selector.SequentialFeatureSelector, , groups: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.feature_selection.sequential_feature_selector.SequentialFeatureSelector* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . Returns self : object The updated object. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with get_params() . Returns self transform(X) Reduce X to its most important features. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. New in v 0.13.0: pandas DataFrames are now also accepted as argument for X. Returns Reduced feature subset of X, shape={n_samples, k_features}","title":"Methods"},{"location":"api_subpackages/mlxtend.feature_selection/#properties","text":"named_estimators Returns List of named estimator tuples, like [('svc', SVC(...))]","title":"Properties"},{"location":"api_subpackages/mlxtend.file_io/","text":"mlxtend version: 0.23.1 find_filegroups find_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True, rstrip='', ignore_substring=None) Find and collect files from different directories in a python dictionary. Parameters paths : list Paths of the directories to be searched. Dictionary keys are build from the first directory. substring : str (default: '') Substring that all files have to contain to be considered. extensions : list (default: None) None or list of allowed file extensions for each path. If provided, the number of extensions must match the number of paths . validity_check : bool (default: None) If True , checks if all dictionary values have the same number of file paths. Prints a warning and returns an empty dictionary if the validity check failed. ignore_invisible : bool (default: True) If True , ignores invisible files (i.e., files starting with a period). rstrip : str (default: '') If provided, strips characters from right side of the file base names after splitting the extension. Useful to trim different filenames to a common stem. E.g,. \"abc_d.txt\" and \"abc_d_.csv\" would share the stem \"abc_d\" if rstrip is set to \"_\". ignore_substring : str (default: None) Ignores files that contain the specified substring. Returns groups : dict Dictionary of files paths. Keys are the file names found in the first directory listed in paths (without file extension). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/file_io/find_filegroups/ find_files find_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True, ignore_substring=None) Find files in a directory based on substring matching. Parameters substring : str Substring of the file to be matched. path : str Path where to look. recursive : bool If true, searches subdirectories recursively. check_ext : str If string (e.g., '.txt'), only returns files that match the specified file extension. ignore_invisible : bool If True , ignores invisible files (i.e., files starting with a period). ignore_substring : str Ignores files that contain the specified substring. Returns results : list List of the matched files. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/file_io/find_files/","title":"Mlxtend.file io"},{"location":"api_subpackages/mlxtend.file_io/#find_filegroups","text":"find_filegroups(paths, substring='', extensions=None, validity_check=True, ignore_invisible=True, rstrip='', ignore_substring=None) Find and collect files from different directories in a python dictionary. Parameters paths : list Paths of the directories to be searched. Dictionary keys are build from the first directory. substring : str (default: '') Substring that all files have to contain to be considered. extensions : list (default: None) None or list of allowed file extensions for each path. If provided, the number of extensions must match the number of paths . validity_check : bool (default: None) If True , checks if all dictionary values have the same number of file paths. Prints a warning and returns an empty dictionary if the validity check failed. ignore_invisible : bool (default: True) If True , ignores invisible files (i.e., files starting with a period). rstrip : str (default: '') If provided, strips characters from right side of the file base names after splitting the extension. Useful to trim different filenames to a common stem. E.g,. \"abc_d.txt\" and \"abc_d_.csv\" would share the stem \"abc_d\" if rstrip is set to \"_\". ignore_substring : str (default: None) Ignores files that contain the specified substring. Returns groups : dict Dictionary of files paths. Keys are the file names found in the first directory listed in paths (without file extension). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/file_io/find_filegroups/","title":"find_filegroups"},{"location":"api_subpackages/mlxtend.file_io/#find_files","text":"find_files(substring, path, recursive=False, check_ext=None, ignore_invisible=True, ignore_substring=None) Find files in a directory based on substring matching. Parameters substring : str Substring of the file to be matched. path : str Path where to look. recursive : bool If true, searches subdirectories recursively. check_ext : str If string (e.g., '.txt'), only returns files that match the specified file extension. ignore_invisible : bool If True , ignores invisible files (i.e., files starting with a period). ignore_substring : str Ignores files that contain the specified substring. Returns results : list List of the matched files. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/file_io/find_files/","title":"find_files"},{"location":"api_subpackages/mlxtend.frequent_patterns/","text":"mlxtend version: 0.23.1 apriori apriori(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0, low_memory=False) Get frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see (https://pandas.pydata.org/pandas-docs/stable/ user_guide/sparse.html#sparse-data-structures) Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minumum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions . use_colnames : bool (default: False) If True , uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Maximum length of the itemsets generated. If None (default) all possible itemsets lengths (under the apriori condition) are evaluated. verbose : int (default: 0) Shows the number of iterations if >= 1 and low_memory is True . If =1 and low_memory is False , shows the number of combinations. low_memory : bool (default: False) If True , uses an iterator to search for combinations above min_support . Note that while low_memory=True should only be used for large dataset if memory resources are limited, because this implementation is approx. 3-6x slower than the default. Returns pandas DataFrame with columns ['support', 'itemsets'] of all itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/ association_rules association_rules(df, metric='confidence', min_threshold=0.8, support_only=False) Generates a DataFrame of association rules including the metrics 'score', 'confidence', and 'lift' Parameters df : pandas DataFrame pandas DataFrame of frequent itemsets with columns ['support', 'itemsets'] metric : string (default: 'confidence') Metric to evaluate if a rule is of interest. Automatically set to 'support' if support_only=True . Otherwise, supported metrics are 'support', 'confidence', 'lift', 'leverage', 'conviction' and 'zhangs_metric' These metrics are computed as follows: - support(A->C) = support(A+C) [aka 'support'], range: [0, 1] - confidence(A->C) = support(A+C) / support(A), range: [0, 1] - lift(A->C) = confidence(A->C) / support(C), range: [0, inf] - leverage(A->C) = support(A->C) - support(A)*support(C), range: [-1, 1] - conviction = [1 - support(C)] / [1 - confidence(A->C)], range: [0, inf] - zhangs_metric(A->C) = leverage(A->C) / max(support(A->C)*(1-support(A)), support(A)*(support(C)-support(A->C))) range: [-1,1] min_threshold : float (default: 0.8) Minimal threshold for the evaluation metric, via the metric parameter, to decide whether a candidate rule is of interest. support_only : bool (default: False) Only computes the rule support and fills the other metric columns with NaNs. This is useful if: a) the input DataFrame is incomplete, e.g., does not contain support values for all rule antecedents and consequents b) you simply want to speed up the computation because you don't need the other metrics. Returns pandas DataFrame with columns \"antecedents\" and \"consequents\" that store itemsets, plus the scoring metric columns: \"antecedent support\", \"consequent support\", \"support\", \"confidence\", \"lift\", \"leverage\", \"conviction\" of all rules for which metric(rule) >= min_threshold. Each entry in the \"antecedents\" and \"consequents\" columns are of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/ fpgrowth fpgrowth(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0) Get frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#sparse-data-structures. Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minimum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions. use_colnames : bool (default: False) If true, uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Maximum length of the itemsets generated. If None (default) all possible itemsets lengths are evaluated. verbose : int (default: 0) Shows the stages of conditional tree generation. Returns pandas DataFrame with columns ['support', 'itemsets'] of all itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpgrowth/ fpmax fpmax(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0) Get maximal frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see (https://pandas.pydata.org/pandas-docs/stable/ user_guide/sparse.html#sparse-data-structures) Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minimum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions. use_colnames : bool (default: False) If true, uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Given the set of all maximal itemsets, return those that are less than max_len . If None (default) all possible itemsets lengths are evaluated. verbose : int (default: 0) Shows the stages of conditional tree generation. Returns pandas DataFrame with columns ['support', 'itemsets'] of all maximal itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpmax/ hmine hmine(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0) -> pandas.core.frame.DataFrame Get frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#sparse-data-structures. Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minimum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions. use_colnames : bool (default: False) If true, uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Maximum length of the itemsets generated. If None (default) all possible itemsets lengths are evaluated. verbose : int (default: 0) Shows the stages of conditional tree generation. Returns pandas DataFrame with columns ['support', 'itemsets'] of all itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/hmine/","title":"Mlxtend.frequent patterns"},{"location":"api_subpackages/mlxtend.frequent_patterns/#apriori","text":"apriori(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0, low_memory=False) Get frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see (https://pandas.pydata.org/pandas-docs/stable/ user_guide/sparse.html#sparse-data-structures) Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minumum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions . use_colnames : bool (default: False) If True , uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Maximum length of the itemsets generated. If None (default) all possible itemsets lengths (under the apriori condition) are evaluated. verbose : int (default: 0) Shows the number of iterations if >= 1 and low_memory is True . If =1 and low_memory is False , shows the number of combinations. low_memory : bool (default: False) If True , uses an iterator to search for combinations above min_support . Note that while low_memory=True should only be used for large dataset if memory resources are limited, because this implementation is approx. 3-6x slower than the default. Returns pandas DataFrame with columns ['support', 'itemsets'] of all itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/","title":"apriori"},{"location":"api_subpackages/mlxtend.frequent_patterns/#association_rules","text":"association_rules(df, metric='confidence', min_threshold=0.8, support_only=False) Generates a DataFrame of association rules including the metrics 'score', 'confidence', and 'lift' Parameters df : pandas DataFrame pandas DataFrame of frequent itemsets with columns ['support', 'itemsets'] metric : string (default: 'confidence') Metric to evaluate if a rule is of interest. Automatically set to 'support' if support_only=True . Otherwise, supported metrics are 'support', 'confidence', 'lift', 'leverage', 'conviction' and 'zhangs_metric' These metrics are computed as follows: - support(A->C) = support(A+C) [aka 'support'], range: [0, 1] - confidence(A->C) = support(A+C) / support(A), range: [0, 1] - lift(A->C) = confidence(A->C) / support(C), range: [0, inf] - leverage(A->C) = support(A->C) - support(A)*support(C), range: [-1, 1] - conviction = [1 - support(C)] / [1 - confidence(A->C)], range: [0, inf] - zhangs_metric(A->C) = leverage(A->C) / max(support(A->C)*(1-support(A)), support(A)*(support(C)-support(A->C))) range: [-1,1] min_threshold : float (default: 0.8) Minimal threshold for the evaluation metric, via the metric parameter, to decide whether a candidate rule is of interest. support_only : bool (default: False) Only computes the rule support and fills the other metric columns with NaNs. This is useful if: a) the input DataFrame is incomplete, e.g., does not contain support values for all rule antecedents and consequents b) you simply want to speed up the computation because you don't need the other metrics. Returns pandas DataFrame with columns \"antecedents\" and \"consequents\" that store itemsets, plus the scoring metric columns: \"antecedent support\", \"consequent support\", \"support\", \"confidence\", \"lift\", \"leverage\", \"conviction\" of all rules for which metric(rule) >= min_threshold. Each entry in the \"antecedents\" and \"consequents\" columns are of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/","title":"association_rules"},{"location":"api_subpackages/mlxtend.frequent_patterns/#fpgrowth","text":"fpgrowth(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0) Get frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#sparse-data-structures. Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minimum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions. use_colnames : bool (default: False) If true, uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Maximum length of the itemsets generated. If None (default) all possible itemsets lengths are evaluated. verbose : int (default: 0) Shows the stages of conditional tree generation. Returns pandas DataFrame with columns ['support', 'itemsets'] of all itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpgrowth/","title":"fpgrowth"},{"location":"api_subpackages/mlxtend.frequent_patterns/#fpmax","text":"fpmax(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0) Get maximal frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see (https://pandas.pydata.org/pandas-docs/stable/ user_guide/sparse.html#sparse-data-structures) Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minimum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions. use_colnames : bool (default: False) If true, uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Given the set of all maximal itemsets, return those that are less than max_len . If None (default) all possible itemsets lengths are evaluated. verbose : int (default: 0) Shows the stages of conditional tree generation. Returns pandas DataFrame with columns ['support', 'itemsets'] of all maximal itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpmax/","title":"fpmax"},{"location":"api_subpackages/mlxtend.frequent_patterns/#hmine","text":"hmine(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0) -> pandas.core.frame.DataFrame Get frequent itemsets from a one-hot DataFrame Parameters df : pandas DataFrame pandas DataFrame the encoded format. Also supports DataFrames with sparse data; for more info, please see https://pandas.pydata.org/pandas-docs/stable/user_guide/sparse.html#sparse-data-structures. Please note that the old pandas SparseDataFrame format is no longer supported in mlxtend >= 0.17.2. The allowed values are either 0/1 or True/False. For example, Apple Bananas Beer Chicken Milk Rice 0 True False True True False True 1 True False True False False True 2 True False True False False False 3 True True False False False False 4 False False True True True True 5 False False True False True True 6 False False True False True False 7 True True False False False False min_support : float (default: 0.5) A float between 0 and 1 for minimum support of the itemsets returned. The support is computed as the fraction transactions_where_item(s)_occur / total_transactions. use_colnames : bool (default: False) If true, uses the DataFrames' column names in the returned DataFrame instead of column indices. max_len : int (default: None) Maximum length of the itemsets generated. If None (default) all possible itemsets lengths are evaluated. verbose : int (default: 0) Shows the stages of conditional tree generation. Returns pandas DataFrame with columns ['support', 'itemsets'] of all itemsets that are >= min_support and < than max_len (if max_len is not None). Each itemset in the 'itemsets' column is of type frozenset , which is a Python built-in type that behaves similarly to sets except that it is immutable (For more info, see https://docs.python.org/3.6/library/stdtypes.html#frozenset). Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/hmine/","title":"hmine"},{"location":"api_subpackages/mlxtend.math/","text":"mlxtend version: 0.23.1 factorial factorial(n) None num_combinations num_combinations(n, k, with_replacement=False) Function to calculate the number of possible combinations. Parameters n : int Total number of items. k : int Number of elements of the target itemset. with_replacement : bool (default: False) Allows repeated elements if True. Returns comb : int Number of possible combinations. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/num_combinations/ num_permutations num_permutations(n, k, with_replacement=False) Function to calculate the number of possible permutations. Parameters n : int Total number of items. k : int Number of elements of the target itemset. with_replacement : bool Allows repeated elements if True. Returns permut : int Number of possible permutations. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/num_permutations/ vectorspace_dimensionality vectorspace_dimensionality(ary) Computes the hyper-volume spanned by a vector set Parameters ary : array-like, shape=[num_vectors, num_vectors] An orthogonal set of vectors (arranged as columns in a matrix) Returns dimensions : int An integer indicating the \"dimensionality\" hyper-volume spanned by the vector set Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/vectorspace_dimensionality/ vectorspace_orthonormalization vectorspace_orthonormalization(ary, eps=1e-13) Transforms a set of column vectors to a orthonormal basis. Given a set of orthogonal vectors, this functions converts such column vectors, arranged in a matrix, into orthonormal basis vectors. Parameters ary : array-like, shape=[num_vectors, num_vectors] An orthogonal set of vectors (arranged as columns in a matrix) eps : float (default: 1e-13) A small tolerance value to determine whether the vector norm is zero or not. Returns arr : array-like, shape=[num_vectors, num_vectors] An orthonormal set of vectors (arranged as columns) Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/vectorspace_orthonormalization/","title":"Mlxtend.math"},{"location":"api_subpackages/mlxtend.math/#factorial","text":"factorial(n) None","title":"factorial"},{"location":"api_subpackages/mlxtend.math/#num_combinations","text":"num_combinations(n, k, with_replacement=False) Function to calculate the number of possible combinations. Parameters n : int Total number of items. k : int Number of elements of the target itemset. with_replacement : bool (default: False) Allows repeated elements if True. Returns comb : int Number of possible combinations. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/num_combinations/","title":"num_combinations"},{"location":"api_subpackages/mlxtend.math/#num_permutations","text":"num_permutations(n, k, with_replacement=False) Function to calculate the number of possible permutations. Parameters n : int Total number of items. k : int Number of elements of the target itemset. with_replacement : bool Allows repeated elements if True. Returns permut : int Number of possible permutations. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/num_permutations/","title":"num_permutations"},{"location":"api_subpackages/mlxtend.math/#vectorspace_dimensionality","text":"vectorspace_dimensionality(ary) Computes the hyper-volume spanned by a vector set Parameters ary : array-like, shape=[num_vectors, num_vectors] An orthogonal set of vectors (arranged as columns in a matrix) Returns dimensions : int An integer indicating the \"dimensionality\" hyper-volume spanned by the vector set Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/vectorspace_dimensionality/","title":"vectorspace_dimensionality"},{"location":"api_subpackages/mlxtend.math/#vectorspace_orthonormalization","text":"vectorspace_orthonormalization(ary, eps=1e-13) Transforms a set of column vectors to a orthonormal basis. Given a set of orthogonal vectors, this functions converts such column vectors, arranged in a matrix, into orthonormal basis vectors. Parameters ary : array-like, shape=[num_vectors, num_vectors] An orthogonal set of vectors (arranged as columns in a matrix) eps : float (default: 1e-13) A small tolerance value to determine whether the vector norm is zero or not. Returns arr : array-like, shape=[num_vectors, num_vectors] An orthonormal set of vectors (arranged as columns) Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/math/vectorspace_orthonormalization/","title":"vectorspace_orthonormalization"},{"location":"api_subpackages/mlxtend.plotting/","text":"mlxtend version: 0.23.1 category_scatter category_scatter(x, y, label_col, data, markers='sxo^v', colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'), alpha=0.7, markersize=20.0, legend_loc='best') Scatter plot to plot categories in different colors/markerstyles. Parameters x : str or int DataFrame column name of the x-axis values or integer for the numpy ndarray column index. y : str DataFrame column name of the y-axis values or integer for the numpy ndarray column index data : Pandas DataFrame object or NumPy ndarray. markers : str Markers that are cycled through the label category. colors : tuple Colors that are cycled through the label category. alpha : float (default: 0.7) Parameter to control the transparency. markersize : float (default` : 20.0) Parameter to control the marker size. legend_loc : str (default: 'best') Location of the plot legend {best, upper left, upper right, lower left, lower right} No legend if legend_loc=False Returns fig : matplotlig.pyplot figure object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/category_scatter/ checkerboard_plot checkerboard_plot(ary, cell_colors=('white', 'black'), font_colors=('black', 'white'), fmt='%.1f', figsize=None, row_labels=None, col_labels=None, fontsize=None) Plot a checkerboard table / heatmap via matplotlib. Parameters ary : array-like, shape = [n, m] A 2D Nnumpy array. cell_colors : tuple or list (default: ('white', 'black')) Tuple or list containing the two colors of the checkerboard pattern. font_colors : tuple or list (default: ('black', 'white')) Font colors corresponding to the cell colors. figsize : tuple (default: (2.5, 2.5)) Height and width of the figure fmt : str (default: '%.1f') Python string formatter for cell values. The default '%.1f' results in floats with 1 digit after the decimal point. Use '%d' to show numbers as integers. row_labels : list (default: None) List of the row labels. Uses the array row indices 0 to n by default. col_labels : list (default: None) List of the column labels. Uses the array column indices 0 to m by default. fontsize : int (default: None) Specifies the font size of the checkerboard table. Uses matplotlib's default if None. Returns fig : matplotlib Figure object. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/checkerboard_plot/ ecdf ecdf(x, y_label='ECDF', x_label=None, ax=None, percentile=None, ecdf_color=None, ecdf_marker='o', percentile_color='black', percentile_linestyle='--') Plots an Empirical Cumulative Distribution Function Parameters x : array or list, shape=[n_samples,] Array-like object containing the feature values y_label : str (default='ECDF') Text label for the y-axis x_label : str (default=None) Text label for the x-axis ax : matplotlib.axes.Axes (default: None) An existing matplotlib Axes. Creates one if ax=None percentile : float (default=None) Float between 0 and 1 for plotting a percentile threshold line ecdf_color : matplotlib color (default=None) Color for the ECDF plot; uses matplotlib defaults if None ecdf_marker : matplotlib marker (default='o') Marker style for the ECDF plot percentile_color : matplotlib color (default='black') Color for the percentile threshold if percentile is not None percentile_linestyle : matplotlib linestyle (default='--') Line style for the percentile threshold if percentile is not None Returns ax : matplotlib.axes.Axes object percentile_threshold : float Feature threshold at the percentile or None if percentile=None percentile_count : Number of if percentile is not None Number of samples that have a feature less or equal than the feature threshold at a percentile threshold or None if percentile=None Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/ecdf/ enrichment_plot enrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2, where='post', grid=True, count_label='Count', xlim='auto', ylim='auto', invert_axes=False, legend_loc='best', ax=None) Plot stacked barplots Parameters df : pandas.DataFrame A pandas DataFrame where columns represent the different categories. colors: str (default: 'bgrcky') The colors of the bars. markers : str (default: ' ') Matplotlib markerstyles, e.g, 'sov' for square,circle, and triangle markers. linestyles : str (default: '-') Matplotlib linestyles, e.g., '-,--' to cycle normal and dashed lines. Note that the different linestyles need to be separated by commas. alpha : float (default: 0.5) Transparency level from 0.0 to 1.0. lw : int or float (default: 2) Linewidth parameter. where : {'post', 'pre', 'mid'} (default: 'post') Starting location of the steps. grid : bool (default: True ) Plots a grid if True. count_label : str (default: 'Count') Label for the \"Count\"-axis. xlim : 'auto' or array-like [min, max] (default: 'auto') Min and maximum position of the x-axis range. ylim : 'auto' or array-like [min, max] (default: 'auto') Min and maximum position of the y-axis range. invert_axes : bool (default: False) Plots count on the x-axis if True. legend_loc : str (default: 'best') Location of the plot legend {best, upper left, upper right, lower left, lower right} No legend if legend_loc=False ax : matplotlib axis, optional (default: None) Use this axis for plotting or make a new one otherwise Returns ax : matplotlib axis Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/enrichment_plot/ heatmap heatmap(matrix, hide_spines=False, hide_ticks=False, figsize=None, cmap=None, colorbar=True, row_names=None, column_names=None, column_name_rotation=45, cell_values=True, cell_fmt='.2f', cell_font_size=None, text_color_threshold=None) Plot a heatmap via matplotlib. Parameters conf_mat : array-like, shape = [n_rows, n_columns] And arbitrary 2D array. hide_spines : bool (default: False) Hides axis spines if True. hide_ticks : bool (default: False) Hides axis ticks if True figsize : tuple (default: (2.5, 2.5)) Height and width of the figure cmap : matplotlib colormap (default: None ) Uses matplotlib.pyplot.cm.viridis if None colorbar : bool (default: True) Shows a colorbar if True row_names : array-like, shape = [n_rows] (default: None) List of row names to be used as y-axis tick labels. column_names : array-like, shape = [n_columns] (default: None) List of column names to be used as x-axis tick labels. column_name_rotation : int (default: 45) Number of degrees for rotating column x-tick labels. cell_values : bool (default: True) Plots cell values if True. cell_fmt : string (default: '.2f') Format specification for cell values (if cell_values=True ) cell_font_size : int (default: None) Font size for cell values (if cell_values=True ) text_color_threshold : float (default: None) Threshold for the black/white text threshold of the text annotation. Default (None) tried to infer a good threshold automatically using np.max(normed_matrix) / 2 . Returns fig, ax : matplotlib.pyplot subplot objects Figure and axis elements of the subplot. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/heatmap/ plot_confusion_matrix plot_confusion_matrix(conf_mat, hide_spines=False, hide_ticks=False, figsize=None, cmap=None, colorbar=False, show_absolute=True, show_normed=False, norm_colormap=None, class_names=None, figure=None, axis=None, fontcolor_threshold=0.5) Plot a confusion matrix via matplotlib. Parameters conf_mat : array-like, shape = [n_classes, n_classes] Confusion matrix from evaluate.confusion matrix. hide_spines : bool (default: False) Hides axis spines if True. hide_ticks : bool (default: False) Hides axis ticks if True figsize : tuple (default: (2.5, 2.5)) Height and width of the figure cmap : matplotlib colormap (default: None ) Uses matplotlib.pyplot.cm.Blues if None colorbar : bool (default: False) Shows a colorbar if True show_absolute : bool (default: True) Shows absolute confusion matrix coefficients if True. At least one of show_absolute or show_normed must be True. show_normed : bool (default: False) Shows normed confusion matrix coefficients if True. The normed confusion matrix coefficients give the proportion of training examples per class that are assigned the correct label. At least one of show_absolute or show_normed must be True. norm_colormap : bool (default: False) Matplotlib color normalization object to normalize the color scale, e.g., matplotlib.colors.LogNorm() . class_names : array-like, shape = [n_classes] (default: None) List of class names. If not None , ticks will be set to these values. figure : None or Matplotlib figure (default: None) If None will create a new figure. axis : None or Matplotlib figure axis (default: None) If None will create a new axis. fontcolor_threshold : Float (default: 0.5) Sets a threshold for choosing black and white font colors for the cells. By default all values larger than 0.5 times the maximum cell value are converted to white, and everything equal or smaller than 0.5 times the maximum cell value are converted to black. Returns fig, ax : matplotlib.pyplot subplot objects Figure and axis elements of the subplot. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/ plot_decision_regions plot_decision_regions(X, y, clf, feature_index=None, filler_feature_values=None, filler_feature_ranges=None, ax=None, X_highlight=None, zoom_factor=1.0, legend=1, hide_spines=True, markers='s^oxv<>', colors='#1f77b4,#ff7f0e,#3ca02c,#d62728,#9467bd,#8c564b,#e377c2,#7f7f7f,#bcbd22,#17becf', scatter_kwargs=None, contourf_kwargs=None, contour_kwargs=None, scatter_highlight_kwargs=None, n_jobs=None) Plot decision regions of a classifier. Please note that this functions assumes that class labels are labeled consecutively, e.g,. 0, 1, 2, 3, 4, and 5. If you have class labels with integer labels > 4, you may want to provide additional colors and/or markers as `colors` and `markers` arguments. See https://matplotlib.org/examples/color/named_colors.html for more information. Parameters X : array-like, shape = [n_samples, n_features] Feature Matrix. y : array-like, shape = [n_samples] True class labels. clf : Classifier object. Must have a .predict method. feature_index : array-like (default: (0,) for 1D, (0, 1) otherwise) Feature indices to use for plotting. The first index in feature_index will be on the x-axis, the second index will be on the y-axis. filler_feature_values : dict (default: None) Only needed for number features > 2. Dictionary of feature index-value pairs for the features not being plotted. filler_feature_ranges : dict (default: None) Only needed for number features > 2. Dictionary of feature index-value pairs for the features not being plotted. Will use the ranges provided to select training samples for plotting. ax : matplotlib.axes.Axes (default: None) An existing matplotlib Axes. Creates one if ax=None. X_highlight : array-like, shape = [n_samples, n_features] (default: None) An array with data points that are used to highlight samples in X . zoom_factor : float (default: 1.0) Controls the scale of the x- and y-axis of the decision plot. hide_spines : bool (default: True) Hide axis spines if True. legend : int (default: 1) Integer to specify the legend location. No legend if legend is 0. markers : str (default: 's^oxv<>') Scatterplot markers. colors : str (default: 'red,blue,limegreen,gray,cyan') Comma separated list of colors. scatter_kwargs : dict (default: None) Keyword arguments for underlying matplotlib scatter function. contourf_kwargs : dict (default: None) Keyword arguments for underlying matplotlib contourf function. contour_kwargs : dict (default: None) Keyword arguments for underlying matplotlib contour function (which draws the lines between decision regions). scatter_highlight_kwargs : dict (default: None) Keyword arguments for underlying matplotlib scatter function. n_jobs : int or None, optional (default=None) The number of CPUs to use to do the computation using Python's multiprocessing library. None means 1. -1 means using all processors. New in v0.22.0. Returns ax : matplotlib.axes.Axes object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/ plot_learning_curves plot_learning_curves(X_train, y_train, X_test, y_test, clf, train_marker='o', test_marker='^', scoring='misclassification error', suppress_plot=False, print_model=True, title_fontsize=12, style='default', legend_loc='best') Plots learning curves of a classifier. Parameters X_train : array-like, shape = [n_samples, n_features] Feature matrix of the training dataset. y_train : array-like, shape = [n_samples] True class labels of the training dataset. X_test : array-like, shape = [n_samples, n_features] Feature matrix of the test dataset. y_test : array-like, shape = [n_samples] True class labels of the test dataset. clf : Classifier object. Must have a .predict .fit method. train_marker : str (default: 'o') Marker for the training set line plot. test_marker : str (default: '^') Marker for the test set line plot. scoring : str (default: 'misclassification error') If not 'misclassification error', accepts the following metrics (from scikit-learn): {'accuracy', 'average_precision', 'f1_micro', 'f1_macro', 'f1_weighted', 'f1_samples', 'log_loss', 'precision', 'recall', 'roc_auc', 'adjusted_rand_score', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'r2'} suppress_plot=False : bool (default: False) Suppress matplotlib plots if True. Recommended for testing purposes. print_model : bool (default: True) Print model parameters in plot title if True. title_fontsize : int (default: 12) Determines the size of the plot title font. style : str (default: 'default') Matplotlib style. For more styles, please see https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html legend_loc : str (default: 'best') Where to place the plot legend: {'best', 'upper left', 'upper right', 'lower left', 'lower right'} Returns errors : (training_error, test_error): tuple of lists Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_learning_curves/ plot_linear_regression plot_linear_regression(X, y, model=LinearRegression(), corr_func='pearsonr', scattercolor='blue', fit_style='k--', legend=True, xlim='auto') Plot a linear regression line fit. Parameters X : numpy array, shape = [n_samples,] Samples. y : numpy array, shape (n_samples,) Target values model: object (default: sklearn.linear_model.LinearRegression) Estimator object for regression. Must implement a .fit() and .predict() method. corr_func: str or function (default: 'pearsonr') Uses pearsonr from scipy.stats if corr_func='pearsonr'. to compute the regression slope. If not 'pearsonr', the corr_func , the corr_func parameter expects a function of the form func( , ) as inputs, which is expected to return a tuple (<correlation_coefficient>, <some_unused_value>) . scattercolor: string (default: blue) Color of scatter plot points. fit_style: string (default: k--) Style for the line fit. legend: bool (default: True) Plots legend with corr_coeff coef., fit coef., and intercept values. xlim: array-like (x_min, x_max) or 'auto' (default: 'auto') X-axis limits for the linear line fit. Returns regression_fit : tuple intercept, slope, corr_coeff (float, float, float) Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_linear_regression/ plot_pca_correlation_graph plot_pca_correlation_graph(X, variables_names, dimensions=(1, 2), figure_axis_size=6, X_pca=None, explained_variance=None) Compute the PCA for X and plots the Correlation graph Parameters X : 2d array like. The columns represent the different variables and the rows are the samples of thos variables variables_names : array like Name of the columns (the variables) of X dimensions: tuple with two elements. dimensions to be plotted (x,y) figure_axis_size : size of the final frame. The figure created is a square with length and width equal to figure_axis_size. X_pca : np.ndarray, shape = [n_samples, n_components]. Optional. X_pca is the matrix of the transformed components from X. If not provided, the function computes PCA automatically using mlxtend.feature_extraction.PrincipalComponentAnalysis Expected n_componentes >= max(dimensions) explained_variance : 1 dimension np.ndarray, length = n_components Optional. explained_variance are the eigenvalues from the diagonalized covariance matrix on the PCA transformatiopn. If not provided, the function computes PCA independently Expected n_componentes == X.shape[1] Returns matplotlib_figure, correlation_matrix Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_pca_correlation_graph/ plot_sequential_feature_selection plot_sequential_feature_selection(metric_dict, figsize=None, kind='std_dev', color='blue', bcolor='steelblue', marker='o', alpha=0.2, ylabel='Performance', confidence_interval=0.95) Plot feature selection results. Parameters metric_dict : mlxtend.SequentialFeatureSelector.get_metric_dict() object figsize : tuple (default: None) Height and width of the figure kind : str (default: \"std_dev\") The kind of error bar or confidence interval in {'std_dev', 'std_err', 'ci', None}. color : str (default: \"blue\") Color of the lineplot (accepts any matplotlib color name) bcolor : str (default: \"steelblue\"). Color of the error bars / confidence intervals (accepts any matplotlib color name). marker : str (default: \"o\") Marker of the line plot (accepts any matplotlib marker name). alpha : float in [0, 1] (default: 0.2) Transparency of the error bars / confidence intervals. ylabel : str (default: \"Performance\") Y-axis label. confidence_interval : float (default: 0.95) Confidence level if kind='ci' . Returns fig : matplotlib.pyplot.figure() object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_sequential_feature_selection/ remove_borders remove_borders(axes, left=False, bottom=False, right=True, top=True) Remove chart junk from matplotlib plots. Parameters axes : iterable An iterable containing plt.gca() or plt.subplot() objects, e.g. [plt.gca()]. left : bool (default: False ) Hide left axis spine if True. bottom : bool (default: False ) Hide bottom axis spine if True. right : bool (default: True ) Hide right axis spine if True. top : bool (default: True ) Hide top axis spine if True. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/remove_chartjunk/ scatter_hist scatter_hist(x, y, xlabel=None, ylabel=None, figsize=(5, 5)) Scatter plot and individual feature histograms along axes. Parameters x : 1D array-like or Pandas Series X-axis values. y : 1D array-like or Pandas Series Y-axis values. xlabel : str (default: None ) Label for the X-axis values. If x is a pandas Series, and xlabel is None , the label is inferred automatically. ylabel : str (default: None ) Label for the X-axis values. If y is a pandas Series, and ylabel is None , the label is inferred automatically. figsize : tuple (default: (5, 5) ) Matplotlib figure size. Returns plot : Matplotlib Figure object scatterplotmatrix scatterplotmatrix(X, fig_axes=None, names=None, figsize=(8, 8), alpha=1.0, kwargs) Lower triangular of a scatterplot matrix Parameters X : array-like, shape={num_examples, num_features} Design matrix containing data instances (examples) with multiple exploratory variables (features). fix_axes : tuple (default: None) A (fig, axes) tuple, where fig is an figure object and axes is an axes object created via matplotlib, for example, by calling the pyplot subplot function fig, axes = plt.subplots(...) names : list (default: None) A list of string names, which should have the same number of elements as there are features (columns) in X . figsize : tuple (default: (8, 8)) Height and width of the subplot grid. Ignored if fig_axes is not None . alpha : float (default: 1.0) Transparency for both the scatter plots and the histograms along the diagonal. **kwargs : kwargs Keyword arguments for the scatterplots. Returns fix_axes : tuple A (fig, axes) tuple, where fig is an figure object and axes is an axes object created via matplotlib, for example, by calling the pyplot subplot function fig, axes = plt.subplots(...) Examples For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/scatterplotmatrix/ stacked_barplot stacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend_loc='best') Function to plot stacked barplots Parameters df : pandas.DataFrame A pandas DataFrame where the index denotes the x-axis labels, and the columns contain the different measurements for each row. bar_width: 'auto' or float (default: 'auto') Parameter to set the widths of the bars. if 'auto', the width is automatically determined by the number of columns in the dataset. colors: str (default: 'bgrcky') The colors of the bars. labels: 'index' or iterable (default: 'index') If 'index', the DataFrame index will be used as x-tick labels. rotation: int (default: 90) Parameter to rotate the x-axis labels. legend_loc : str (default: 'best') Location of the plot legend {best, upper left, upper right, lower left, lower right} No legend if legend_loc=False Returns fig : matplotlib.pyplot figure object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/stacked_barplot/","title":"Mlxtend.plotting"},{"location":"api_subpackages/mlxtend.plotting/#category_scatter","text":"category_scatter(x, y, label_col, data, markers='sxo^v', colors=('blue', 'green', 'red', 'purple', 'gray', 'cyan'), alpha=0.7, markersize=20.0, legend_loc='best') Scatter plot to plot categories in different colors/markerstyles. Parameters x : str or int DataFrame column name of the x-axis values or integer for the numpy ndarray column index. y : str DataFrame column name of the y-axis values or integer for the numpy ndarray column index data : Pandas DataFrame object or NumPy ndarray. markers : str Markers that are cycled through the label category. colors : tuple Colors that are cycled through the label category. alpha : float (default: 0.7) Parameter to control the transparency. markersize : float (default` : 20.0) Parameter to control the marker size. legend_loc : str (default: 'best') Location of the plot legend {best, upper left, upper right, lower left, lower right} No legend if legend_loc=False Returns fig : matplotlig.pyplot figure object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/category_scatter/","title":"category_scatter"},{"location":"api_subpackages/mlxtend.plotting/#checkerboard_plot","text":"checkerboard_plot(ary, cell_colors=('white', 'black'), font_colors=('black', 'white'), fmt='%.1f', figsize=None, row_labels=None, col_labels=None, fontsize=None) Plot a checkerboard table / heatmap via matplotlib. Parameters ary : array-like, shape = [n, m] A 2D Nnumpy array. cell_colors : tuple or list (default: ('white', 'black')) Tuple or list containing the two colors of the checkerboard pattern. font_colors : tuple or list (default: ('black', 'white')) Font colors corresponding to the cell colors. figsize : tuple (default: (2.5, 2.5)) Height and width of the figure fmt : str (default: '%.1f') Python string formatter for cell values. The default '%.1f' results in floats with 1 digit after the decimal point. Use '%d' to show numbers as integers. row_labels : list (default: None) List of the row labels. Uses the array row indices 0 to n by default. col_labels : list (default: None) List of the column labels. Uses the array column indices 0 to m by default. fontsize : int (default: None) Specifies the font size of the checkerboard table. Uses matplotlib's default if None. Returns fig : matplotlib Figure object. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/checkerboard_plot/","title":"checkerboard_plot"},{"location":"api_subpackages/mlxtend.plotting/#ecdf","text":"ecdf(x, y_label='ECDF', x_label=None, ax=None, percentile=None, ecdf_color=None, ecdf_marker='o', percentile_color='black', percentile_linestyle='--') Plots an Empirical Cumulative Distribution Function Parameters x : array or list, shape=[n_samples,] Array-like object containing the feature values y_label : str (default='ECDF') Text label for the y-axis x_label : str (default=None) Text label for the x-axis ax : matplotlib.axes.Axes (default: None) An existing matplotlib Axes. Creates one if ax=None percentile : float (default=None) Float between 0 and 1 for plotting a percentile threshold line ecdf_color : matplotlib color (default=None) Color for the ECDF plot; uses matplotlib defaults if None ecdf_marker : matplotlib marker (default='o') Marker style for the ECDF plot percentile_color : matplotlib color (default='black') Color for the percentile threshold if percentile is not None percentile_linestyle : matplotlib linestyle (default='--') Line style for the percentile threshold if percentile is not None Returns ax : matplotlib.axes.Axes object percentile_threshold : float Feature threshold at the percentile or None if percentile=None percentile_count : Number of if percentile is not None Number of samples that have a feature less or equal than the feature threshold at a percentile threshold or None if percentile=None Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/ecdf/","title":"ecdf"},{"location":"api_subpackages/mlxtend.plotting/#enrichment_plot","text":"enrichment_plot(df, colors='bgrkcy', markers=' ', linestyles='-', alpha=0.5, lw=2, where='post', grid=True, count_label='Count', xlim='auto', ylim='auto', invert_axes=False, legend_loc='best', ax=None) Plot stacked barplots Parameters df : pandas.DataFrame A pandas DataFrame where columns represent the different categories. colors: str (default: 'bgrcky') The colors of the bars. markers : str (default: ' ') Matplotlib markerstyles, e.g, 'sov' for square,circle, and triangle markers. linestyles : str (default: '-') Matplotlib linestyles, e.g., '-,--' to cycle normal and dashed lines. Note that the different linestyles need to be separated by commas. alpha : float (default: 0.5) Transparency level from 0.0 to 1.0. lw : int or float (default: 2) Linewidth parameter. where : {'post', 'pre', 'mid'} (default: 'post') Starting location of the steps. grid : bool (default: True ) Plots a grid if True. count_label : str (default: 'Count') Label for the \"Count\"-axis. xlim : 'auto' or array-like [min, max] (default: 'auto') Min and maximum position of the x-axis range. ylim : 'auto' or array-like [min, max] (default: 'auto') Min and maximum position of the y-axis range. invert_axes : bool (default: False) Plots count on the x-axis if True. legend_loc : str (default: 'best') Location of the plot legend {best, upper left, upper right, lower left, lower right} No legend if legend_loc=False ax : matplotlib axis, optional (default: None) Use this axis for plotting or make a new one otherwise Returns ax : matplotlib axis Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/enrichment_plot/","title":"enrichment_plot"},{"location":"api_subpackages/mlxtend.plotting/#heatmap","text":"heatmap(matrix, hide_spines=False, hide_ticks=False, figsize=None, cmap=None, colorbar=True, row_names=None, column_names=None, column_name_rotation=45, cell_values=True, cell_fmt='.2f', cell_font_size=None, text_color_threshold=None) Plot a heatmap via matplotlib. Parameters conf_mat : array-like, shape = [n_rows, n_columns] And arbitrary 2D array. hide_spines : bool (default: False) Hides axis spines if True. hide_ticks : bool (default: False) Hides axis ticks if True figsize : tuple (default: (2.5, 2.5)) Height and width of the figure cmap : matplotlib colormap (default: None ) Uses matplotlib.pyplot.cm.viridis if None colorbar : bool (default: True) Shows a colorbar if True row_names : array-like, shape = [n_rows] (default: None) List of row names to be used as y-axis tick labels. column_names : array-like, shape = [n_columns] (default: None) List of column names to be used as x-axis tick labels. column_name_rotation : int (default: 45) Number of degrees for rotating column x-tick labels. cell_values : bool (default: True) Plots cell values if True. cell_fmt : string (default: '.2f') Format specification for cell values (if cell_values=True ) cell_font_size : int (default: None) Font size for cell values (if cell_values=True ) text_color_threshold : float (default: None) Threshold for the black/white text threshold of the text annotation. Default (None) tried to infer a good threshold automatically using np.max(normed_matrix) / 2 . Returns fig, ax : matplotlib.pyplot subplot objects Figure and axis elements of the subplot. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/heatmap/","title":"heatmap"},{"location":"api_subpackages/mlxtend.plotting/#plot_confusion_matrix","text":"plot_confusion_matrix(conf_mat, hide_spines=False, hide_ticks=False, figsize=None, cmap=None, colorbar=False, show_absolute=True, show_normed=False, norm_colormap=None, class_names=None, figure=None, axis=None, fontcolor_threshold=0.5) Plot a confusion matrix via matplotlib. Parameters conf_mat : array-like, shape = [n_classes, n_classes] Confusion matrix from evaluate.confusion matrix. hide_spines : bool (default: False) Hides axis spines if True. hide_ticks : bool (default: False) Hides axis ticks if True figsize : tuple (default: (2.5, 2.5)) Height and width of the figure cmap : matplotlib colormap (default: None ) Uses matplotlib.pyplot.cm.Blues if None colorbar : bool (default: False) Shows a colorbar if True show_absolute : bool (default: True) Shows absolute confusion matrix coefficients if True. At least one of show_absolute or show_normed must be True. show_normed : bool (default: False) Shows normed confusion matrix coefficients if True. The normed confusion matrix coefficients give the proportion of training examples per class that are assigned the correct label. At least one of show_absolute or show_normed must be True. norm_colormap : bool (default: False) Matplotlib color normalization object to normalize the color scale, e.g., matplotlib.colors.LogNorm() . class_names : array-like, shape = [n_classes] (default: None) List of class names. If not None , ticks will be set to these values. figure : None or Matplotlib figure (default: None) If None will create a new figure. axis : None or Matplotlib figure axis (default: None) If None will create a new axis. fontcolor_threshold : Float (default: 0.5) Sets a threshold for choosing black and white font colors for the cells. By default all values larger than 0.5 times the maximum cell value are converted to white, and everything equal or smaller than 0.5 times the maximum cell value are converted to black. Returns fig, ax : matplotlib.pyplot subplot objects Figure and axis elements of the subplot. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_confusion_matrix/","title":"plot_confusion_matrix"},{"location":"api_subpackages/mlxtend.plotting/#plot_decision_regions","text":"plot_decision_regions(X, y, clf, feature_index=None, filler_feature_values=None, filler_feature_ranges=None, ax=None, X_highlight=None, zoom_factor=1.0, legend=1, hide_spines=True, markers='s^oxv<>', colors='#1f77b4,#ff7f0e,#3ca02c,#d62728,#9467bd,#8c564b,#e377c2,#7f7f7f,#bcbd22,#17becf', scatter_kwargs=None, contourf_kwargs=None, contour_kwargs=None, scatter_highlight_kwargs=None, n_jobs=None) Plot decision regions of a classifier. Please note that this functions assumes that class labels are labeled consecutively, e.g,. 0, 1, 2, 3, 4, and 5. If you have class labels with integer labels > 4, you may want to provide additional colors and/or markers as `colors` and `markers` arguments. See https://matplotlib.org/examples/color/named_colors.html for more information. Parameters X : array-like, shape = [n_samples, n_features] Feature Matrix. y : array-like, shape = [n_samples] True class labels. clf : Classifier object. Must have a .predict method. feature_index : array-like (default: (0,) for 1D, (0, 1) otherwise) Feature indices to use for plotting. The first index in feature_index will be on the x-axis, the second index will be on the y-axis. filler_feature_values : dict (default: None) Only needed for number features > 2. Dictionary of feature index-value pairs for the features not being plotted. filler_feature_ranges : dict (default: None) Only needed for number features > 2. Dictionary of feature index-value pairs for the features not being plotted. Will use the ranges provided to select training samples for plotting. ax : matplotlib.axes.Axes (default: None) An existing matplotlib Axes. Creates one if ax=None. X_highlight : array-like, shape = [n_samples, n_features] (default: None) An array with data points that are used to highlight samples in X . zoom_factor : float (default: 1.0) Controls the scale of the x- and y-axis of the decision plot. hide_spines : bool (default: True) Hide axis spines if True. legend : int (default: 1) Integer to specify the legend location. No legend if legend is 0. markers : str (default: 's^oxv<>') Scatterplot markers. colors : str (default: 'red,blue,limegreen,gray,cyan') Comma separated list of colors. scatter_kwargs : dict (default: None) Keyword arguments for underlying matplotlib scatter function. contourf_kwargs : dict (default: None) Keyword arguments for underlying matplotlib contourf function. contour_kwargs : dict (default: None) Keyword arguments for underlying matplotlib contour function (which draws the lines between decision regions). scatter_highlight_kwargs : dict (default: None) Keyword arguments for underlying matplotlib scatter function. n_jobs : int or None, optional (default=None) The number of CPUs to use to do the computation using Python's multiprocessing library. None means 1. -1 means using all processors. New in v0.22.0. Returns ax : matplotlib.axes.Axes object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/","title":"plot_decision_regions"},{"location":"api_subpackages/mlxtend.plotting/#plot_learning_curves","text":"plot_learning_curves(X_train, y_train, X_test, y_test, clf, train_marker='o', test_marker='^', scoring='misclassification error', suppress_plot=False, print_model=True, title_fontsize=12, style='default', legend_loc='best') Plots learning curves of a classifier. Parameters X_train : array-like, shape = [n_samples, n_features] Feature matrix of the training dataset. y_train : array-like, shape = [n_samples] True class labels of the training dataset. X_test : array-like, shape = [n_samples, n_features] Feature matrix of the test dataset. y_test : array-like, shape = [n_samples] True class labels of the test dataset. clf : Classifier object. Must have a .predict .fit method. train_marker : str (default: 'o') Marker for the training set line plot. test_marker : str (default: '^') Marker for the test set line plot. scoring : str (default: 'misclassification error') If not 'misclassification error', accepts the following metrics (from scikit-learn): {'accuracy', 'average_precision', 'f1_micro', 'f1_macro', 'f1_weighted', 'f1_samples', 'log_loss', 'precision', 'recall', 'roc_auc', 'adjusted_rand_score', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'r2'} suppress_plot=False : bool (default: False) Suppress matplotlib plots if True. Recommended for testing purposes. print_model : bool (default: True) Print model parameters in plot title if True. title_fontsize : int (default: 12) Determines the size of the plot title font. style : str (default: 'default') Matplotlib style. For more styles, please see https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html legend_loc : str (default: 'best') Where to place the plot legend: {'best', 'upper left', 'upper right', 'lower left', 'lower right'} Returns errors : (training_error, test_error): tuple of lists Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_learning_curves/","title":"plot_learning_curves"},{"location":"api_subpackages/mlxtend.plotting/#plot_linear_regression","text":"plot_linear_regression(X, y, model=LinearRegression(), corr_func='pearsonr', scattercolor='blue', fit_style='k--', legend=True, xlim='auto') Plot a linear regression line fit. Parameters X : numpy array, shape = [n_samples,] Samples. y : numpy array, shape (n_samples,) Target values model: object (default: sklearn.linear_model.LinearRegression) Estimator object for regression. Must implement a .fit() and .predict() method. corr_func: str or function (default: 'pearsonr') Uses pearsonr from scipy.stats if corr_func='pearsonr'. to compute the regression slope. If not 'pearsonr', the corr_func , the corr_func parameter expects a function of the form func( , ) as inputs, which is expected to return a tuple (<correlation_coefficient>, <some_unused_value>) . scattercolor: string (default: blue) Color of scatter plot points. fit_style: string (default: k--) Style for the line fit. legend: bool (default: True) Plots legend with corr_coeff coef., fit coef., and intercept values. xlim: array-like (x_min, x_max) or 'auto' (default: 'auto') X-axis limits for the linear line fit. Returns regression_fit : tuple intercept, slope, corr_coeff (float, float, float) Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_linear_regression/","title":"plot_linear_regression"},{"location":"api_subpackages/mlxtend.plotting/#plot_pca_correlation_graph","text":"plot_pca_correlation_graph(X, variables_names, dimensions=(1, 2), figure_axis_size=6, X_pca=None, explained_variance=None) Compute the PCA for X and plots the Correlation graph Parameters X : 2d array like. The columns represent the different variables and the rows are the samples of thos variables variables_names : array like Name of the columns (the variables) of X dimensions: tuple with two elements. dimensions to be plotted (x,y) figure_axis_size : size of the final frame. The figure created is a square with length and width equal to figure_axis_size. X_pca : np.ndarray, shape = [n_samples, n_components]. Optional. X_pca is the matrix of the transformed components from X. If not provided, the function computes PCA automatically using mlxtend.feature_extraction.PrincipalComponentAnalysis Expected n_componentes >= max(dimensions) explained_variance : 1 dimension np.ndarray, length = n_components Optional. explained_variance are the eigenvalues from the diagonalized covariance matrix on the PCA transformatiopn. If not provided, the function computes PCA independently Expected n_componentes == X.shape[1] Returns matplotlib_figure, correlation_matrix Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_pca_correlation_graph/","title":"plot_pca_correlation_graph"},{"location":"api_subpackages/mlxtend.plotting/#plot_sequential_feature_selection","text":"plot_sequential_feature_selection(metric_dict, figsize=None, kind='std_dev', color='blue', bcolor='steelblue', marker='o', alpha=0.2, ylabel='Performance', confidence_interval=0.95) Plot feature selection results. Parameters metric_dict : mlxtend.SequentialFeatureSelector.get_metric_dict() object figsize : tuple (default: None) Height and width of the figure kind : str (default: \"std_dev\") The kind of error bar or confidence interval in {'std_dev', 'std_err', 'ci', None}. color : str (default: \"blue\") Color of the lineplot (accepts any matplotlib color name) bcolor : str (default: \"steelblue\"). Color of the error bars / confidence intervals (accepts any matplotlib color name). marker : str (default: \"o\") Marker of the line plot (accepts any matplotlib marker name). alpha : float in [0, 1] (default: 0.2) Transparency of the error bars / confidence intervals. ylabel : str (default: \"Performance\") Y-axis label. confidence_interval : float (default: 0.95) Confidence level if kind='ci' . Returns fig : matplotlib.pyplot.figure() object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/plot_sequential_feature_selection/","title":"plot_sequential_feature_selection"},{"location":"api_subpackages/mlxtend.plotting/#remove_borders","text":"remove_borders(axes, left=False, bottom=False, right=True, top=True) Remove chart junk from matplotlib plots. Parameters axes : iterable An iterable containing plt.gca() or plt.subplot() objects, e.g. [plt.gca()]. left : bool (default: False ) Hide left axis spine if True. bottom : bool (default: False ) Hide bottom axis spine if True. right : bool (default: True ) Hide right axis spine if True. top : bool (default: True ) Hide top axis spine if True. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/remove_chartjunk/","title":"remove_borders"},{"location":"api_subpackages/mlxtend.plotting/#scatter_hist","text":"scatter_hist(x, y, xlabel=None, ylabel=None, figsize=(5, 5)) Scatter plot and individual feature histograms along axes. Parameters x : 1D array-like or Pandas Series X-axis values. y : 1D array-like or Pandas Series Y-axis values. xlabel : str (default: None ) Label for the X-axis values. If x is a pandas Series, and xlabel is None , the label is inferred automatically. ylabel : str (default: None ) Label for the X-axis values. If y is a pandas Series, and ylabel is None , the label is inferred automatically. figsize : tuple (default: (5, 5) ) Matplotlib figure size. Returns plot : Matplotlib Figure object","title":"scatter_hist"},{"location":"api_subpackages/mlxtend.plotting/#scatterplotmatrix","text":"scatterplotmatrix(X, fig_axes=None, names=None, figsize=(8, 8), alpha=1.0, kwargs) Lower triangular of a scatterplot matrix Parameters X : array-like, shape={num_examples, num_features} Design matrix containing data instances (examples) with multiple exploratory variables (features). fix_axes : tuple (default: None) A (fig, axes) tuple, where fig is an figure object and axes is an axes object created via matplotlib, for example, by calling the pyplot subplot function fig, axes = plt.subplots(...) names : list (default: None) A list of string names, which should have the same number of elements as there are features (columns) in X . figsize : tuple (default: (8, 8)) Height and width of the subplot grid. Ignored if fig_axes is not None . alpha : float (default: 1.0) Transparency for both the scatter plots and the histograms along the diagonal. **kwargs : kwargs Keyword arguments for the scatterplots. Returns fix_axes : tuple A (fig, axes) tuple, where fig is an figure object and axes is an axes object created via matplotlib, for example, by calling the pyplot subplot function fig, axes = plt.subplots(...) Examples For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/scatterplotmatrix/","title":"scatterplotmatrix"},{"location":"api_subpackages/mlxtend.plotting/#stacked_barplot","text":"stacked_barplot(df, bar_width='auto', colors='bgrcky', labels='index', rotation=90, legend_loc='best') Function to plot stacked barplots Parameters df : pandas.DataFrame A pandas DataFrame where the index denotes the x-axis labels, and the columns contain the different measurements for each row. bar_width: 'auto' or float (default: 'auto') Parameter to set the widths of the bars. if 'auto', the width is automatically determined by the number of columns in the dataset. colors: str (default: 'bgrcky') The colors of the bars. labels: 'index' or iterable (default: 'index') If 'index', the DataFrame index will be used as x-tick labels. rotation: int (default: 90) Parameter to rotate the x-axis labels. legend_loc : str (default: 'best') Location of the plot legend {best, upper left, upper right, lower left, lower right} No legend if legend_loc=False Returns fig : matplotlib.pyplot figure object Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/plotting/stacked_barplot/","title":"stacked_barplot"},{"location":"api_subpackages/mlxtend.preprocessing/","text":"mlxtend version: 0.23.1 CopyTransformer CopyTransformer() Transformer that returns a copy of the input array For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/CopyTransformer/ Methods fit(X, y=None) Mock method. Does nothing. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns self fit_transform(X, y=None) Return a copy of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_copy : copy of the input X array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X, y=None) Return a copy of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_copy : copy of the input X array. DenseTransformer DenseTransformer(return_copy=True) Convert a sparse array into a dense array. For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/DenseTransformer/ Methods fit(X, y=None) Mock method. Does nothing. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns self fit_transform(X, y=None) Return a dense version of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_dense : dense version of the input X array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X, y=None) Return a dense version of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_dense : dense version of the input X array. MeanCenterer MeanCenterer() Column centering of vectors and matrices. Attributes col_means : numpy.ndarray [n_columns] NumPy array storing the mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/MeanCenterer/ Methods fit(X) Gets the column means for mean centering. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Array of data vectors, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X) Fits and transforms an arry. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Array of data vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_tr : {array-like, sparse matrix}, shape = [n_samples, n_features] A copy of the input array with the columns centered. transform(X) Centers a NumPy array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Array of data vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_tr : {array-like, sparse matrix}, shape = [n_samples, n_features] A copy of the input array with the columns centered. TransactionEncoder TransactionEncoder() Encoder class for transaction data in Python lists Parameters None Attributes columns_: list List of unique names in the X input list of lists Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/TransactionEncoder/ Methods fit(X) Learn unique column names from transaction DataFrame Parameters X : list of lists A python list of lists, where the outer list stores the n transactions and the inner list stores the items in each transaction. For example, [['Apple', 'Beer', 'Rice', 'Chicken'], ['Apple', 'Beer', 'Rice'], ['Apple', 'Beer'], ['Apple', 'Bananas'], ['Milk', 'Beer', 'Rice', 'Chicken'], ['Milk', 'Beer', 'Rice'], ['Milk', 'Beer'], ['Apple', 'Bananas']] fit_transform(X, sparse=False) Fit a TransactionEncoder encoder and transform a dataset. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. inverse_transform(array) Transforms an encoded NumPy array back into transactions. Parameters array : NumPy array [n_transactions, n_unique_items] The NumPy one-hot encoded boolean array of the input transactions, where the columns represent the unique items found in the input array in alphabetic order For example, array([[True , False, True , True , False, True ], [True , False, True , False, False, True ], [True , False, True , False, False, False], [True , True , False, False, False, False], [False, False, True , True , True , True ], [False, False, True , False, True , True ], [False, False, True , False, True , False], [True , True , False, False, False, False]]) The corresponding column labels are available as self.columns_, e.g., ['Apple', 'Bananas', 'Beer', 'Chicken', 'Milk', 'Rice'] Returns X : list of lists A python list of lists, where the outer list stores the n transactions and the inner list stores the items in each transaction. For example, [['Apple', 'Beer', 'Rice', 'Chicken'], ['Apple', 'Beer', 'Rice'], ['Apple', 'Beer'], ['Apple', 'Bananas'], ['Milk', 'Beer', 'Rice', 'Chicken'], ['Milk', 'Beer', 'Rice'], ['Milk', 'Beer'], ['Apple', 'Bananas']] set_inverse_transform_request(self: mlxtend.preprocessing.transactionencoder.TransactionEncoder, , array: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.preprocessing.transactionencoder.TransactionEncoder* Request metadata passed to the inverse_transform method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``inverse_transform`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``inverse_transform``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters array : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for array parameter in inverse_transform . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. set_transform_request(self: mlxtend.preprocessing.transactionencoder.TransactionEncoder, , sparse: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.preprocessing.transactionencoder.TransactionEncoder* Request metadata passed to the transform method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sparse : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sparse parameter in transform . Returns self : object The updated object. transform(X, sparse=False) Transform transactions into a one-hot encoded NumPy array. Parameters X : list of lists A python list of lists, where the outer list stores the n transactions and the inner list stores the items in each transaction. For example, [['Apple', 'Beer', 'Rice', 'Chicken'], ['Apple', 'Beer', 'Rice'], ['Apple', 'Beer'], ['Apple', 'Bananas'], ['Milk', 'Beer', 'Rice', 'Chicken'], ['Milk', 'Beer', 'Rice'], ['Milk', 'Beer'], ['Apple', 'Bananas']] sparse: bool (default=False) If True, transform will return Compressed Sparse Row matrix instead of the regular one. Returns array : NumPy array [n_transactions, n_unique_items] if sparse=False (default). Compressed Sparse Row matrix otherwise The one-hot encoded boolean array of the input transactions, where the columns represent the unique items found in the input array in alphabetic order. Exact representation depends on the sparse argument For example, array([[True , False, True , True , False, True ], [True , False, True , False, False, True ], [True , False, True , False, False, False], [True , True , False, False, False, False], [False, False, True , True , True , True ], [False, False, True , False, True , True ], [False, False, True , False, True , False], [True , True , False, False, False, False]]) The corresponding column labels are available as self.columns_, e.g., ['Apple', 'Bananas', 'Beer', 'Chicken', 'Milk', 'Rice'] minmax_scaling minmax_scaling(array, columns, min_val=0, max_val=1) Min max scaling of pandas' DataFrames. Parameters array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/minmax_scaling/ one_hot one_hot(y, num_labels='auto', dtype='float') One-hot encoding of class labels Parameters y : array-like, shape = [n_classlabels] Python list or numpy array consisting of class labels. num_labels : int or 'auto' Number of unique labels in the class label array. Infers the number of unique labels from the input array if set to 'auto'. dtype : str NumPy array type (float, float32, float64) of the output array. Returns ary : numpy.ndarray, shape = [n_classlabels] One-hot encoded array, where each sample is represented as a row vector in the returned array. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/one_hot/ shuffle_arrays_unison shuffle_arrays_unison(arrays, random_seed=None) Shuffle NumPy arrays in unison. Parameters arrays : array-like, shape = [n_arrays] A list of NumPy arrays. random_seed : int (default: None) Sets the random state. Returns shuffled_arrays : A list of NumPy arrays after shuffling. Examples ``` >>> import numpy as np >>> from mlxtend.preprocessing import shuffle_arrays_unison >>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> y1 = np.array([1, 2, 3]) >>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_seed=3) >>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all()) >>> assert(y2.all() == np.array([2, 1, 3]).all()) >>> For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/shuffle_arrays_unison/ ``` standardize standardize(array, columns=None, ddof=0, return_params=False, params=None) Standardize columns in pandas DataFrames. Parameters array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] (default: None) Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] If None, standardizes all columns. ddof : int (default: 0) Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. return_params : dict (default: False) If set to True, a dictionary is returned in addition to the standardized array. The parameter dictionary contains the column means ('avgs') and standard deviations ('stds') of the individual columns. params : dict (default: None) A dictionary with column means and standard deviations as returned by the standardize function if return_params was set to True. If a params dictionary is provided, the standardize function will use these instead of computing them from the current array. Notes If all values in a given column are the same, these values are all set to 0.0 . The standard deviation in the parameters dictionary is consequently set to 1.0 to avoid dividing by zero. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with standardized columns. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/standardize/","title":"Mlxtend.preprocessing"},{"location":"api_subpackages/mlxtend.preprocessing/#copytransformer","text":"CopyTransformer() Transformer that returns a copy of the input array For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/CopyTransformer/","title":"CopyTransformer"},{"location":"api_subpackages/mlxtend.preprocessing/#methods","text":"fit(X, y=None) Mock method. Does nothing. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns self fit_transform(X, y=None) Return a copy of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_copy : copy of the input X array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X, y=None) Return a copy of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_copy : copy of the input X array.","title":"Methods"},{"location":"api_subpackages/mlxtend.preprocessing/#densetransformer","text":"DenseTransformer(return_copy=True) Convert a sparse array into a dense array. For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/DenseTransformer/","title":"DenseTransformer"},{"location":"api_subpackages/mlxtend.preprocessing/#methods_1","text":"fit(X, y=None) Mock method. Does nothing. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns self fit_transform(X, y=None) Return a dense version of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_dense : dense version of the input X array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X, y=None) Return a dense version of the input array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] (default: None) Returns X_dense : dense version of the input X array.","title":"Methods"},{"location":"api_subpackages/mlxtend.preprocessing/#meancenterer","text":"MeanCenterer() Column centering of vectors and matrices. Attributes col_means : numpy.ndarray [n_columns] NumPy array storing the mean values for centering after fitting the MeanCenterer object. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/MeanCenterer/","title":"MeanCenterer"},{"location":"api_subpackages/mlxtend.preprocessing/#methods_2","text":"fit(X) Gets the column means for mean centering. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Array of data vectors, where n_samples is the number of samples and n_features is the number of features. Returns self fit_transform(X) Fits and transforms an arry. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Array of data vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_tr : {array-like, sparse matrix}, shape = [n_samples, n_features] A copy of the input array with the columns centered. transform(X) Centers a NumPy array. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Array of data vectors, where n_samples is the number of samples and n_features is the number of features. Returns X_tr : {array-like, sparse matrix}, shape = [n_samples, n_features] A copy of the input array with the columns centered.","title":"Methods"},{"location":"api_subpackages/mlxtend.preprocessing/#transactionencoder","text":"TransactionEncoder() Encoder class for transaction data in Python lists Parameters None Attributes columns_: list List of unique names in the X input list of lists Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/TransactionEncoder/","title":"TransactionEncoder"},{"location":"api_subpackages/mlxtend.preprocessing/#methods_3","text":"fit(X) Learn unique column names from transaction DataFrame Parameters X : list of lists A python list of lists, where the outer list stores the n transactions and the inner list stores the items in each transaction. For example, [['Apple', 'Beer', 'Rice', 'Chicken'], ['Apple', 'Beer', 'Rice'], ['Apple', 'Beer'], ['Apple', 'Bananas'], ['Milk', 'Beer', 'Rice', 'Chicken'], ['Milk', 'Beer', 'Rice'], ['Milk', 'Beer'], ['Apple', 'Bananas']] fit_transform(X, sparse=False) Fit a TransactionEncoder encoder and transform a dataset. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. inverse_transform(array) Transforms an encoded NumPy array back into transactions. Parameters array : NumPy array [n_transactions, n_unique_items] The NumPy one-hot encoded boolean array of the input transactions, where the columns represent the unique items found in the input array in alphabetic order For example, array([[True , False, True , True , False, True ], [True , False, True , False, False, True ], [True , False, True , False, False, False], [True , True , False, False, False, False], [False, False, True , True , True , True ], [False, False, True , False, True , True ], [False, False, True , False, True , False], [True , True , False, False, False, False]]) The corresponding column labels are available as self.columns_, e.g., ['Apple', 'Bananas', 'Beer', 'Chicken', 'Milk', 'Rice'] Returns X : list of lists A python list of lists, where the outer list stores the n transactions and the inner list stores the items in each transaction. For example, [['Apple', 'Beer', 'Rice', 'Chicken'], ['Apple', 'Beer', 'Rice'], ['Apple', 'Beer'], ['Apple', 'Bananas'], ['Milk', 'Beer', 'Rice', 'Chicken'], ['Milk', 'Beer', 'Rice'], ['Milk', 'Beer'], ['Apple', 'Bananas']] set_inverse_transform_request(self: mlxtend.preprocessing.transactionencoder.TransactionEncoder, , array: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.preprocessing.transactionencoder.TransactionEncoder* Request metadata passed to the inverse_transform method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``inverse_transform`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``inverse_transform``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters array : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for array parameter in inverse_transform . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. set_transform_request(self: mlxtend.preprocessing.transactionencoder.TransactionEncoder, , sparse: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.preprocessing.transactionencoder.TransactionEncoder* Request metadata passed to the transform method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``transform`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``transform``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sparse : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sparse parameter in transform . Returns self : object The updated object. transform(X, sparse=False) Transform transactions into a one-hot encoded NumPy array. Parameters X : list of lists A python list of lists, where the outer list stores the n transactions and the inner list stores the items in each transaction. For example, [['Apple', 'Beer', 'Rice', 'Chicken'], ['Apple', 'Beer', 'Rice'], ['Apple', 'Beer'], ['Apple', 'Bananas'], ['Milk', 'Beer', 'Rice', 'Chicken'], ['Milk', 'Beer', 'Rice'], ['Milk', 'Beer'], ['Apple', 'Bananas']] sparse: bool (default=False) If True, transform will return Compressed Sparse Row matrix instead of the regular one. Returns array : NumPy array [n_transactions, n_unique_items] if sparse=False (default). Compressed Sparse Row matrix otherwise The one-hot encoded boolean array of the input transactions, where the columns represent the unique items found in the input array in alphabetic order. Exact representation depends on the sparse argument For example, array([[True , False, True , True , False, True ], [True , False, True , False, False, True ], [True , False, True , False, False, False], [True , True , False, False, False, False], [False, False, True , True , True , True ], [False, False, True , False, True , True ], [False, False, True , False, True , False], [True , True , False, False, False, False]]) The corresponding column labels are available as self.columns_, e.g., ['Apple', 'Bananas', 'Beer', 'Chicken', 'Milk', 'Rice']","title":"Methods"},{"location":"api_subpackages/mlxtend.preprocessing/#minmax_scaling","text":"minmax_scaling(array, columns, min_val=0, max_val=1) Min max scaling of pandas' DataFrames. Parameters array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] min_val : int or float , optional (default= 0 ) minimum value after rescaling. max_val : int or float , optional (default= 1 ) maximum value after rescaling. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with rescaled columns. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/minmax_scaling/","title":"minmax_scaling"},{"location":"api_subpackages/mlxtend.preprocessing/#one_hot","text":"one_hot(y, num_labels='auto', dtype='float') One-hot encoding of class labels Parameters y : array-like, shape = [n_classlabels] Python list or numpy array consisting of class labels. num_labels : int or 'auto' Number of unique labels in the class label array. Infers the number of unique labels from the input array if set to 'auto'. dtype : str NumPy array type (float, float32, float64) of the output array. Returns ary : numpy.ndarray, shape = [n_classlabels] One-hot encoded array, where each sample is represented as a row vector in the returned array. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/one_hot/","title":"one_hot"},{"location":"api_subpackages/mlxtend.preprocessing/#shuffle_arrays_unison","text":"shuffle_arrays_unison(arrays, random_seed=None) Shuffle NumPy arrays in unison. Parameters arrays : array-like, shape = [n_arrays] A list of NumPy arrays. random_seed : int (default: None) Sets the random state. Returns shuffled_arrays : A list of NumPy arrays after shuffling. Examples ``` >>> import numpy as np >>> from mlxtend.preprocessing import shuffle_arrays_unison >>> X1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) >>> y1 = np.array([1, 2, 3]) >>> X2, y2 = shuffle_arrays_unison(arrays=[X1, y1], random_seed=3) >>> assert(X2.all() == np.array([[4, 5, 6], [1, 2, 3], [7, 8, 9]]).all()) >>> assert(y2.all() == np.array([2, 1, 3]).all()) >>> For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/shuffle_arrays_unison/ ```","title":"shuffle_arrays_unison"},{"location":"api_subpackages/mlxtend.preprocessing/#standardize","text":"standardize(array, columns=None, ddof=0, return_params=False, params=None) Standardize columns in pandas DataFrames. Parameters array : pandas DataFrame or NumPy ndarray, shape = [n_rows, n_columns]. columns : array-like, shape = [n_columns] (default: None) Array-like with column names, e.g., ['col1', 'col2', ...] or column indices [0, 2, 4, ...] If None, standardizes all columns. ddof : int (default: 0) Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. return_params : dict (default: False) If set to True, a dictionary is returned in addition to the standardized array. The parameter dictionary contains the column means ('avgs') and standard deviations ('stds') of the individual columns. params : dict (default: None) A dictionary with column means and standard deviations as returned by the standardize function if return_params was set to True. If a params dictionary is provided, the standardize function will use these instead of computing them from the current array. Notes If all values in a given column are the same, these values are all set to 0.0 . The standard deviation in the parameters dictionary is consequently set to 1.0 to avoid dividing by zero. Returns df_new : pandas DataFrame object. Copy of the array or DataFrame with standardized columns. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/preprocessing/standardize/","title":"standardize"},{"location":"api_subpackages/mlxtend.regressor/","text":"mlxtend version: 0.23.1 LinearRegression LinearRegression(method='direct', eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0) Ordinary least squares linear regression. Parameters method : string (default: 'direct') For gradient descent-based optimization, use sgd (see minibatch parameter for further options). Otherwise, if direct (default), the analytical method is used. For alternative, numerically more stable solutions, use either qr (QR decomopisition) or svd (Singular Value Decomposition). eta : float (default: 0.01) solver learning rate (between 0.0 and 1.0). Used with method = 'sgd' . (See methods parameter for details) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. Used with method = 'sgd' . (See methods parameter for details) minibatches : int (default: None) The number of minibatches for gradient-based optimization. If None: Direct method, QR, or SVD method (see method parameter for details) If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent learning If 1 < minibatches < len(y): Minibatch learning random_seed : int (default: None) Set random state for shuffling and initializing the weights. Used in method = 'sgd' . (See methods parameter for details) print_progress : int (default: 0) Prints progress in fitting to stderr if method = 'sgd' . 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list Sum of squared errors after each epoch; ignored if solver='normal equation' Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/ Methods fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause StackingCVRegressor StackingCVRegressor(regressors, meta_regressor, cv=5, shuffle=True, random_state=None, verbose=0, refit=True, use_features_in_secondary=False, store_train_meta_features=False, n_jobs=None, pre_dispatch='2 n_jobs', multi_output=False)* A 'Stacking Cross-Validation' regressor for scikit-learn estimators. Parameters regressors : array-like, shape = [n_regressors] A list of regressors. Invoking the fit method on the StackingCVRegressor will fit clones of these original regressors that will be stored in the class attribute self.regr_ . meta_regressor : object The meta-regressor to be fitted on the ensemble of regressor cv : int, cross-validation generator or iterable, optional (default: 5) Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross validation, - integer, to specify the number of folds in a KFold , - An object to be used as a cross-validation generator. - An iterable yielding train, test splits. For integer/None inputs, it will use KFold cross-validation shuffle : bool (default: True) If True, and the cv argument is integer, the training data will be shuffled at fitting stage prior to cross-validation. If the cv argument is a specific cross validation technique, this argument is omitted. random_state : int, RandomState instance or None, optional (default: None) Constrols the randomness of the cv splitter. Used when cv is integer and shuffle=True . New in v0.16.0. verbose : int, optional (default=0) Controls the verbosity of the building process. New in v0.16.0 refit : bool (default: True) Clones the regressors for stacking regression if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Setting refit=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. use_features_in_secondary : bool (default: False) If True, the meta-regressor will be trained both on the predictions of the original regressors and the original dataset. If False, the meta-regressor will be trained only on the predictions of the original regressors. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-regressor stored in the self.train_meta_features_ array, which can be accessed after calling fit . n_jobs : int or None, optional (default=None) The number of CPUs to use to do the computation. None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details. New in v0.16.0. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs - An int, giving the exact number of total jobs that are spawned - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' multi_output : bool (default: False) If True, allow multi-output targets, but forbid nan or inf values. If False, y will be checked to be a vector. (New in v0.19.0.) Attributes train_meta_features : numpy array, shape = [n_samples, n_regressors] meta-features for training data, where n_samples is the number of samples in training data and len(self.regressors) is the number of regressors. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor/ Methods fit(X, y, groups=None, sample_weight=None) Fit ensemble regressors and the meta-regressor. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : numpy array, shape = [n_samples] or [n_samples, n_targets] Target values. Multiple targets are supported only if self.multi_output is True. groups : numpy array/None, shape = [n_samples] The group that each sample belongs to. This is used by specific folding strategies such as GroupKFold() sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. predict(X) Predict target values for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns y_target : array-like, shape = [n_samples] or [n_samples, n_targets] Predicted target values. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, len(self.regressors)] meta-features for test data, where n_samples is the number of samples in test data and len(self.regressors) is the number of regressors. If self.multi_output is True, then the number of columns is len(self.regressors) * n_targets. score(X, y, sample_weight=None) Return the coefficient of determination of the prediction. The coefficient of determination :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual sum of squares ((y_true - y_pred)** 2).sum() and :math: v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum() . The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y , disregarding the input features, would get a :math: R^2 score of 0.0. Parameters X : array-like of shape (n_samples, n_features) Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted) , where n_samples_fitted is the number of samples used in the fitting for the estimator. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True values for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float :math: R^2 of self.predict(X) w.r.t. y . Notes The :math: R^2 score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of :func: ~sklearn.metrics.r2_score . This influences the score method of all the multioutput regressors (except for :class: ~sklearn.multioutput.MultiOutputRegressor ). set_fit_request(self: mlxtend.regressor.stacking_cv_regression.StackingCVRegressor, , groups: Union[bool, NoneType, str] = ' UNCHANGED ', sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_cv_regression.StackingCVRegressor* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.regressor.stacking_cv_regression.StackingCVRegressor, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_cv_regression.StackingCVRegressor* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object. Properties named_regressors Returns List of named estimator tuples, like [('svc', SVC(...))] StackingRegressor StackingRegressor(regressors, meta_regressor, verbose=0, use_features_in_secondary=False, store_train_meta_features=False, refit=True, multi_output=False) A Stacking regressor for scikit-learn estimators for regression. Parameters regressors : array-like, shape = [n_regressors] A list of regressors. Invoking the fit method on the StackingRegressor will fit clones of those original regressors that will be stored in the class attribute self.regr_ . meta_regressor : object The meta-regressor to be fitted on the ensemble of regressors verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the regressor being fitted - verbose=2 : Prints info about the parameters of the regressor being fitted - verbose>2 : Changes verbose param of the underlying regressor to self.verbose - 2 use_features_in_secondary : bool (default: False) If True, the meta-regressor will be trained both on the predictions of the original regressors and the original dataset. If False, the meta-regressor will be trained only on the predictions of the original regressors. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-regressor stored in the self.train_meta_features_ array, which can be accessed after calling fit . Attributes regr_ : list, shape=[n_regressors] Fitted regressors (clones of the original regressors) meta_regr_ : estimator Fitted meta-regressor (clone of the original meta-estimator) coef_ : array-like, shape = [n_features] Model coefficients of the fitted meta-estimator intercept_ : float Intercept of the fitted meta-estimator train_meta_features : numpy array, shape = [n_samples, len(self.regressors)] meta-features for training data, where n_samples is the number of samples in training data and len(self.regressors) is the number of regressors. refit : bool (default: True) Clones the regressors for stacking regression if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Setting refit=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor/ Methods fit(X, y, sample_weight=None) Learn weight coefficients from training data for each regressor. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : numpy array, shape = [n_samples] or [n_samples, n_targets] Target values. Multiple targets are supported only if self.multi_output is True. sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict target values for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns y_target : array-like, shape = [n_samples] or [n_samples, n_targets] Predicted target values. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, len(self.regressors)] meta-features for test data, where n_samples is the number of samples in test data and len(self.regressors) is the number of regressors. If self.multi_output is True, then the number of columns is len(self.regressors) * n_targets score(X, y, sample_weight=None) Return the coefficient of determination of the prediction. The coefficient of determination :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual sum of squares ((y_true - y_pred)** 2).sum() and :math: v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum() . The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y , disregarding the input features, would get a :math: R^2 score of 0.0. Parameters X : array-like of shape (n_samples, n_features) Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted) , where n_samples_fitted is the number of samples used in the fitting for the estimator. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True values for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float :math: R^2 of self.predict(X) w.r.t. y . Notes The :math: R^2 score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of :func: ~sklearn.metrics.r2_score . This influences the score method of all the multioutput regressors (except for :class: ~sklearn.multioutput.MultiOutputRegressor ). set_fit_request(self: mlxtend.regressor.stacking_regression.StackingRegressor, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_regression.StackingRegressor* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.regressor.stacking_regression.StackingRegressor, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_regression.StackingRegressor* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object. Properties coef_ None intercept_ None named_regressors None","title":"Mlxtend.regressor"},{"location":"api_subpackages/mlxtend.regressor/#linearregression","text":"LinearRegression(method='direct', eta=0.01, epochs=50, minibatches=None, random_seed=None, print_progress=0) Ordinary least squares linear regression. Parameters method : string (default: 'direct') For gradient descent-based optimization, use sgd (see minibatch parameter for further options). Otherwise, if direct (default), the analytical method is used. For alternative, numerically more stable solutions, use either qr (QR decomopisition) or svd (Singular Value Decomposition). eta : float (default: 0.01) solver learning rate (between 0.0 and 1.0). Used with method = 'sgd' . (See methods parameter for details) epochs : int (default: 50) Passes over the training dataset. Prior to each epoch, the dataset is shuffled if minibatches > 1 to prevent cycles in stochastic gradient descent. Used with method = 'sgd' . (See methods parameter for details) minibatches : int (default: None) The number of minibatches for gradient-based optimization. If None: Direct method, QR, or SVD method (see method parameter for details) If 1: Gradient Descent learning If len(y): Stochastic Gradient Descent learning If 1 < minibatches < len(y): Minibatch learning random_seed : int (default: None) Set random state for shuffling and initializing the weights. Used in method = 'sgd' . (See methods parameter for details) print_progress : int (default: 0) Prints progress in fitting to stderr if method = 'sgd' . 0: No output 1: Epochs elapsed and cost 2: 1 plus time elapsed 3: 2 plus estimated time until completion Attributes w_ : 2d-array, shape={n_features, 1} Model weights after fitting. b_ : 1d-array, shape={1,} Bias unit after fitting. cost_ : list Sum of squared errors after each epoch; ignored if solver='normal equation' Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/regressor/LinearRegression/","title":"LinearRegression"},{"location":"api_subpackages/mlxtend.regressor/#methods","text":"fit(X, y, init_params=True) Learn model from training data. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape = [n_samples] Target values. init_params : bool (default: True) Re-initializes model parameters prior to fitting. Set False to continue training with weights from a previous model fitting. Returns self : object get_params(deep=True) Get parameters for this estimator. Parameters deep : boolean, optional If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : mapping of string to any Parameter names mapped to their values.' adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux gael.varoquaux@normalesup.org License: BSD 3 clause predict(X) Predict targets from X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns target_values : array-like, shape = [n_samples] Predicted target values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Returns self adapted from https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py Author: Gael Varoquaux <gael.varoquaux@normalesup.org> License: BSD 3 clause","title":"Methods"},{"location":"api_subpackages/mlxtend.regressor/#stackingcvregressor","text":"StackingCVRegressor(regressors, meta_regressor, cv=5, shuffle=True, random_state=None, verbose=0, refit=True, use_features_in_secondary=False, store_train_meta_features=False, n_jobs=None, pre_dispatch='2 n_jobs', multi_output=False)* A 'Stacking Cross-Validation' regressor for scikit-learn estimators. Parameters regressors : array-like, shape = [n_regressors] A list of regressors. Invoking the fit method on the StackingCVRegressor will fit clones of these original regressors that will be stored in the class attribute self.regr_ . meta_regressor : object The meta-regressor to be fitted on the ensemble of regressor cv : int, cross-validation generator or iterable, optional (default: 5) Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 5-fold cross validation, - integer, to specify the number of folds in a KFold , - An object to be used as a cross-validation generator. - An iterable yielding train, test splits. For integer/None inputs, it will use KFold cross-validation shuffle : bool (default: True) If True, and the cv argument is integer, the training data will be shuffled at fitting stage prior to cross-validation. If the cv argument is a specific cross validation technique, this argument is omitted. random_state : int, RandomState instance or None, optional (default: None) Constrols the randomness of the cv splitter. Used when cv is integer and shuffle=True . New in v0.16.0. verbose : int, optional (default=0) Controls the verbosity of the building process. New in v0.16.0 refit : bool (default: True) Clones the regressors for stacking regression if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Setting refit=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. use_features_in_secondary : bool (default: False) If True, the meta-regressor will be trained both on the predictions of the original regressors and the original dataset. If False, the meta-regressor will be trained only on the predictions of the original regressors. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-regressor stored in the self.train_meta_features_ array, which can be accessed after calling fit . n_jobs : int or None, optional (default=None) The number of CPUs to use to do the computation. None means 1 unless in a :obj: joblib.parallel_backend context. -1 means using all processors. See :term: Glossary <n_jobs> for more details. New in v0.16.0. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs - An int, giving the exact number of total jobs that are spawned - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' multi_output : bool (default: False) If True, allow multi-output targets, but forbid nan or inf values. If False, y will be checked to be a vector. (New in v0.19.0.) Attributes train_meta_features : numpy array, shape = [n_samples, n_regressors] meta-features for training data, where n_samples is the number of samples in training data and len(self.regressors) is the number of regressors. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor/","title":"StackingCVRegressor"},{"location":"api_subpackages/mlxtend.regressor/#methods_1","text":"fit(X, y, groups=None, sample_weight=None) Fit ensemble regressors and the meta-regressor. Parameters X : numpy array, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : numpy array, shape = [n_samples] or [n_samples, n_targets] Target values. Multiple targets are supported only if self.multi_output is True. groups : numpy array/None, shape = [n_samples] The group that each sample belongs to. This is used by specific folding strategies such as GroupKFold() sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. predict(X) Predict target values for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns y_target : array-like, shape = [n_samples] or [n_samples, n_targets] Predicted target values. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, len(self.regressors)] meta-features for test data, where n_samples is the number of samples in test data and len(self.regressors) is the number of regressors. If self.multi_output is True, then the number of columns is len(self.regressors) * n_targets. score(X, y, sample_weight=None) Return the coefficient of determination of the prediction. The coefficient of determination :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual sum of squares ((y_true - y_pred)** 2).sum() and :math: v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum() . The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y , disregarding the input features, would get a :math: R^2 score of 0.0. Parameters X : array-like of shape (n_samples, n_features) Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted) , where n_samples_fitted is the number of samples used in the fitting for the estimator. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True values for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float :math: R^2 of self.predict(X) w.r.t. y . Notes The :math: R^2 score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of :func: ~sklearn.metrics.r2_score . This influences the score method of all the multioutput regressors (except for :class: ~sklearn.multioutput.MultiOutputRegressor ). set_fit_request(self: mlxtend.regressor.stacking_cv_regression.StackingCVRegressor, , groups: Union[bool, NoneType, str] = ' UNCHANGED ', sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_cv_regression.StackingCVRegressor* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters groups : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for groups parameter in fit . sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.regressor.stacking_cv_regression.StackingCVRegressor, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_cv_regression.StackingCVRegressor* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object.","title":"Methods"},{"location":"api_subpackages/mlxtend.regressor/#properties","text":"named_regressors Returns List of named estimator tuples, like [('svc', SVC(...))]","title":"Properties"},{"location":"api_subpackages/mlxtend.regressor/#stackingregressor","text":"StackingRegressor(regressors, meta_regressor, verbose=0, use_features_in_secondary=False, store_train_meta_features=False, refit=True, multi_output=False) A Stacking regressor for scikit-learn estimators for regression. Parameters regressors : array-like, shape = [n_regressors] A list of regressors. Invoking the fit method on the StackingRegressor will fit clones of those original regressors that will be stored in the class attribute self.regr_ . meta_regressor : object The meta-regressor to be fitted on the ensemble of regressors verbose : int, optional (default=0) Controls the verbosity of the building process. - verbose=0 (default): Prints nothing - verbose=1 : Prints the number & name of the regressor being fitted - verbose=2 : Prints info about the parameters of the regressor being fitted - verbose>2 : Changes verbose param of the underlying regressor to self.verbose - 2 use_features_in_secondary : bool (default: False) If True, the meta-regressor will be trained both on the predictions of the original regressors and the original dataset. If False, the meta-regressor will be trained only on the predictions of the original regressors. store_train_meta_features : bool (default: False) If True, the meta-features computed from the training data used for fitting the meta-regressor stored in the self.train_meta_features_ array, which can be accessed after calling fit . Attributes regr_ : list, shape=[n_regressors] Fitted regressors (clones of the original regressors) meta_regr_ : estimator Fitted meta-regressor (clone of the original meta-estimator) coef_ : array-like, shape = [n_features] Model coefficients of the fitted meta-estimator intercept_ : float Intercept of the fitted meta-estimator train_meta_features : numpy array, shape = [n_samples, len(self.regressors)] meta-features for training data, where n_samples is the number of samples in training data and len(self.regressors) is the number of regressors. refit : bool (default: True) Clones the regressors for stacking regression if True (default) or else uses the original ones, which will be refitted on the dataset upon calling the fit method. Setting refit=False is recommended if you are working with estimators that are supporting the scikit-learn fit/predict API interface but are not compatible to scikit-learn's clone function. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor/","title":"StackingRegressor"},{"location":"api_subpackages/mlxtend.regressor/#methods_2","text":"fit(X, y, sample_weight=None) Learn weight coefficients from training data for each regressor. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : numpy array, shape = [n_samples] or [n_samples, n_targets] Target values. Multiple targets are supported only if self.multi_output is True. sample_weight : array-like, shape = [n_samples], optional Sample weights passed as sample_weights to each regressor in the regressors list as well as the meta_regressor. Raises error if some regressor does not support sample_weight in the fit() method. Returns self : object fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_metadata_routing() Get metadata routing of this object. Please check :ref:`User Guide <metadata_routing>` on how the routing mechanism works. Returns routing : MetadataRequest A :class: ~sklearn.utils.metadata_routing.MetadataRequest encapsulating routing information. get_params(deep=True) Return estimator parameter names for GridSearch support. predict(X) Predict target values for X. Parameters X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. Returns y_target : array-like, shape = [n_samples] or [n_samples, n_targets] Predicted target values. predict_meta_features(X) Get meta-features of test-data. Parameters X : numpy array, shape = [n_samples, n_features] Test vectors, where n_samples is the number of samples and n_features is the number of features. Returns meta-features : numpy array, shape = [n_samples, len(self.regressors)] meta-features for test data, where n_samples is the number of samples in test data and len(self.regressors) is the number of regressors. If self.multi_output is True, then the number of columns is len(self.regressors) * n_targets score(X, y, sample_weight=None) Return the coefficient of determination of the prediction. The coefficient of determination :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual sum of squares ((y_true - y_pred)** 2).sum() and :math: v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum() . The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y , disregarding the input features, would get a :math: R^2 score of 0.0. Parameters X : array-like of shape (n_samples, n_features) Test samples. For some estimators this may be a precomputed kernel matrix or a list of generic objects instead with shape (n_samples, n_samples_fitted) , where n_samples_fitted is the number of samples used in the fitting for the estimator. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True values for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns score : float :math: R^2 of self.predict(X) w.r.t. y . Notes The :math: R^2 score used when calling score on a regressor uses multioutput='uniform_average' from version 0.23 to keep consistent with default value of :func: ~sklearn.metrics.r2_score . This influences the score method of all the multioutput regressors (except for :class: ~sklearn.multioutput.MultiOutputRegressor ). set_fit_request(self: mlxtend.regressor.stacking_regression.StackingRegressor, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_regression.StackingRegressor* Request metadata passed to the fit method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in fit . Returns self : object The updated object. set_output( , transform=None)* Set output container. See :ref:`sphx_glr_auto_examples_miscellaneous_plot_set_output.py` for an example on how to use the API. Parameters transform : {\"default\", \"pandas\"}, default=None Configure output of transform and fit_transform . \"default\" : Default output format of a transformer \"pandas\" : DataFrame output None : Transform configuration is unchanged Returns self : estimator instance Estimator instance. set_params( params) Set the parameters of this estimator. Valid parameter keys can be listed with ``get_params()``. Returns self set_score_request(self: mlxtend.regressor.stacking_regression.StackingRegressor, , sample_weight: Union[bool, NoneType, str] = ' UNCHANGED ') -> mlxtend.regressor.stacking_regression.StackingRegressor* Request metadata passed to the score method. Note that this method is only relevant if ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`). Please see :ref:`User Guide <metadata_routing>` on how the routing mechanism works. The options for each parameter are: - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided. - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``. - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it. - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name. The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the existing request. This allows you to change the request for some parameters and not others. .. versionadded:: 1.3 .. note:: This method is only relevant if this estimator is used as a sub-estimator of a meta-estimator, e.g. used inside a :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect. Parameters sample_weight : str, True, False, or None, default=sklearn.utils.metadata_routing.UNCHANGED Metadata routing for sample_weight parameter in score . Returns self : object The updated object.","title":"Methods"},{"location":"api_subpackages/mlxtend.regressor/#properties_1","text":"coef_ None intercept_ None named_regressors None","title":"Properties"},{"location":"api_subpackages/mlxtend.text/","text":"mlxtend version: 0.23.1 generalize_names generalize_names(name, output_sep=' ', firstname_output_letters=1) Generalize a person's first and last name. Returns a person's name in the format <last_name><separator><firstname letter(s)> (all lowercase) Parameters name : str Name of the player output_sep : str (default: ' ') String for separating last name and first name in the output. firstname_output_letters : int Number of letters in the abbreviated first name. Returns gen_name : str The generalized name. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/generalize_names/ generalize_names_duplcheck generalize_names_duplcheck(df, col_name) Generalizes names and removes duplicates. Applies mlxtend.text.generalize_names to a DataFrame with 1 first name letter by default and uses more first name letters if duplicates are detected. Parameters df : pandas.DataFrame DataFrame that contains a column where generalize_names should be applied. col_name : str Name of the DataFrame column where generalize_names function should be applied to. Returns df_new : str New DataFrame object where generalize_names function has been applied without duplicates. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/generalize_names_duplcheck/ tokenizer_emoticons tokenizer_emoticons(text) Return emoticons from text Examples >>> tokenizer_emoticons('</a>This :) is :( a test :-)!') [':)', ':(', ':-)'] For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/tokenizer_emoticons/ tokenizer_words_and_emoticons tokenizer_words_and_emoticons(text) Convert text to lowercase words and emoticons. Examples >>> tokenizer_words_and_emoticons('</a>This :) is :( a test :-)!') ['this', 'is', 'a', 'test', ':)', ':(', ':-)'] For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/tokenizer_words_and_emoticons/","title":"Mlxtend.text"},{"location":"api_subpackages/mlxtend.text/#generalize_names","text":"generalize_names(name, output_sep=' ', firstname_output_letters=1) Generalize a person's first and last name. Returns a person's name in the format <last_name><separator><firstname letter(s)> (all lowercase) Parameters name : str Name of the player output_sep : str (default: ' ') String for separating last name and first name in the output. firstname_output_letters : int Number of letters in the abbreviated first name. Returns gen_name : str The generalized name. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/generalize_names/","title":"generalize_names"},{"location":"api_subpackages/mlxtend.text/#generalize_names_duplcheck","text":"generalize_names_duplcheck(df, col_name) Generalizes names and removes duplicates. Applies mlxtend.text.generalize_names to a DataFrame with 1 first name letter by default and uses more first name letters if duplicates are detected. Parameters df : pandas.DataFrame DataFrame that contains a column where generalize_names should be applied. col_name : str Name of the DataFrame column where generalize_names function should be applied to. Returns df_new : str New DataFrame object where generalize_names function has been applied without duplicates. Examples For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/generalize_names_duplcheck/","title":"generalize_names_duplcheck"},{"location":"api_subpackages/mlxtend.text/#tokenizer_emoticons","text":"tokenizer_emoticons(text) Return emoticons from text Examples >>> tokenizer_emoticons('</a>This :) is :( a test :-)!') [':)', ':(', ':-)'] For usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/tokenizer_emoticons/","title":"tokenizer_emoticons"},{"location":"api_subpackages/mlxtend.text/#tokenizer_words_and_emoticons","text":"tokenizer_words_and_emoticons(text) Convert text to lowercase words and emoticons. Examples >>> tokenizer_words_and_emoticons('</a>This :) is :( a test :-)!') ['this', 'is', 'a', 'test', ':)', ':(', ':-)'] For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/text/tokenizer_words_and_emoticons/","title":"tokenizer_words_and_emoticons"},{"location":"api_subpackages/mlxtend.utils/","text":"mlxtend version: 0.23.1 Counter Counter(stderr=False, start_newline=True, precision=0, name=None) Class to display the progress of for-loop iterators. Parameters stderr : bool (default: True) Prints output to sys.stderr if True; uses sys.stdout otherwise. start_newline : bool (default: True) Prepends a new line to the counter, which prevents overwriting counters if multiple counters are printed in succession. precision: int (default: 0) Sets the number of decimal places when displaying the time elapsed in seconds. name : string (default: None) Prepends the specified name before the counter to allow distinguishing between multiple counters. Attributes curr_iter : int The current iteration. start_time : float The system's time in seconds when the Counter was initialized. end_time : float The system's time in seconds when the Counter was last updated. Examples >>> cnt = Counter() >>> for i in range(20): ... # do some computation ... time.sleep(0.1) ... cnt.update() 20 iter | 2 sec >>> print('The counter was initialized.' ' %d seconds ago.' % (time.time() - cnt.start_time)) The counter was initialized 2 seconds ago >>> print('The counter was last updated' ' %d seconds ago.' % (time.time() - cnt.end_time)) The counter was last updated 0 seconds ago. For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/utils/Counter/ Methods update() Print current iteration and time elapsed. assert_raises assert_raises(exception_type, message, func, args, * kwargs) Check that an exception is raised with a specific message Parameters exception_type : exception The exception that should be raised message : str (default: None) The error message that should be raised. Ignored if False or None. func : callable The function that raises the exception *args : positional arguments to func . **kwargs : keyword arguments to func check_Xy check_Xy(X, y, y_int=True) None format_kwarg_dictionaries format_kwarg_dictionaries(default_kwargs=None, user_kwargs=None, protected_keys=None) Function to combine default and user specified kwargs dictionaries Parameters default_kwargs : dict, optional Default kwargs (default is None). user_kwargs : dict, optional User specified kwargs (default is None). protected_keys : array_like, optional Sequence of keys to be removed from the returned dictionary (default is None). Returns formatted_kwargs : dict Formatted kwargs dictionary.","title":"Mlxtend.utils"},{"location":"api_subpackages/mlxtend.utils/#counter","text":"Counter(stderr=False, start_newline=True, precision=0, name=None) Class to display the progress of for-loop iterators. Parameters stderr : bool (default: True) Prints output to sys.stderr if True; uses sys.stdout otherwise. start_newline : bool (default: True) Prepends a new line to the counter, which prevents overwriting counters if multiple counters are printed in succession. precision: int (default: 0) Sets the number of decimal places when displaying the time elapsed in seconds. name : string (default: None) Prepends the specified name before the counter to allow distinguishing between multiple counters. Attributes curr_iter : int The current iteration. start_time : float The system's time in seconds when the Counter was initialized. end_time : float The system's time in seconds when the Counter was last updated. Examples >>> cnt = Counter() >>> for i in range(20): ... # do some computation ... time.sleep(0.1) ... cnt.update() 20 iter | 2 sec >>> print('The counter was initialized.' ' %d seconds ago.' % (time.time() - cnt.start_time)) The counter was initialized 2 seconds ago >>> print('The counter was last updated' ' %d seconds ago.' % (time.time() - cnt.end_time)) The counter was last updated 0 seconds ago. For more usage examples, please see https://rasbt.github.io/mlxtend/user_guide/utils/Counter/","title":"Counter"},{"location":"api_subpackages/mlxtend.utils/#methods","text":"update() Print current iteration and time elapsed.","title":"Methods"},{"location":"api_subpackages/mlxtend.utils/#assert_raises","text":"assert_raises(exception_type, message, func, args, * kwargs) Check that an exception is raised with a specific message Parameters exception_type : exception The exception that should be raised message : str (default: None) The error message that should be raised. Ignored if False or None. func : callable The function that raises the exception *args : positional arguments to func . **kwargs : keyword arguments to func","title":"assert_raises"},{"location":"api_subpackages/mlxtend.utils/#check_xy","text":"check_Xy(X, y, y_int=True) None","title":"check_Xy"},{"location":"api_subpackages/mlxtend.utils/#format_kwarg_dictionaries","text":"format_kwarg_dictionaries(default_kwargs=None, user_kwargs=None, protected_keys=None) Function to combine default and user specified kwargs dictionaries Parameters default_kwargs : dict, optional Default kwargs (default is None). user_kwargs : dict, optional User specified kwargs (default is None). protected_keys : array_like, optional Sequence of keys to be removed from the returned dictionary (default is None). Returns formatted_kwargs : dict Formatted kwargs dictionary.","title":"format_kwarg_dictionaries"}]}