<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="A library consisting of useful tools and extensions for the day-to-day data science tasks."> 
    <meta name="author" content="Sebastian Raschka"> 
    <link rel="canonical" href="http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.feature_selection/">
    <link rel="shortcut icon" href="../../img/favicon.ico">

    <title>Mlxtend.feature selection - mlxtend</title>

    <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdn.jsdelivr.net/font-hack/2.018/css/hack.min.css">
    <link href='//fonts.googleapis.com/css?family=PT+Sans:400,400italic,700,700italic&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,700italic,400,300,600,700&subset=latin-ext,latin' rel='stylesheet' type='text/css'>
    <link href="../../css/base.css" rel="stylesheet">
    <link href="../../css/cinder.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/highlight.css">


    <link href="../../cinder/css/base.css" rel="stylesheet">


    <link href="../../cinder/css/bootstrap-custom.css" rel="stylesheet">


    <link href="../../cinder/css/bootstrap-custom.min.css" rel="stylesheet">


    <link href="../../cinder/css/cinder.css" rel="stylesheet">


    <link href="../../cinder/css/font-awesome-4.0.3.css" rel="stylesheet">


    <link href="../../cinder/css/highlight.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.5.18/webfont.js"></script>
    <script>
    WebFont.load({
        google: {
            families: ['Open Sans', 'PT Sans']
        }
    });
    </script>

    
    <script>
    (function(i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r;
        i[r] = i[r] || function() {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date();
        a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
        a.async = 1;
        a.src = g;
        m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-38457794-2', 'rasbt.github.io/mlxtend/');
    ga('send', 'pageview');
    </script>
    
</head>

<body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->

            <a class="navbar-brand" href="../..">mlxtend</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                
                
                    <li >
                        <a href="../..">Home</a>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">User Guide <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../USER_GUIDE_INDEX/">User Guide Index</a>
</li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">classifier</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/classifier/Adaline/">Adaline: Adaptive Linear Neuron Classifier</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/EnsembleVoteClassifier/">EnsembleVoteClassifier: A majority voting classifier</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/LogisticRegression/">LogisticRegression: A binary classifier</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/MultiLayerPerceptron/">MultilayerPerceptron: A simple multilayer neural network</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/OneRClassifier/">OneRClassifier: One Rule (OneR) method for classfication</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/Perceptron/">Perceptron: A simple binary classifier</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/SoftmaxRegression/">SoftmaxRegression: Multiclass version of logistic regression</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/StackingClassifier/">StackingClassifier: Simple stacking</a>
</li>

        
            
<li >
    <a href="../../user_guide/classifier/StackingCVClassifier/">StackingCVClassifier: Stacking with cross-validation</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">cluster</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/cluster/Kmeans/">Kmeans: k-means clustering</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">data</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/data/autompg_data/">autompg_data: The Auto-MPG dataset for regression</a>
</li>

        
            
<li >
    <a href="../../user_guide/data/boston_housing_data/">boston_housing_data: The Boston housing dataset for regression</a>
</li>

        
            
<li >
    <a href="../../user_guide/data/iris_data/">iris_data: The 3-class iris dataset for classification</a>
</li>

        
            
<li >
    <a href="../../user_guide/data/loadlocal_mnist/">loadlocal_mnist: A function for loading MNIST from the original ubyte files</a>
</li>

        
            
<li >
    <a href="../../user_guide/data/make_multiplexer_dataset/">make_multiplexer_dataset: A function for creating multiplexer data</a>
</li>

        
            
<li >
    <a href="../../user_guide/data/mnist_data/">mnist_data: A subset of the MNIST dataset for classification</a>
</li>

        
            
<li >
    <a href="../../user_guide/data/three_blobs_data/">three_blobs_data: The synthetic blobs for classification</a>
</li>

        
            
<li >
    <a href="../../user_guide/data/wine_data/">wine_data: A 3-class wine dataset for classification</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">evaluate</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/evaluate/accuracy_score/">accuracy_score: Computing standard, balanced, and per-class accuracy</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/bias_variance_decomp/">bias_variance_decomp: Bias-variance decomposition for classification and regression losses</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/bootstrap/">bootstrap: The ordinary nonparametric boostrap for arbitrary parameters</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/bootstrap_point632_score/">bootstrap_point632_score: The .632 and .632+ boostrap for classifier evaluation</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/BootstrapOutOfBag/">BootstrapOutOfBag: A scikit-learn compatible version of the out-of-bag bootstrap</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/cochrans_q/">cochrans_q: Cochran's Q test for comparing multiple classifiers</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/combined_ftest_5x2cv/">combined_ftest_5x2cv: 5x2cv combined *F* test for classifier comparisons</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/confusion_matrix/">confusion_matrix: creating a confusion matrix for model evaluation</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/create_counterfactual/">create_counterfactual: Interpreting models via counterfactuals</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/feature_importance_permutation/">feature_importance_permutation: Estimate feature importance via feature permutation.</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/ftest/">ftest: F-test for classifier comparisons</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/GroupTimeSeriesSplit/">GroupTimeSeriesSplit: A scikit-learn compatible version of the time series validation with groups</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/lift_score/">lift_score: Lift score for classification and association rule mining</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/mcnemar_table/">mcnemar_table: Ccontingency table for McNemar's test</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/mcnemar_tables/">mcnemar_tables: contingency tables for McNemar's test and Cochran's Q test</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/mcnemar/">mcnemar: McNemar's test for classifier comparisons</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/paired_ttest_5x2cv/">paired_ttest_5x2cv: 5x2cv paired *t* test for classifier comparisons</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/paired_ttest_kfold_cv/">paired_ttest_kfold_cv: K-fold cross-validated paired *t* test</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/paired_ttest_resampled/">paired_ttest_resample: Resampled paired *t* test</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/permutation_test/">permutation_test: Permutation test for hypothesis testing</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/PredefinedHoldoutSplit/">PredefinedHoldoutSplit: Utility for the holdout method compatible with scikit-learn</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/RandomHoldoutSplit/">RandomHoldoutSplit: split a dataset into a train and validation subset for validation</a>
</li>

        
            
<li >
    <a href="../../user_guide/evaluate/scoring/">scoring: computing various performance metrics</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">feature_extraction</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/feature_extraction/LinearDiscriminantAnalysis/">LinearDiscriminantAnalysis: Linear discriminant analysis for dimensionality reduction</a>
</li>

        
            
<li >
    <a href="../../user_guide/feature_extraction/PrincipalComponentAnalysis/">PrincipalComponentAnalysis: Principal component analysis (PCA) for dimensionality reduction</a>
</li>

        
            
<li >
    <a href="../../user_guide/feature_extraction/RBFKernelPCA/">RBFKernelPCA</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">feature_selection</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/feature_selection/ColumnSelector/">ColumnSelector: Scikit-learn utility function to select specific columns in a pipeline</a>
</li>

        
            
<li >
    <a href="../../user_guide/feature_selection/ExhaustiveFeatureSelector/">ExhaustiveFeatureSelector: Optimal feature sets by considering all possible feature combinations</a>
</li>

        
            
<li >
    <a href="../../user_guide/feature_selection/SequentialFeatureSelector/">SequentialFeatureSelector: The popular forward and backward feature selection approaches (including floating variants)</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">file_io</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/file_io/find_filegroups/">find_filegroups: Find files that only differ via their file extensions</a>
</li>

        
            
<li >
    <a href="../../user_guide/file_io/find_files/">find_files: Find files based on substring matches</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">frequent_patterns</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/frequent_patterns/apriori/">Apriori</a>
</li>

        
            
<li >
    <a href="../../user_guide/frequent_patterns/association_rules/">Association rules</a>
</li>

        
            
<li >
    <a href="../../user_guide/frequent_patterns/fpgrowth/">Fpgrowth</a>
</li>

        
            
<li >
    <a href="../../user_guide/frequent_patterns/fpmax/">Fpmax</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">image</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/image/extract_face_landmarks/">extract_face_landmarks: extract 68 landmark features from face images</a>
</li>

        
            
<li >
    <a href="../../user_guide/image/eyepad_align/">EyepadAlign:  align face images based on eye location</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">math</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/math/num_combinations/">num_combinations: combinations for creating subsequences of *k* elements</a>
</li>

        
            
<li >
    <a href="../../user_guide/math/num_permutations/">num_permutations: number of permutations for creating subsequences of *k* elements</a>
</li>

        
            
<li >
    <a href="../../user_guide/math/vectorspace_dimensionality/">vectorspace_dimensionality: compute the number of dimensions that a set of vectors spans</a>
</li>

        
            
<li >
    <a href="../../user_guide/math/vectorspace_orthonormalization/">vectorspace_orthonormalization: Converts a set of linearly independent vectors to a set of orthonormal basis vectors</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">plotting</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/plotting/category_scatter/">Scategory_scatter: Create a scatterplot with categories in different colors</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/checkerboard_plot/">checkerboard_plot: Create a checkerboard plot in matplotlib</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/plot_pca_correlation_graph/">plot_pca_correlation_graph: plot correlations between original features and principal components</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/ecdf/">ecdf: Create an empirical cumulative distribution function plot</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/enrichment_plot/">enrichment_plot: create an enrichment plot for cumulative counts</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/heatmap/">heatmap: Create a heatmap in matplotlib</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/plot_confusion_matrix/">plot_confusion_matrix: Visualize confusion matrices</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/plot_decision_regions/">plot_decision_regions: Visualize the decision regions of a classifier</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/plot_learning_curves/">plot_learning_curves: Plot learning curves from training and test sets</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/plot_linear_regression/">plot_linear_regression: A quick way for plotting linear regression fits</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/plot_sequential_feature_selection/">plot_sequential_feature_selection: Visualize selected feature subset performances from the SequentialFeatureSelector</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/scatterplotmatrix/">scatterplotmatrix: visualize datasets via a scatter plot matrix</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/scatter_hist/">scatter_hist: create a scatter histogram plot</a>
</li>

        
            
<li >
    <a href="../../user_guide/plotting/stacked_barplot/">stacked_barplot: Plot stacked bar plots in matplotlib</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">preprocessing</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/preprocessing/CopyTransformer/">CopyTransformer: A function that creates a copy of the input array in a scikit-learn pipeline</a>
</li>

        
            
<li >
    <a href="../../user_guide/preprocessing/DenseTransformer/">DenseTransformer: Transforms a sparse into a dense NumPy array, e.g., in a scikit-learn pipeline</a>
</li>

        
            
<li >
    <a href="../../user_guide/preprocessing/MeanCenterer/">MeanCenterer: column-based mean centering on a NumPy array</a>
</li>

        
            
<li >
    <a href="../../user_guide/preprocessing/minmax_scaling/">MinMaxScaling: Min-max scaling fpr pandas DataFrames and NumPy arrays</a>
</li>

        
            
<li >
    <a href="../../user_guide/preprocessing/one-hot_encoding/">One hot encoding</a>
</li>

        
            
<li >
    <a href="../../user_guide/preprocessing/shuffle_arrays_unison/">shuffle_arrays_unison: shuffle arrays in a consistent fashion</a>
</li>

        
            
<li >
    <a href="../../user_guide/preprocessing/standardize/">standardize: A function to standardize columns in a 2D NumPy array</a>
</li>

        
            
<li >
    <a href="../../user_guide/preprocessing/TransactionEncoder/">TransactionEncoder</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">regressor</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/regressor/LinearRegression/">LinearRegression: An implementation of ordinary least-squares linear regression</a>
</li>

        
            
<li >
    <a href="../../user_guide/regressor/StackingCVRegressor/">StackingCVRegressor: stacking with cross-validation for regression</a>
</li>

        
            
<li >
    <a href="../../user_guide/regressor/StackingRegressor/">StackingRegressor: a simple stacking implementation for regression</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">text</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/text/generalize_names/">generalize_names: convert names into a generalized format</a>
</li>

        
            
<li >
    <a href="../../user_guide/text/generalize_names_duplcheck/">generalize_names_duplcheck: Generalize names while preventing duplicates among different names</a>
</li>

        
            
<li >
    <a href="../../user_guide/text/tokenizer/">tokenizer_emoticons: tokenizers for emoticons</a>
</li>

        
    </ul>
  </li>

                        
                            
  <li class="dropdown-submenu">
    <a tabindex="-1" href="">utils</a>
    <ul class="dropdown-menu">
        
            
<li >
    <a href="../../user_guide/utils/Counter/">Counter: A simple progress counter</a>
</li>

        
    </ul>
  </li>

                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">API <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../mlxtend.classifier/">Mlxtend.classifier</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.cluster/">Mlxtend.cluster</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.data/">Mlxtend.data</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.evaluate/">Mlxtend.evaluate</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.feature_extraction/">Mlxtend.feature extraction</a>
</li>

                        
                            
<li class="active">
    <a href="./">Mlxtend.feature selection</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.file_io/">Mlxtend.file io</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.frequent_patterns/">Mlxtend.frequent patterns</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.image/">Mlxtend.image</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.plotting/">Mlxtend.plotting</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.preprocessing/">Mlxtend.preprocessing</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.regressor/">Mlxtend.regressor</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.text/">Mlxtend.text</a>
</li>

                        
                            
<li >
    <a href="../mlxtend.utils/">Mlxtend.utils</a>
</li>

                        
                        </ul>
                    </li>
                
                
                
                    <li >
                        <a href="../../installation/">Installation</a>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">About <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            
<li >
    <a href="../../CHANGELOG/">Release Notes</a>
</li>

                        
                            
<li >
    <a href="../../Code-of-Conduct/">Code of Conduct</a>
</li>

                        
                            
<li >
    <a href="../../CONTRIBUTING/">How To Contribute</a>
</li>

                        
                            
<li >
    <a href="../../contributors/">Contributors</a>
</li>

                        
                            
<li >
    <a href="../../license/">License</a>
</li>

                        
                            
<li >
    <a href="../../cite/">Citing Mlxtend</a>
</li>

                        
                            
<li >
    <a href="../../discuss/">Discuss</a>
</li>

                        
                        </ul>
                    </li>
                
                
                </ul>

            <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                            <i class="fa fa-search"></i> Search
                        </a>
                    </li>

                <!--
                    <li >
                        <a rel="next" href="../mlxtend.feature_extraction/">
                            <i class="fa fa-arrow-left"></i> Previous
                        </a>
                    </li>
                    <li >
                        <a rel="prev" href="../mlxtend.file_io/">
                            Next <i class="fa fa-arrow-right"></i>
                        </a>
                    </li>-->
                    <li>
                        <a href="https://github.com/rasbt/mlxtend"><i class="fa fa-github"></i> GitHub</a>
                    </li>
            </ul>
        </div>
    </div>
</div>

    <div class="container">
        
        
        <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="first-level active"><a href="#columnselector">ColumnSelector</a></li>
            <li class="second-level"><a href="#methods">Methods</a></li>
                 <!--   -->
        <li class="first-level "><a href="#exhaustivefeatureselector">ExhaustiveFeatureSelector</a></li>
            <li class="second-level"><a href="#methods_1">Methods</a></li>
                 <!--   -->
        <li class="first-level "><a href="#sequentialfeatureselector">SequentialFeatureSelector</a></li>
            <li class="second-level"><a href="#methods_2">Methods</a></li>
                 <!--   -->
            <li class="second-level"><a href="#properties">Properties</a></li>
                 <!--   -->
    </ul>
</div></div>
        <div class="col-md-9" role="main">

<p>mlxtend version: 0.21.0 </p>
<h2 id="columnselector">ColumnSelector</h2>
<p><em>ColumnSelector(cols=None, drop_axis=False)</em></p>
<p>Object for selecting specific columns from a data set.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>cols</code> : array-like (default: None)</p>
<p>A list specifying the feature indices to be selected. For example,
[1, 4, 5] to select the 2nd, 5th, and 6th feature columns, and
['A','C','D'] to select the name of feature columns A, C and D.
If None, returns all columns in the array.</p>
</li>
<li>
<p><code>drop_axis</code> : bool (default=False)</p>
<p>Drops last axis if True and the only one column is selected. This
is useful, e.g., when the ColumnSelector is used for selecting
only one column and the resulting array should be fed to e.g.,
a scikit-learn column selector. E.g., instead of returning an
array with shape (n_samples, 1), drop_axis=True will return an
aray with shape (n_samples,).</p>
</li>
</ul>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    http://rasbt.github.io/mlxtend/user_guide/feature_selection/ColumnSelector/</p>
<h3 id="methods">Methods</h3>
<hr>

<p><em>fit(X, y=None)</em></p>
<p>Mock method. Does nothing.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples] (default: None)</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>self</p>
<hr>

<p><em>fit_transform(X, y=None)</em></p>
<p>Return a slice of the input array.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples] (default: None)</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>X_slice</code> : shape = [n_samples, k_features]</p>
<p>Subset of the feature space where k_features &lt;= n_features</p>
</li>
</ul>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Get parameters for this estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>deep</code> : bool, default=True</p>
<p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>params</code> : dict</p>
<p>Parameter names mapped to their values.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.</p>
<pre><code>The method works on simple estimators as well as on nested objects
(such as :class:`~sklearn.pipeline.Pipeline`). The latter have
parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
possible to update each component of a nested object.
</code></pre>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>**params</code> : dict</p>
<p>Estimator parameters.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>self</code> : estimator instance</p>
<p>Estimator instance.</p>
</li>
</ul>
<hr>

<p><em>transform(X, y=None)</em></p>
<p>Return a slice of the input array.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples] (default: None)</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>X_slice</code> : shape = [n_samples, k_features]</p>
<p>Subset of the feature space where k_features &lt;= n_features</p>
</li>
</ul>
<h2 id="exhaustivefeatureselector">ExhaustiveFeatureSelector</h2>
<p><em>ExhaustiveFeatureSelector(estimator, min_features=1, max_features=1, print_progress=True, scoring='accuracy', cv=5, n_jobs=1, pre_dispatch='2</em>n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)*</p>
<p>Exhaustive Feature Selection for Classification and Regression.
    (new in v0.4.3)</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>estimator</code> : scikit-learn classifier or regressor</p>
</li>
<li>
<p><code>min_features</code> : int (default: 1)</p>
<p>Minumum number of features to select</p>
</li>
<li>
<p><code>max_features</code> : int (default: 1)</p>
<p>Maximum number of features to select. If parameter <code>feature_groups</code> is not
None, the number of features is equal to the number of feature groups, i.e.
<code>len(feature_groups)</code>. For  example, if <code>feature_groups = [[0], [1], [2, 3],
[4]]</code>, then the <code>max_features</code> value cannot exceed 4.</p>
</li>
<li>
<p><code>print_progress</code> : bool (default: True)</p>
<p>Prints progress as the number of epochs
to stderr.</p>
</li>
<li>
<p><code>scoring</code> : str, (default='accuracy')</p>
<p>Scoring metric in {accuracy, f1, precision, recall, roc_auc}
for classifiers,
{'mean_absolute_error', 'mean_squared_error',
'median_absolute_error', 'r2'} for regressors,
or a callable object or function with
signature <code>scorer(estimator, X, y)</code>.</p>
</li>
<li>
<p><code>cv</code> : int (default: 5)</p>
<p>Scikit-learn cross-validation generator or <code>int</code>.
If estimator is a classifier (or y consists of integer class labels),
stratified k-fold is performed, and regular k-fold cross-validation
otherwise.
No cross-validation if cv is None, False, or 0.</p>
</li>
<li>
<p><code>n_jobs</code> : int (default: 1)</p>
<p>The number of CPUs to use for evaluating different feature subsets
in parallel. -1 means 'all CPUs'.</p>
</li>
<li>
<p><code>pre_dispatch</code> : int, or string (default: '2*n_jobs')</p>
<p>Controls the number of jobs that get dispatched
during parallel execution if <code>n_jobs &gt; 1</code> or <code>n_jobs=-1</code>.
Reducing this number can be useful to avoid an explosion of
memory consumption when more jobs get dispatched than CPUs can process.
This parameter can be:
None, in which case all the jobs are immediately created and spawned.
Use this for lightweight and fast-running jobs,
to avoid delays due to on-demand spawning of the jobs
An int, giving the exact number of total jobs that are spawned
A string, giving an expression as a function
of n_jobs, as in <code>2*n_jobs</code></p>
</li>
<li>
<p><code>clone_estimator</code> : bool (default: True)</p>
<p>Clones estimator if True; works with the original estimator instance
if False. Set to False if the estimator doesn't
implement scikit-learn's set_params and get_params methods.
In addition, it is required to set cv=0, and n_jobs=1.</p>
</li>
<li>
<p><code>fixed_features</code> : tuple (default: None)</p>
<p>If not <code>None</code>, the feature indices provided as a tuple will be
regarded as fixed by the feature selector. For example, if
<code>fixed_features=(1, 3, 7)</code>, the 2nd, 4th, and 8th feature are
guaranteed to be present in the solution. Note that if
<code>fixed_features</code> is not <code>None</code>, make sure that the number of
features to be selected is greater than <code>len(fixed_features)</code>.
In other words, ensure that <code>k_features &gt; len(fixed_features)</code>.</p>
</li>
<li>
<p><code>feature_groups</code> : list or None (default: None)</p>
<p>Optional argument for treating certain features as a group.
This means, the features within a group are always selected together,
never split.
For example, <code>feature_groups=[[1], [2], [3, 4, 5]]</code>
specifies 3 feature groups.In this case,
possible feature selection results with <code>k_features=2</code>
are <code>[[1], [2]</code>, <code>[[1], [3, 4, 5]]</code>, or <code>[[2], [3, 4, 5]]</code>.
Feature groups can be useful for
interpretability, for example, if features 3, 4, 5 are one-hot
encoded features.  (For  more details, please read the notes at the
bottom of this docstring).  New in mlxtend v. 0.21.0.</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>best_idx_</code> : array-like, shape = [n_predictions]</p>
<p>Feature Indices of the selected feature subsets.</p>
</li>
<li>
<p><code>best_feature_names_</code> : array-like, shape = [n_predictions]</p>
<p>Feature names of the selected feature subsets. If pandas
DataFrames are used in the <code>fit</code> method, the feature
names correspond to the column names. Otherwise, the
feature names are string representation of the feature
array indices. New in v 0.13.0.</p>
</li>
<li>
<p><code>best_score_</code> : float</p>
<p>Cross validation average score of the selected subset.</p>
</li>
<li>
<p><code>subsets_</code> : dict</p>
<p>A dictionary of selected feature subsets during the
exhaustive selection, where the dictionary keys are
the lengths k of these feature subsets. The dictionary
values are dictionaries themselves with the following
keys: 'feature_idx' (tuple of indices of the feature subset)
'feature_names' (tuple of feature names of the feat. subset)
'cv_scores' (list individual cross-validation scores)
'avg_score' (average cross-validation score)
Note that if pandas
DataFrames are used in the <code>fit</code> method, the 'feature_names'
correspond to the column names. Otherwise, the
feature names are string representation of the feature
array indices. The 'feature_names' is new in v. 0.13.0.</p>
</li>
</ul>
<p><strong>Notes</strong></p>
<p>(1) If parameter <code>feature_groups</code> is not None, the
    number of features is equal to the number of feature groups, i.e.
    <code>len(feature_groups)</code>. For  example, if <code>feature_groups = [[0], [1], [2, 3],
    [4]]</code>, then the <code>max_features</code> value cannot exceed 4.</p>
<pre><code>(2) Although two or more individual features may be considered as one group
throughout the feature-selection process, it does not mean the individual
features of that group have the same impact on the outcome. For instance, in
linear regression, the coefficient of the feature 2 and 3 can be different
even if they are considered as one group in feature_groups.

(3) If both fixed_features and feature_groups are specified, ensure that each
feature group contains the fixed_features selection. E.g., for a 3-feature set
fixed_features=[0, 1] and feature_groups=[[0, 1], [2]] is valid;
fixed_features=[0, 1] and feature_groups=[[0], [1, 2]] is not valid.

(4) In case of KeyboardInterrupt, the dictionary subsets may not be completed.
If user is still interested in getting the best score, they can use method
`finalize_fit`.
</code></pre>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    http://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/</p>
<h3 id="methods_1">Methods</h3>
<hr>

<p><em>finalize_fit()</em></p>
<p>None</p>
<hr>

<p><em>fit(X, y, groups=None, </em><em>fit_params)</em></p>
<p>Perform feature selection and learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>groups</code> : array-like, with shape (n_samples,), optional</p>
<p>Group labels for the samples used while splitting the dataset into
train/test set. Passed to the fit method of the cross-validator.</p>
</li>
<li>
<p><code>fit_params</code> : dict of string -&gt; object, optional</p>
<p>Parameters to pass to to the fit method of classifier.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>fit_transform(X, y, groups=None, </em><em>fit_params)</em></p>
<p>Fit to training data and return the best selected features from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.</p>
</li>
<li>
<p><code>groups</code> : array-like, with shape (n_samples,), optional</p>
<p>Group labels for the samples used while splitting the dataset into
train/test set. Passed to the fit method of the cross-validator.</p>
</li>
<li>
<p><code>fit_params</code> : dict of string -&gt; object, optional</p>
<p>Parameters to pass to to the fit method of classifier.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Feature subset of X, shape={n_samples, k_features}</p>
<hr>

<p><em>get_metric_dict(confidence_interval=0.95)</em></p>
<p>Return metric dictionary</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>confidence_interval</code> : float (default: 0.95)</p>
<p>A positive float between 0.0 and 1.0 to compute the confidence
interval bounds of the CV score averages.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Dictionary with items where each dictionary value is a list
    with the number of iterations (number of feature subsets) as
    its length. The dictionary keys corresponding to these lists
    are as follows:
    'feature_idx': tuple of the indices of the feature subset
    'cv_scores': list with individual CV scores
    'avg_score': of CV average scores
    'std_dev': standard deviation of the CV score average
    'std_err': standard error of the CV score average
    'ci_bound': confidence interval bound of the CV score average</p>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Get parameters for this estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>deep</code> : bool, default=True</p>
<p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>params</code> : dict</p>
<p>Parameter names mapped to their values.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.</p>
<pre><code>The method works on simple estimators as well as on nested objects
(such as :class:`~sklearn.pipeline.Pipeline`). The latter have
parameters of the form ``&lt;component&gt;__&lt;parameter&gt;`` so that it's
possible to update each component of a nested object.
</code></pre>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>**params</code> : dict</p>
<p>Estimator parameters.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>self</code> : estimator instance</p>
<p>Estimator instance.</p>
</li>
</ul>
<hr>

<p><em>transform(X)</em></p>
<p>Return the best selected features from X.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Feature subset of X, shape={n_samples, k_features}</p>
<h2 id="sequentialfeatureselector">SequentialFeatureSelector</h2>
<p><em>SequentialFeatureSelector(estimator, k_features=1, forward=True, floating=False, verbose=0, scoring=None, cv=5, n_jobs=1, pre_dispatch='2</em>n_jobs', clone_estimator=True, fixed_features=None, feature_groups=None)*</p>
<p>Sequential Feature Selection for Classification and Regression.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>estimator</code> : scikit-learn classifier or regressor</p>
</li>
<li>
<p><code>k_features</code> : int or tuple or str (default: 1)</p>
<p>Number of features to select,
where k_features &lt; the full feature set.
New in 0.4.2: A tuple containing a min and max value can be provided,
and the SFS will consider return any feature combination between
min and max that scored highest in cross-validation. For example,
the tuple (1, 4) will return any combination from
1 up to 4 features instead of a fixed number of features k.
New in 0.8.0: A string argument "best" or "parsimonious".
If "best" is provided, the feature selector will return the
feature subset with the best cross-validation performance.
If "parsimonious" is provided as an argument, the smallest
feature subset that is within one standard error of the
cross-validation performance will be selected.</p>
</li>
<li>
<p><code>forward</code> : bool (default: True)</p>
<p>Forward selection if True,
backward selection otherwise</p>
</li>
<li>
<p><code>floating</code> : bool (default: False)</p>
<p>Adds a conditional exclusion/inclusion if True.</p>
</li>
<li>
<p><code>verbose</code> : int (default: 0), level of verbosity to use in logging.</p>
<p>If 0, no output,
if 1 number of features in current set, if 2 detailed logging i
ncluding timestamp and cv scores at step.</p>
</li>
<li>
<p><code>scoring</code> : str, callable, or None (default: None)</p>
<p>If None (default), uses 'accuracy' for sklearn classifiers
and 'r2' for sklearn regressors.
If str, uses a sklearn scoring metric string identifier, for example
{accuracy, f1, precision, recall, roc_auc} for classifiers,
{'mean_absolute_error', 'mean_squared_error'/'neg_mean_squared_error',
'median_absolute_error', 'r2'} for regressors.
If a callable object or function is provided, it has to be conform with
sklearn's signature <code>scorer(estimator, X, y)</code>; see
http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html
for more information.</p>
</li>
<li>
<p><code>cv</code> : int (default: 5)</p>
<p>Integer or iterable yielding train, test splits. If cv is an integer
and <code>estimator</code> is a classifier (or y consists of integer class
labels) stratified k-fold. Otherwise regular k-fold cross-validation
is performed. No cross-validation if cv is None, False, or 0.</p>
</li>
<li>
<p><code>n_jobs</code> : int (default: 1)</p>
<p>The number of CPUs to use for evaluating different feature subsets
in parallel. -1 means 'all CPUs'.</p>
</li>
<li>
<p><code>pre_dispatch</code> : int, or string (default: '2*n_jobs')</p>
<p>Controls the number of jobs that get dispatched
during parallel execution if <code>n_jobs &gt; 1</code> or <code>n_jobs=-1</code>.
Reducing this number can be useful to avoid an explosion of
memory consumption when more jobs get dispatched than CPUs can process.
This parameter can be:
None, in which case all the jobs are immediately created and spawned.
Use this for lightweight and fast-running jobs,
to avoid delays due to on-demand spawning of the jobs
An int, giving the exact number of total jobs that are spawned
A string, giving an expression as a function
of n_jobs, as in <code>2*n_jobs</code></p>
</li>
<li>
<p><code>clone_estimator</code> : bool (default: True)</p>
<p>Clones estimator if True; works with the original estimator instance
if False. Set to False if the estimator doesn't
implement scikit-learn's set_params and get_params methods.
In addition, it is required to set cv=0, and n_jobs=1.</p>
</li>
<li>
<p><code>fixed_features</code> : tuple (default: None)</p>
<p>If not <code>None</code>, the feature indices provided as a tuple will be
regarded as fixed by the feature selector. For example, if
<code>fixed_features=(1, 3, 7)</code>, the 2nd, 4th, and 8th feature are
guaranteed to be present in the solution. Note that if
<code>fixed_features</code> is not <code>None</code>, make sure that the number of
features to be selected is greater than <code>len(fixed_features)</code>.
In other words, ensure that <code>k_features &gt; len(fixed_features)</code>.
New in mlxtend v. 0.18.0.</p>
</li>
<li>
<p><code>feature_groups</code> : list or None (default: None)</p>
<p>Optional argument for treating certain features as a group.
This means, the features within a group are always selected together,
never split.
For example, <code>feature_groups=[[1], [2], [3, 4, 5]]</code>
specifies 3 feature groups. In this case,
possible feature selection results with <code>k_features=2</code>
are <code>[[1], [2]</code>, <code>[[1], [3, 4, 5]]</code>, or <code>[[2], [3, 4, 5]]</code>.
Feature groups can be useful for
interpretability, for example, if features 3, 4, 5 are one-hot
encoded features.  (For  more details, please read the notes at the
bottom of this docstring).  New in mlxtend v. 0.21.0.</p>
</li>
</ul>
<p><strong>Attributes</strong></p>
<ul>
<li>
<p><code>k_feature_idx_</code> : array-like, shape = [n_predictions]</p>
<p>Feature Indices of the selected feature subsets.</p>
</li>
<li>
<p><code>k_feature_names_</code> : array-like, shape = [n_predictions]</p>
<p>Feature names of the selected feature subsets. If pandas
DataFrames are used in the <code>fit</code> method, the feature
names correspond to the column names. Otherwise, the
feature names are string representation of the feature
array indices. New in v 0.13.0.</p>
</li>
<li>
<p><code>k_score_</code> : float</p>
<p>Cross validation average score of the selected subset.</p>
</li>
<li>
<p><code>subsets_</code> : dict</p>
<p>A dictionary of selected feature subsets during the
sequential selection, where the dictionary keys are
the lengths k of these feature subsets. If the parameter
<code>feature_groups</code> is not None, the value of key indicates
the number of groups that are selected together. The dictionary
values are dictionaries themselves with the following
keys: 'feature_idx' (tuple of indices of the feature subset)
'feature_names' (tuple of feature names of the feat. subset)
'cv_scores' (list individual cross-validation scores)
'avg_score' (average cross-validation score)
Note that if pandas
DataFrames are used in the <code>fit</code> method, the 'feature_names'
correspond to the column names. Otherwise, the
feature names are string representation of the feature
array indices. The 'feature_names' is new in v 0.13.0.</p>
</li>
</ul>
<p><strong>Notes</strong></p>
<p>(1) If parameter <code>feature_groups</code> is not None, the
    number of features is equal to the number of feature groups, i.e.
    <code>len(feature_groups)</code>. For  example, if <code>feature_groups = [[0], [1], [2, 3],
    [4]]</code>, then the <code>max_features</code> value cannot exceed 4.</p>
<pre><code>(2) Although two or more individual features may be considered as one group
throughout the feature-selection process, it does not mean the individual
features of that group have the same impact on the outcome. For instance, in
linear regression, the coefficient of the feature 2 and 3 can be different
even if they are considered as one group in feature_groups.

(3) If both fixed_features and feature_groups are specified, ensure that each
feature group contains the fixed_features selection. E.g., for a 3-feature set
fixed_features=[0, 1] and feature_groups=[[0, 1], [2]] is valid;
fixed_features=[0, 1] and feature_groups=[[0], [1, 2]] is not valid.

(4) In case of KeyboardInterrupt, the dictionary subsets may not be completed.
If user is still interested in getting the best score, they can use method
`finalize_fit`.
</code></pre>
<p><strong>Examples</strong></p>
<p>For usage examples, please see
    http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/</p>
<h3 id="methods_2">Methods</h3>
<hr>

<p><em>finalize_fit()</em></p>
<p>None</p>
<hr>

<p><em>fit(X, y, groups=None, </em><em>fit_params)</em></p>
<p>Perform feature selection and learn model from training data.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for y.</p>
</li>
<li>
<p><code>groups</code> : array-like, with shape (n_samples,), optional</p>
<p>Group labels for the samples used while splitting the dataset into
train/test set. Passed to the fit method of the cross-validator.</p>
</li>
<li>
<p><code>fit_params</code> : various, optional</p>
<p>Additional parameters that are being passed to the estimator.
For example, <code>sample_weights=weights</code>.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><code>self</code> : object</li>
</ul>
<hr>

<p><em>fit_transform(X, y, groups=None, </em><em>fit_params)</em></p>
<p>Fit to training data then reduce X to its most important features.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
<li>
<p><code>y</code> : array-like, shape = [n_samples]</p>
<p>Target values.
New in v 0.13.0: a pandas Series are now also accepted as
argument for y.</p>
</li>
<li>
<p><code>groups</code> : array-like, with shape (n_samples,), optional</p>
<p>Group labels for the samples used while splitting the dataset into
train/test set. Passed to the fit method of the cross-validator.</p>
</li>
<li>
<p><code>fit_params</code> : various, optional</p>
<p>Additional parameters that are being passed to the estimator.
For example, <code>sample_weights=weights</code>.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Reduced feature subset of X, shape={n_samples, k_features}</p>
<hr>

<p><em>generate_error_message_k_features(name)</em></p>
<p>None</p>
<hr>

<p><em>get_metric_dict(confidence_interval=0.95)</em></p>
<p>Return metric dictionary</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>confidence_interval</code> : float (default: 0.95)</p>
<p>A positive float between 0.0 and 1.0 to compute the confidence
interval bounds of the CV score averages.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Dictionary with items where each dictionary value is a list
    with the number of iterations (number of feature subsets) as
    its length. The dictionary keys corresponding to these lists
    are as follows:
    'feature_idx': tuple of the indices of the feature subset
    'cv_scores': list with individual CV scores
    'avg_score': of CV average scores
    'std_dev': standard deviation of the CV score average
    'std_err': standard error of the CV score average
    'ci_bound': confidence interval bound of the CV score average</p>
<hr>

<p><em>get_params(deep=True)</em></p>
<p>Get parameters for this estimator.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>deep</code> : bool, default=True</p>
<p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li>
<p><code>params</code> : dict</p>
<p>Parameter names mapped to their values.</p>
</li>
</ul>
<hr>

<p><em>set_params(</em><em>params)</em></p>
<p>Set the parameters of this estimator.
    Valid parameter keys can be listed with <code>get_params()</code>.</p>
<p><strong>Returns</strong></p>
<p>self</p>
<hr>

<p><em>transform(X)</em></p>
<p>Reduce X to its most important features.</p>
<p><strong>Parameters</strong></p>
<ul>
<li>
<p><code>X</code> : {array-like, sparse matrix}, shape = [n_samples, n_features]</p>
<p>Training vectors, where n_samples is the number of samples and
n_features is the number of features.
New in v 0.13.0: pandas DataFrames are now also accepted as
argument for X.</p>
</li>
</ul>
<p><strong>Returns</strong></p>
<p>Reduced feature subset of X, shape={n_samples, k_features}</p>
<h3 id="properties">Properties</h3>
<hr>

<p><em>named_estimators</em></p>
<p><strong>Returns</strong></p>
<p>List of named estimator tuples, like [('svc', SVC(...))]</p></div>
        
        
    </div>

    <footer class="col-md-12 text-center">
        <hr>
        <p>
        <small>Copyright &copy; 2014-2022 <a href="http://sebastianraschka.com">Sebastian Raschka</a><br></small>
        
        <small>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p></small>
    </footer>

    <script src="../../js/jquery-1.10.2.min.js"></script>
    <script src="../../js/bootstrap-3.0.3.min.js"></script>
    <script src="../../js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script>
    var base_url = '../..';
    </script>
    <script data-main="../../mkdocs/js/search.js" src="../../mkdocs/js/require.js"></script>
    <script src="../../js/base.js"></script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <script src="../../mathjaxhelper.js"></script>
    <script src="../../search/main.js"></script>

    <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
                <div class="modal-header">
                    <button type="button" class="close" data-dismiss="modal">
                        <span aria-hidden="true">&times;</span>
                        <span class="sr-only">Close</span>
                    </button>
                    <h4 class="modal-title" id="exampleModalLabel">Search</h4>
                </div>
                <div class="modal-body">
                    <p>
                        From here you can search these documents. Enter your search terms below.
                    </p>
                    <form role="form">
                        <div class="form-group">
                            <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query">
                        </div>
                    </form>
                    <div id="mkdocs-search-results"></div>
                </div>
                <div class="modal-footer">
                </div>
            </div>
        </div>
    </div>

    </body>

</html>
